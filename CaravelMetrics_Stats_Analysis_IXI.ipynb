{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Population-Level Brain Vessel Analysis Pipeline\n",
    "## Flexible Analysis Framework for Variable Feature Sets\n",
    "\n",
    "**Purpose:** Comprehensive vessel analysis pipeline that automatically adapts to available features in any CSV format:\n",
    "- **Dynamic Feature Detection** - Automatically identifies available metrics\n",
    "- **Modular Analysis** - Runs appropriate analyses based on detected features\n",
    "- **Robust Handling** - Gracefully handles missing regions, tortuosity, or other features\n",
    "- **Comprehensive Output** - Generates publication-ready figures and statistics\n",
    "\n",
    "**Key Features:**\n",
    "- Works with any vessel feature CSV structure\n",
    "- Fixed metadata file format (IXI_METADATA.xls)\n",
    "- Automatic feature categorization (morphometric, topological, curvature)\n",
    "- Conditional analysis execution based on available data\n",
    "- Export-ready results for ISBI paper\n",
    "  \n",
    "**Date:** October 23, 2025\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Structure:\n",
    "1. Setup and Configuration\n",
    "2. Data Loading with Feature Detection\n",
    "3. Automatic Feature Categorization\n",
    "4. Data Quality Assessment\n",
    "5. Descriptive Statistics\n",
    "6. Age-Related Analysis\n",
    "7. Sex-Based Analysis\n",
    "8. Anthropometric Correlations (Height, Weight, BMI)\n",
    "9. Multi-Center Analysis\n",
    "10. Regional Analysis (if available)\n",
    "11. Hemispheric Asymmetry (if available)\n",
    "12. Advanced Analyses (ML, interactions, stratification)\n",
    "13. Summary and Export for Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, linregress, ttest_ind, f_oneway\n",
    "from scipy.stats import mannwhitneyu, kruskal, chi2_contingency\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from glob import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Statistical modeling\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "try:\n",
    "    from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "    MIXEDLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MIXEDLM_AVAILABLE = False\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# Configure display and plotting\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Publication-quality figure settings\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "# Professional styling\n",
    "plt.rcParams['axes.linewidth'] = 0.8\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['xtick.major.width'] = 0.8\n",
    "plt.rcParams['ytick.major.width'] = 0.8\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Mixed-effects models available: {MIXEDLM_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Configure Data Paths\n",
    "\n",
    "**IMPORTANT:** Update these paths to match your data structure.\n",
    "\n",
    "**Required Files:**\n",
    "- **DEMOGRAPHICS_FILE**: Fixed format (IXI_METADATA.xls) with required columns:\n",
    "  - IXI_ID, AGE, SEX_ID, HEIGHT, WEIGHT, ETHNIC_ID, etc.\n",
    "- **FEATURES_CSV**: Any vessel feature CSV with columns:\n",
    "  - Must have 'subject_id' column matching IXI format (e.g., 'IXI001')\n",
    "  - Can contain any vessel metrics (morphometric, topological, curvature)\n",
    "  - Optional 'region' column for regional analysis\n",
    "  - Optional hemisphere indicators for asymmetry analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VESSEL_METRIC_FLAG = True  # Set to True to indicate vessel metrics analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not VESSEL_METRIC_FLAG:\n",
    "    # ============================================================================\n",
    "    # CONFIGURATION: Update these paths for your local setup\n",
    "    # ============================================================================\n",
    "\n",
    "    # Path to demographics file (FIXED FORMAT - required)\n",
    "    DEMOGRAPHICS_FILE = \"/path/to/IXI_METADATA.xls\"\n",
    "\n",
    "    # Path to vessel features CSV (FLEXIBLE FORMAT - any structure)\n",
    "    # This can be:\n",
    "    #   - A single aggregated CSV with one row per subject\n",
    "    #   - A regional CSV with multiple rows per subject\n",
    "    #   - Any CSV with vessel features and a 'subject_id' column\n",
    "    FEATURES_CSV = \"/path/to/vessel_features.csv\"\n",
    "\n",
    "    # Output directory for results\n",
    "    OUTPUT_DIR = \"outputs_universal_analysis/\"\n",
    "\n",
    "    # Subject ID pattern (for extracting IDs from filenames if needed)\n",
    "    SUBJECT_ID_REGEX = r\"(IXI\\d{3})\"\n",
    "\n",
    "    # ============================================================================\n",
    "\n",
    "    # Create output directories\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    FIGURES_DIR = Path(OUTPUT_DIR) / 'figures'\n",
    "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TABLES_DIR = Path(OUTPUT_DIR) / 'tables'\n",
    "    TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"✓ Configuration set\")\n",
    "    print(f\"  Demographics: {DEMOGRAPHICS_FILE}\")\n",
    "    print(f\"  Features CSV: {FEATURES_CSV}\")\n",
    "    print(f\"  Output:       {OUTPUT_DIR}\")\n",
    "    print(f\"  Figures:      {FIGURES_DIR}\")\n",
    "    print(f\"  Tables:       {TABLES_DIR}\")\n",
    "else:\n",
    "    print(\"✓ Vessel metrics analysis mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VESSEL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VESSEL_METRIC_FLAG:\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CONFIGURATION: Update these paths for your local setup\n",
    "    # ============================================================================\n",
    "\n",
    "    # Path to demographics file (FIXED FORMAT - required)\n",
    "    DEMOGRAPHICS_FILE = \"/home/falcetta/ISBI2025/METADATA/IXI_METADATA.xls\"\n",
    "\n",
    "    # Path to vessel segmentation data directory\n",
    "    VESSEL_DATA_DIR = \"/home/falcetta/ISBI2025/VESSELVIO_FEATURES\"\n",
    "    VESSEL_DATA_DIR = \"/home/falcetta/ISBI2025/VESSELEXP_FEATURES\"\n",
    "    VESSEL_DATA_DIR = \"/home/falcetta/ISBI2025/IXI_EXTRACTED_FEATURES\"\n",
    "\n",
    "    # File naming patterns (what to look for in VESSEL_DATA_DIR)\n",
    "    # Option 1: Use region summary files (one CSV per subject with regional data)\n",
    "    REGION_SUMMARY_PATTERN = \"*region_summary*.csv\"\n",
    "\n",
    "    # Option 2: Use component files (optional, for component-level analysis)\n",
    "    COMPONENTS_PATTERN = \"*all_components*.csv\"\n",
    "\n",
    "    # Option 3: Use any custom CSV pattern that contains vessel features\n",
    "    # CUSTOM_PATTERN = \"*vessel_features*.csv\"\n",
    "\n",
    "    # Which file type to use as primary features?\n",
    "    # Options: 'region_summary', 'components', or 'custom' or 'VesselExpress' or 'VesselVio'\n",
    "    FEATURE_FILE_TYPE = 'VesselExpress'  if 'VESSELEXP' in VESSEL_DATA_DIR else 'VesselVio' if 'VESSELVIO' in VESSEL_DATA_DIR else 'components' # 'region_summary' or 'components'\n",
    "\n",
    "    # Output directory for results\n",
    "    OUTPUT_DIR = f\"outputs_universal_analysis_VESSEL_METRICS_{FEATURE_FILE_TYPE}_REMOVE_IOP/\"\n",
    "\n",
    "    # Subject ID pattern (for extracting IDs from filenames)\n",
    "    SUBJECT_ID_REGEX = r\"(IXI\\d{3})\"\n",
    "\n",
    "    # ============================================================================\n",
    "\n",
    "    # Create output directories\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    FIGURES_DIR = Path(OUTPUT_DIR) / 'figures'\n",
    "    FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TABLES_DIR = Path(OUTPUT_DIR) / 'tables'\n",
    "    TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"✓ Configuration set\")\n",
    "    print(f\"  Demographics: {DEMOGRAPHICS_FILE}\")\n",
    "    print(f\"  Vessel data:  {VESSEL_DATA_DIR}\")\n",
    "    print(f\"  File type:    {FEATURE_FILE_TYPE}\")\n",
    "    print(f\"  Output:       {OUTPUT_DIR}\")\n",
    "    print(f\"  Figures:      {FIGURES_DIR}\")\n",
    "    print(f\"  Tables:       {TABLES_DIR}\")\n",
    "else:\n",
    "    print(\"✓ Universal analysis mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading with Automatic Feature Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Demographics (Fixed Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demographics\n",
    "print(\"Loading demographics...\")\n",
    "try:\n",
    "    demographics = pd.read_excel(DEMOGRAPHICS_FILE)\n",
    "    print(f\"✓ Loaded demographics: {len(demographics)} subjects\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading demographics: {e}\")\n",
    "    raise\n",
    "\n",
    "# Standardize subject ID column\n",
    "if 'IXI_ID' in demographics.columns:\n",
    "    demographics['subject_id'] = 'IXI' + demographics['IXI_ID'].astype(str).str.zfill(3)\n",
    "elif 'subject_id' not in demographics.columns:\n",
    "    raise ValueError(\"Demographics must have 'IXI_ID' or 'subject_id' column\")\n",
    "\n",
    "# Data quality: Remove implausible height/weight values\n",
    "if 'HEIGHT' in demographics.columns:\n",
    "    demographics.loc[(demographics['HEIGHT'] < 50) | (demographics['HEIGHT'] > 250), 'HEIGHT'] = np.nan\n",
    "if 'WEIGHT' in demographics.columns:\n",
    "    demographics.loc[(demographics['WEIGHT'] < 20) | (demographics['WEIGHT'] > 300), 'WEIGHT'] = np.nan\n",
    "\n",
    "# Calculate BMI if height and weight are available\n",
    "if 'HEIGHT' in demographics.columns and 'WEIGHT' in demographics.columns:\n",
    "    demographics['BMI'] = demographics['WEIGHT'] / (demographics['HEIGHT'] / 100) ** 2\n",
    "    demographics.loc[(demographics['BMI'] < 10) | (demographics['BMI'] > 60), 'BMI'] = np.nan\n",
    "    print(f\"✓ BMI calculated for subjects with valid height/weight\")\n",
    "\n",
    "# Rename SEX_ID (1=m, 2=f) to 'SEX_ID'\n",
    "if 'SEX_ID (1=m, 2=f)' in demographics.columns:\n",
    "    demographics.rename(columns={'SEX_ID (1=m, 2=f)': 'SEX_ID'}, inplace=True)\n",
    "    print(\"✓ Renamed 'SEX_ID (1=m, 2=f)' to 'SEX_ID'\")\n",
    "\n",
    "# Identify available demographic variables\n",
    "DEMOGRAPHIC_VARS = [col for col in demographics.columns if col not in ['subject_id', 'IXI_ID', 'DOB', 'STUDY_DATE', 'DATE_AVAILABLE']]\n",
    "\n",
    "#Remove DOB\tDATE_AVAILABLE\tSTUDY_DATE from demographic variables\n",
    "demographics = demographics.drop(columns=['DOB', 'DATE_AVAILABLE', 'STUDY_DATE'], errors='ignore')\n",
    "print(f\"\\nAvailable demographic variables: {DEMOGRAPHIC_VARS}\")\n",
    "print(f\"\\nFirst 3 subjects:\")\n",
    "display(demographics.head(3))\n",
    "\n",
    "DEMOGRAPHIC_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load Vessel Features (Flexible Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not VESSEL_METRIC_FLAG:\n",
    "    #Load vessel features CSV\n",
    "    print(\"\\nLoading vessel features...\")\n",
    "    try:\n",
    "        features_df = pd.read_csv(FEATURES_CSV)\n",
    "        print(f\"✓ Loaded features CSV: {len(features_df)} rows, {len(features_df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading features CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Verify subject_id column exists\n",
    "    if 'subject_id' not in features_df.columns:\n",
    "        raise ValueError(\"Features CSV must contain 'subject_id' column\")\n",
    "\n",
    "    # Check if this is regional data (multiple rows per subject)\n",
    "    rows_per_subject = features_df.groupby('subject_id').size()\n",
    "    IS_REGIONAL_DATA = (rows_per_subject > 1).any()\n",
    "\n",
    "    print(f\"\\nData structure:\")\n",
    "    print(f\"  Unique subjects: {features_df['subject_id'].nunique()}\")\n",
    "    print(f\"  Regional data: {IS_REGIONAL_DATA}\")\n",
    "    if IS_REGIONAL_DATA:\n",
    "        print(f\"  Rows per subject: {rows_per_subject.describe()}\")\n",
    "\n",
    "    # Display first rows\n",
    "    print(f\"\\nFirst 5 rows of features:\")\n",
    "    display(features_df.head())\n",
    "\n",
    "    print(f\"\\nColumn names:\")\n",
    "    print(features_df.columns.tolist())\n",
    "else:\n",
    "    print(\"\\nVessel metrics analysis mode enabled; skipping generic features loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VESSEL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VESSEL_METRIC_FLAG:\n",
    "    # Load vessel features from directory\n",
    "    print(\"\\nLoading vessel features from directory...\")\n",
    "\n",
    "    # Select the appropriate file pattern\n",
    "    if FEATURE_FILE_TYPE == 'region_summary':\n",
    "        pattern = REGION_SUMMARY_PATTERN\n",
    "    elif FEATURE_FILE_TYPE == 'components':\n",
    "        pattern = COMPONENTS_PATTERN\n",
    "    elif FEATURE_FILE_TYPE == 'VesselExpress':\n",
    "        pattern = \"*VesselExpress*.csv\"\n",
    "    elif pattern == 'VesselVio':\n",
    "        pattern = \"*VesselVio*.csv\"\n",
    "    else:\n",
    "        pattern = CUSTOM_PATTERN if 'CUSTOM_PATTERN' in globals() else REGION_SUMMARY_PATTERN\n",
    "\n",
    "    # Find all matching files\n",
    "    feature_files = glob(str(Path(VESSEL_DATA_DIR) / \"**\" / pattern), recursive=True)\n",
    "    print(f\"Found {len(feature_files)} files matching pattern '{pattern}'\")\n",
    "\n",
    "    if len(feature_files) == 0:\n",
    "        raise ValueError(f\"No files found matching pattern {pattern} in {VESSEL_DATA_DIR}\")\n",
    "\n",
    "    # Load all files\n",
    "    feature_data_list = []\n",
    "    for file in feature_files:\n",
    "        # Extract subject ID from filename\n",
    "        match = re.search(SUBJECT_ID_REGEX, str(file))\n",
    "        if match:\n",
    "            subject_id = match.group(1)\n",
    "            try:\n",
    "                df_temp = pd.read_csv(file)\n",
    "                if FEATURE_FILE_TYPE == 'VesselExpress' or FEATURE_FILE_TYPE == 'VesselVio':\n",
    "                    df_temp = pd.read_csv(file, sep=';')  # Example adjustment for VesselExpress/VesselVio format\n",
    "                df_temp['subject_id'] = subject_id\n",
    "                feature_data_list.append(df_temp)\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  Error loading {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if len(feature_data_list) == 0:\n",
    "        raise ValueError(\"No valid feature files could be loaded\")\n",
    "\n",
    "    # Concatenate all data\n",
    "    features_df = pd.concat(feature_data_list, ignore_index=True)\n",
    "    print(f\"✓ Loaded features from {len(feature_data_list)} subjects\")\n",
    "\n",
    "    # Check if this is regional data (multiple rows per subject)\n",
    "    rows_per_subject = features_df.groupby('subject_id').size()\n",
    "    IS_REGIONAL_DATA = (rows_per_subject > 1).any()\n",
    "\n",
    "    print(f\"\\nData structure:\")\n",
    "    print(f\"  Total rows: {len(features_df)}\")\n",
    "    print(f\"  Unique subjects: {features_df['subject_id'].nunique()}\")\n",
    "    print(f\"  Regional data: {IS_REGIONAL_DATA}\")\n",
    "    if IS_REGIONAL_DATA:\n",
    "        print(f\"  Rows per subject: min={rows_per_subject.min()}, max={rows_per_subject.max()}, mean={rows_per_subject.mean():.1f}\")\n",
    "\n",
    "    # Display first rows\n",
    "    print(f\"\\nFirst 5 rows of features:\")\n",
    "    display(features_df.head())\n",
    "\n",
    "    print(f\"\\nColumn names ({len(features_df.columns)} columns):\")\n",
    "    print(features_df.columns.tolist())\n",
    "else:\n",
    "    print(\"✓ Universal analysis mode enabled\")### VESSEL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_df.to_csv(Path(OUTPUT_DIR) / \"loaded_features_preview.csv\", index=False)\n",
    "#print(f'\\nSaved preview of loaded features to {Path(OUTPUT_DIR) / \"loaded_features_preview.csv\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the features that contain 'curvature'\n",
    "curvature_features = [col for col in features_df.columns if 'c' in col.lower()]\n",
    "print(f\"\\nFeatures containing 'curv': {curvature_features}\")\n",
    "\n",
    "# Aggregate root_mean_curvature \n",
    "root_mean_curvature_cols = [col for col in features_df.columns if 'root_mean_curvature' in col.lower()]\n",
    "if root_mean_curvature_cols:\n",
    "    features_df['root_mean_curvature_avg'] = features_df[root_mean_curvature_cols].mean(axis=1)\n",
    "    print(f\"✓ Aggregated root_mean_curvature into 'root_mean_curvature_avg'\")\n",
    "    # remove individual columns if desired\n",
    "    features_df.drop(columns=root_mean_curvature_cols, inplace=True)\n",
    "    \n",
    "# Aggregate mean_squared_curvature\n",
    "mean_squared_curvature_cols = [col for col in features_df.columns if 'mean_square_curvature' in col.lower()]\n",
    "if mean_squared_curvature_cols:\n",
    "    features_df['mean_squared_curvature_avg'] = features_df[mean_squared_curvature_cols].mean(axis=1)\n",
    "    print(f\"✓ Aggregated mean_squared_curvature into 'mean_squared_curvature_avg'\")\n",
    "    # remove individual columns if desired\n",
    "    features_df.drop(columns=mean_squared_curvature_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Present In           | Attributes                                                                 |\n",
    "|-----------------------|----------------------------------------------------------------------------|\n",
    "| **Both**              | region_label, total_length, num_bifurcations, volume, num_loops, num_abnormal_degree_nodes, subject_id, root_mean_curvature_avg, mean_squared_curvature_avg |\n",
    "| **Only in COMPONENT** | bifurcation_density, fractal_dimension, lacunarity                         |\n",
    "| **Only in REGION SUMMARY** | num_components, betti_0, betti_1, betti_2                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE IOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# # Directory you listed earlier (adjust if different)\n",
    "# ixi_dir = '/data/galati/brain_data/IXI_FINAL/fold1/imagesTs'\n",
    "\n",
    "# # Build list (or reuse existing IXI_LIST variable if already defined)\n",
    "# try:\n",
    "#     IXI_LIST = os.listdir(ixi_dir)\n",
    "# except Exception as e:\n",
    "#     IXI_LIST = globals().get('IXI_LIST', [])\n",
    "#     print(f\"Warning: could not list {ixi_dir}: {e}. Using existing IXI_LIST (len={len(IXI_LIST)})\")\n",
    "\n",
    "# # Prepare DataFrame and save\n",
    "# out_dir = '/home/falcetta/ISBI2025/METADATA'\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "# out_path = os.path.join(out_dir, 'IXI_LIST.csv')\n",
    "\n",
    "# df = pd.DataFrame({'filename': IXI_LIST})\n",
    "# df.to_csv(out_path, header=True)\n",
    "# print(f\"Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "#Read csv from /home/falcetta/ISBI2025/METADATA/IXI_LIST.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "file_path = '/home/falcetta/ISBI2025/METADATA/IXI_LIST.csv'\n",
    "\n",
    "try:\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    COMPLETE_IXI = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display the first 5 rows of the DataFrame\n",
    "    print(\"Successfully loaded the CSV file. Here are the some rows:\")\n",
    "    \n",
    "    # To display the entire DataFrame, you could just use:\n",
    "    # print(df)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the path: {file_path}\")\n",
    "    print(\"Please make sure the file path is correct and the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "COMPLETE_IXI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Step 2: Function to extract site from a full filename ---\n",
    "# def identify_site(filename):\n",
    "#     \"\"\"Identifies the IXI imaging site (Guys, HH, or IOP) from a filename string.\"\"\"\n",
    "#     match = re.search(r'(Guys|HH|IOP)', filename)\n",
    "#     if match:\n",
    "#         return match.group(1)\n",
    "#     return 'Unknown'\n",
    "\n",
    "\n",
    "# # --- Step 3: Main function to find the site from a short ID ---\n",
    "# def get_site_from_id(subject_id, COMPLETE_IXI=COMPLETE_IXI):\n",
    "#     \"\"\"\n",
    "#     Finds the site for a short subject ID (e.g., 'IXI002') by searching a DataFrame.\n",
    "    \n",
    "#     Args:\n",
    "#         subject_id (str): The short ID to search for (e.g., 'IXI002').\n",
    "#         df (pd.DataFrame): The DataFrame containing a 'filename' column.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The corresponding site name or a message if not found.\n",
    "#     \"\"\"\n",
    "#     df = COMPLETE_IXI\n",
    "#     # Find all rows where the filename starts with the subject_id\n",
    "#     # This is more precise than 'contains' as it avoids matching 'IXI012' with 'IXI0123'\n",
    "#     matching_rows = df[df['filename'].str.startswith(subject_id)]\n",
    "    \n",
    "#     # If we found at least one match...\n",
    "#     if not matching_rows.empty:\n",
    "#         # Get the full filename from the first result\n",
    "#         full_filename = matching_rows.iloc[0]['filename']\n",
    "#         # Extract and return the site from that filename\n",
    "#         return identify_site(full_filename)\n",
    "#     else:\n",
    "#         # If no filenames matched, return a 'not found' message\n",
    "#         return f\"Subject ID '{subject_id}' not found in the DataFrame.\"\n",
    "\n",
    "# # Add site information\n",
    "# features_df['site'] = features_df['subject_id'].apply(get_site_from_id)\n",
    "\n",
    "# # Remove 'Unknown' site entries\n",
    "# features_df = features_df[features_df['site'] != 'Unknown']\n",
    "# site_counts = features_df['site'].value_counts()\n",
    "# features_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimized version ---\n",
    "import re\n",
    "\n",
    "# Create a lookup dictionary once (instead of searching repeatedly)\n",
    "def create_site_lookup(df):\n",
    "    \"\"\"Creates a fast lookup dictionary mapping subject_id to site.\"\"\"\n",
    "    site_lookup = {}\n",
    "    pattern = re.compile(r'IXI\\d+')  # Match subject IDs like IXI002, IXI123, etc.\n",
    "    \n",
    "    for filename in df['filename'].unique():\n",
    "        # Extract subject ID from filename\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            subject_id = match.group()\n",
    "            # Extract site from filename\n",
    "            site_match = re.search(r'(Guys|HH|IOP)', filename)\n",
    "            if site_match and subject_id not in site_lookup:\n",
    "                site_lookup[subject_id] = site_match.group(1)\n",
    "    \n",
    "    return site_lookup\n",
    "\n",
    "# Build the lookup dictionary once\n",
    "site_lookup = create_site_lookup(COMPLETE_IXI)\n",
    "\n",
    "# Apply the lookup (vectorized operation)\n",
    "features_df['site'] = features_df['subject_id'].map(site_lookup).fillna('Unknown')\n",
    "\n",
    "# Remove 'Unknown' site entries\n",
    "features_df = features_df[features_df['site'] != 'Unknown']\n",
    "site_counts = features_df['site'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE IOP\n",
    "features_df = features_df[features_df['site'] != 'IOP']\n",
    "# REMOVE GUYS\n",
    "#features_df = features_df[features_df['site'] != 'Guys']\n",
    "# REMOVE HH\n",
    "#features_df = features_df[features_df['site'] != 'HH']\n",
    "\n",
    "# JUST IOP\n",
    "#features_df = features_df[features_df['site'] == 'IOP']\n",
    "# JUST GUYS\n",
    "#features_df = features_df[features_df['site'] == 'Guys']\n",
    "# JUST HH\n",
    "#features_df = features_df[features_df['site'] !== 'HH']\n",
    "site_counts = features_df['site'].value_counts()\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Automatic Feature Categorization and Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categorization keywords\n",
    "MORPHOMETRIC_KEYWORDS = ['length', 'volume', 'area', 'diameter', 'radius', 'thickness', 'density', 'count', 'num_']\n",
    "TOPOLOGICAL_KEYWORDS = ['fractal', 'lacunarity', 'betti', 'bifurcation', 'loop', 'component', 'branch', 'node', 'degree']\n",
    "CURVATURE_KEYWORDS = ['curvature', 'tortuosity', 'sinuosity', 'curl']\n",
    "REGION_KEYWORDS = ['region', 'territory', 'hemisphere', 'lobe', 'area', 'region_label']\n",
    "\n",
    "def categorize_feature(col_name):\n",
    "    \"\"\"Categorize a feature column based on its name.\"\"\"\n",
    "    col_lower = col_name.lower()\n",
    "    \n",
    "    if any(kw in col_lower for kw in CURVATURE_KEYWORDS):\n",
    "        return 'curvature'\n",
    "    elif any(kw in col_lower for kw in TOPOLOGICAL_KEYWORDS):\n",
    "        return 'topological'\n",
    "    elif any(kw in col_lower for kw in MORPHOMETRIC_KEYWORDS):\n",
    "        return 'morphometric'\n",
    "    elif any(kw in col_lower for kw in REGION_KEYWORDS):\n",
    "        return 'region_identifier'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Identify feature columns (exclude subject_id and known identifiers)\n",
    "EXCLUDE_COLS = ['subject_id', 'region', 'region_id', 'hemisphere', 'territory', 'site', 'label']\n",
    "feature_columns = [col for col in features_df.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "# Categorize all features\n",
    "feature_categories = {}\n",
    "for col in feature_columns:\n",
    "    # Only include numeric columns as features\n",
    "    if pd.api.types.is_numeric_dtype(features_df[col]):\n",
    "        feature_categories[col] = categorize_feature(col)\n",
    "\n",
    "# Organize features by category\n",
    "MORPHOMETRIC_FEATURES = [f for f, cat in feature_categories.items() if cat == 'morphometric']\n",
    "TOPOLOGICAL_FEATURES = [f for f, cat in feature_categories.items() if cat == 'topological']\n",
    "CURVATURE_FEATURES = [f for f, cat in feature_categories.items() if cat == 'curvature']\n",
    "OTHER_FEATURES = [f for f, cat in feature_categories.items() if cat == 'other']\n",
    "ALL_FEATURES = list(feature_categories.keys())\n",
    "# Remove region_label from ALL_FEATURES if present\n",
    "if 'region_label' in ALL_FEATURES:\n",
    "    ALL_FEATURES.remove('region_label')\n",
    "\n",
    "# Check for regional indicators\n",
    "HAS_REGIONS = 'region' in features_df.columns or 'region_id' in features_df.columns or any('region' in str(col).lower() for col in features_df.columns)\n",
    "HAS_HEMISPHERE = 'hemisphere' in features_df.columns or any('hemisphere' in str(col).lower() for col in features_df.columns) \n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE DETECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal features detected: {len(ALL_FEATURES)}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  Morphometric features: {len(MORPHOMETRIC_FEATURES)}\")\n",
    "if len(MORPHOMETRIC_FEATURES) > 0:\n",
    "    print(f\"    Examples: {MORPHOMETRIC_FEATURES[:5]}\")\n",
    "print(f\"  Topological features:  {len(TOPOLOGICAL_FEATURES)}\")\n",
    "if len(TOPOLOGICAL_FEATURES) > 0:\n",
    "    print(f\"    Examples: {TOPOLOGICAL_FEATURES[:5]}\")\n",
    "print(f\"  Curvature features:    {len(CURVATURE_FEATURES)}\")\n",
    "if len(CURVATURE_FEATURES) > 0:\n",
    "    print(f\"    Examples: {CURVATURE_FEATURES[:5]}\")\n",
    "print(f\"  Other numeric features: {len(OTHER_FEATURES)}\")\n",
    "if len(OTHER_FEATURES) > 0:\n",
    "    print(f\"    Examples: {OTHER_FEATURES[:5]}\")\n",
    "\n",
    "print(f\"\\nData structure flags:\")\n",
    "print(f\"  Regional data:     {IS_REGIONAL_DATA}\")\n",
    "print(f\"  Has regions:       {HAS_REGIONS}\")\n",
    "print(f\"  Has hemispheres:   {HAS_HEMISPHERE}\")\n",
    "\n",
    "# Save feature catalog\n",
    "feature_catalog = pd.DataFrame([\n",
    "    {'feature': f, 'category': cat, 'dtype': features_df[f].dtype}\n",
    "    for f, cat in feature_categories.items()\n",
    "])\n",
    "feature_catalog.to_csv(TABLES_DIR / 'feature_catalog.csv', index=False)\n",
    "print(f\"\\n✓ Feature catalog saved to {TABLES_DIR / 'feature_catalog.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Aggregate Regional Data (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If regional data, create whole-brain aggregates\n",
    "if IS_REGIONAL_DATA:\n",
    "    print(\"\\nCreating whole-brain aggregates from regional data...\")\n",
    "    \n",
    "    # Save original regional data\n",
    "    regional_df = features_df.copy()\n",
    "    \n",
    "    # Aggregate strategies by feature type\n",
    "    agg_dict = {}\n",
    "    for feature in ALL_FEATURES:\n",
    "        # Sum for extensive properties (length, volume, counts)\n",
    "        if any(kw in feature.lower() for kw in ['length', 'volume', 'count', 'num_', 'area']):\n",
    "            agg_dict[feature] = 'sum'\n",
    "        # Mean for intensive properties (density, curvature, etc.)\n",
    "        else:\n",
    "            agg_dict[feature] = 'mean'\n",
    "    \n",
    "    # Create aggregated dataset\n",
    "    features_df = regional_df.groupby('subject_id').agg(agg_dict).reset_index()\n",
    "    \n",
    "    print(f\"✓ Aggregated to whole-brain features\")\n",
    "    print(f\"  Subjects: {len(features_df)}\")\n",
    "    print(f\"  Features: {len(ALL_FEATURES)}\")\n",
    "else:\n",
    "    print(\"\\nData already at subject level (one row per subject)\")\n",
    "    regional_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Merge Demographics with Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge demographics with vessel features\n",
    "print(\"\\nMerging demographics with vessel features...\")\n",
    "df = demographics.merge(features_df, on='subject_id', how='inner')\n",
    "\n",
    "print(f\"✓ Merged dataset:\")\n",
    "print(f\"  Total subjects: {len(df)}\")\n",
    "print(f\"  Vessel features: {len(ALL_FEATURES)}\")\n",
    "print(f\"  Demographic variables: {len(DEMOGRAPHIC_VARS)}\")\n",
    "print(f\"  Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Check for missing data\n",
    "missing_summary = df[ALL_FEATURES + DEMOGRAPHIC_VARS].isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "if len(missing_summary) > 0:\n",
    "    print(f\"\\nVariables with missing data:\")\n",
    "    print(missing_summary)\n",
    "else:\n",
    "    print(f\"\\n✓ No missing data detected\")\n",
    "\n",
    "# Display merged data\n",
    "print(f\"\\nFirst 3 subjects (merged data):\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Check for duplicates\n",
    "n_duplicates = df.duplicated(subset=['subject_id']).sum()\n",
    "print(f\"\\n1. Duplicate subjects: {n_duplicates}\")\n",
    "if n_duplicates > 0:\n",
    "    print(\"   ⚠️  Warning: Duplicate subject IDs detected\")\n",
    "    df = df.drop_duplicates(subset=['subject_id'], keep='first')\n",
    "    print(f\"   Removed duplicates, {len(df)} subjects remaining\")\n",
    "\n",
    "# 2. Check age range\n",
    "if 'AGE' in df.columns:\n",
    "    print(f\"\\n2. Age distribution:\")\n",
    "    print(f\"   Range: {df['AGE'].min():.1f} - {df['AGE'].max():.1f} years\")\n",
    "    print(f\"   Mean ± SD: {df['AGE'].mean():.1f} ± {df['AGE'].std():.1f} years\")\n",
    "    print(f\"   Median: {df['AGE'].median():.1f} years\")\n",
    "\n",
    "# 3. Check sex distribution\n",
    "if 'SEX_ID' in df.columns:\n",
    "    print(f\"\\n3. Sex distribution:\")\n",
    "    sex_counts = df['SEX_ID'].value_counts()\n",
    "    for sex_id, count in sex_counts.items():\n",
    "        sex_label = 'Male' if sex_id == 1 else 'Female' if sex_id == 2 else f'Unknown ({sex_id})'\n",
    "        print(f\"   {sex_label}: {count} ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "# 4. Feature distributions\n",
    "print(f\"\\n4. Feature value ranges:\")\n",
    "feature_stats = df[ALL_FEATURES].describe()\n",
    "print(f\"   Min values: {(feature_stats.loc['min'] == 0).sum()} features are all zeros\")\n",
    "print(f\"   Max values: Largest = {feature_stats.loc['max'].max():.2e}\")\n",
    "print(f\"   Missing: {df[ALL_FEATURES].isnull().any().sum()} features have missing values\")\n",
    "\n",
    "# 5. Outlier detection (IQR method)\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 3 * IQR\n",
    "    upper = Q3 + 3 * IQR\n",
    "    return ((series < lower) | (series > upper)).sum()\n",
    "\n",
    "outliers_per_feature = df[ALL_FEATURES].apply(detect_outliers_iqr)\n",
    "features_with_outliers = outliers_per_feature[outliers_per_feature > 0]\n",
    "print(f\"\\n5. Outliers (3×IQR method):\")\n",
    "print(f\"   Features with outliers: {len(features_with_outliers)}\")\n",
    "if len(features_with_outliers) > 0:\n",
    "    print(f\"   Top 5 features by outlier count:\")\n",
    "    print(features_with_outliers.sort_values(ascending=False).head())\n",
    "\n",
    "print(f\"\\n✓ Quality assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive descriptive statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Demographics summary\n",
    "print(\"\\nDemographic Variables:\")\n",
    "demo_summary = df[DEMOGRAPHIC_VARS].describe()\n",
    "display(demo_summary)\n",
    "\n",
    "# Features summary (by category)\n",
    "if len(MORPHOMETRIC_FEATURES) > 0:\n",
    "    print(f\"\\nMorphometric Features (n={len(MORPHOMETRIC_FEATURES)}):\")\n",
    "    display(df[MORPHOMETRIC_FEATURES].describe())\n",
    "\n",
    "if len(TOPOLOGICAL_FEATURES) > 0:\n",
    "    print(f\"\\nTopological Features (n={len(TOPOLOGICAL_FEATURES)}):\")\n",
    "    display(df[TOPOLOGICAL_FEATURES].describe())\n",
    "\n",
    "if len(CURVATURE_FEATURES) > 0:\n",
    "    print(f\"\\nCurvature Features (n={len(CURVATURE_FEATURES)}):\")\n",
    "    display(df[CURVATURE_FEATURES].describe())\n",
    "\n",
    "# Save descriptive statistics\n",
    "all_stats = df[DEMOGRAPHIC_VARS + ALL_FEATURES].describe()\n",
    "all_stats.to_csv(TABLES_DIR / 'descriptive_statistics.csv')\n",
    "print(f\"\\n✓ Descriptive statistics saved to {TABLES_DIR / 'descriptive_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualize Demographic Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Demographic Distributions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Age distribution\n",
    "if 'AGE' in df.columns:\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(df['AGE'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(df['AGE'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"AGE\"].mean():.1f}')\n",
    "    ax.set_xlabel('Age (years)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Age Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sex distribution\n",
    "if 'SEX_ID' in df.columns:\n",
    "    ax = axes[0, 1]\n",
    "    sex_counts = df['SEX_ID'].value_counts()\n",
    "    sex_labels = ['Male' if x==1 else 'Female' for x in sex_counts.index]\n",
    "    bars = ax.bar(sex_labels, sex_counts.values, edgecolor='black', alpha=0.7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Sex Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({100*height/len(df):.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "# BMI distribution\n",
    "if 'BMI' in df.columns:\n",
    "    ax = axes[1, 0]\n",
    "    bmi_data = df['BMI'].dropna()\n",
    "    ax.hist(bmi_data, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(bmi_data.mean(), color='red', linestyle='--', label=f'Mean: {bmi_data.mean():.1f}')\n",
    "    ax.set_xlabel('BMI (kg/m²)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('BMI Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Age by sex\n",
    "if 'AGE' in df.columns and 'SEX_ID' in df.columns:\n",
    "    ax = axes[1, 1]\n",
    "    for sex_id in sorted(df['SEX_ID'].dropna().unique()):\n",
    "        sex_label = 'Male' if sex_id == 1 else 'Female'\n",
    "        age_data = df[df['SEX_ID'] == sex_id]['AGE'].dropna()\n",
    "        ax.hist(age_data, bins=20, alpha=0.6, label=sex_label, edgecolor='black')\n",
    "    ax.set_xlabel('Age (years)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Age Distribution by Sex')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'demographic_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Demographic distributions saved to {FIGURES_DIR / 'demographic_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Demographic Distributions', fontsize=14, fontweight='bold')\n",
    "\n",
    "\n",
    "if 'ETHNIC_ID' in df.columns:\n",
    "    ax = axes[0, 0]\n",
    "    ethnic_counts = df['ETHNIC_ID'].value_counts()\n",
    "    ethnic_labels = [str(x) for x in ethnic_counts.index]\n",
    "    bars = ax.bar(ethnic_labels, ethnic_counts.values, edgecolor='black', alpha=0.7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Ethnic Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({100*height/len(df):.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "\n",
    "if 'MARITAL_ID' in df.columns:\n",
    "    ax = axes[0, 1]\n",
    "    marital_counts = df['MARITAL_ID'].value_counts()\n",
    "    marital_labels = [str(x) for x in marital_counts.index]\n",
    "    bars = ax.bar(marital_labels, marital_counts.values, edgecolor='black', alpha=0.7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Marital Status Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({100*height/len(df):.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "if 'OCCUPATION_ID' in df.columns:\n",
    "    ax = axes[1, 0]\n",
    "    occupation_counts = df['OCCUPATION_ID'].value_counts()\n",
    "    occupation_labels = [str(x) for x in occupation_counts.index]\n",
    "    bars = ax.bar(occupation_labels, occupation_counts.values, edgecolor='black', alpha=0.7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Occupation Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({100*height/len(df):.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "if 'QUALIFICATION_ID' in df.columns:\n",
    "    ax = axes[1, 1]\n",
    "    qualification_counts = df['QUALIFICATION_ID'].value_counts()\n",
    "    qualification_labels = [str(x) for x in qualification_counts.index]\n",
    "    bars = ax.bar(qualification_labels, qualification_counts.values, edgecolor='black', alpha=0.7)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Qualification Distribution')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}\\n({100*height/len(df):.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'demographic_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Demographic distributions saved to {FIGURES_DIR / 'demographic_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Age-Related Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Age Correlations for All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if age is available\n",
    "if 'AGE' not in df.columns:\n",
    "    print(\"⚠️  AGE variable not found. Skipping age-related analyses.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGE-RELATED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate correlations with age for all features\n",
    "    age_correlations = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Remove missing values\n",
    "        valid_data = df[['AGE', feature]].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Calculate Pearson and Spearman correlations\n",
    "        pearson_r, pearson_p = pearsonr(valid_data['AGE'], valid_data[feature])\n",
    "        spearman_r, spearman_p = spearmanr(valid_data['AGE'], valid_data[feature])\n",
    "        \n",
    "        # Linear regression for slope\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(valid_data['AGE'], valid_data[feature])\n",
    "        \n",
    "        age_correlations.append({\n",
    "            'feature': feature,\n",
    "            'category': feature_categories[feature],\n",
    "            'n': len(valid_data),\n",
    "            'pearson_r': pearson_r,\n",
    "            'pearson_p': pearson_p,\n",
    "            'spearman_r': spearman_r,\n",
    "            'spearman_p': spearman_p,\n",
    "            'slope': slope,\n",
    "            'r_squared': r_value**2\n",
    "        })\n",
    "    \n",
    "    age_corr_df = pd.DataFrame(age_correlations)\n",
    "    \n",
    "    # Multiple testing correction\n",
    "    if len(age_corr_df) > 0:\n",
    "        age_corr_df['pearson_p_fdr'] = multipletests(age_corr_df['pearson_p'], method='fdr_bh')[1]\n",
    "        age_corr_df['spearman_p_fdr'] = multipletests(age_corr_df['spearman_p'], method='fdr_bh')[1]\n",
    "        age_corr_df['significant_pearson'] = age_corr_df['pearson_p_fdr'] < 0.05\n",
    "        age_corr_df['significant_spearman'] = age_corr_df['spearman_p_fdr'] < 0.05\n",
    "        \n",
    "        # Sort by absolute correlation strength\n",
    "        age_corr_df = age_corr_df.sort_values('pearson_r', key=abs, ascending=False)\n",
    "        \n",
    "        # Summary\n",
    "        n_sig_pearson = age_corr_df['significant_pearson'].sum()\n",
    "        n_sig_spearman = age_corr_df['significant_spearman'].sum()\n",
    "        \n",
    "        print(f\"\\nAge Correlation Summary:\")\n",
    "        print(f\"  Total features tested: {len(age_corr_df)}\")\n",
    "        print(f\"  Significant (Pearson, FDR<0.05): {n_sig_pearson} ({100*n_sig_pearson/len(age_corr_df):.1f}%)\")\n",
    "        print(f\"  Significant (Spearman, FDR<0.05): {n_sig_spearman} ({100*n_sig_spearman/len(age_corr_df):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTop 10 age-correlated features (by absolute Pearson r):\")\n",
    "        display(age_corr_df.head(10)[['feature', 'category', 'pearson_r', 'pearson_p_fdr', 'r_squared']])\n",
    "        \n",
    "        # Save results\n",
    "        age_corr_df.to_csv(TABLES_DIR / 'age_correlations.csv', index=False)\n",
    "        print(f\"\\n✓ Age correlations saved to {TABLES_DIR / 'age_correlations.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualize Top Age Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AGE' in df.columns and len(age_corr_df) > 0:\n",
    "    # Select top 6 most strongly correlated features\n",
    "    top_features = age_corr_df.head(6)['feature'].tolist()\n",
    "    \n",
    "    # Create scatter plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Top Age-Correlated Vessel Features', fontsize=14, fontweight='bold')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get data\n",
    "        valid_data = df[['AGE', feature]].dropna()\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(valid_data['AGE'], valid_data[feature], alpha=0.5, s=20)\n",
    "        \n",
    "        # Regression line\n",
    "        z = np.polyfit(valid_data['AGE'], valid_data[feature], 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(valid_data['AGE'], p(valid_data['AGE']), 'r--', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Get correlation stats\n",
    "        feat_stats = age_corr_df[age_corr_df['feature'] == feature].iloc[0]\n",
    "        \n",
    "        ax.set_xlabel('Age (years)')\n",
    "        ax.set_ylabel(feature)\n",
    "        ax.set_title(f\"{feature}\\nr={feat_stats['pearson_r']:.3f}, p={feat_stats['pearson_p']:.2e}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'age_correlations_top6.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Top age correlations plot saved to {FIGURES_DIR / 'age_correlations_top6.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Age Correlation Heatmap by Feature Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality figure settings\n",
    "plt.rcParams['figure.dpi'] = 1000\n",
    "plt.rcParams['savefig.dpi'] = 1000\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 25\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 25\n",
    "plt.rcParams['ytick.labelsize'] = 25\n",
    "plt.rcParams['legend.fontsize'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AGE' in df.columns and len(age_corr_df) > 0:\n",
    "    # Create heatmap of correlations grouped by category\n",
    "    fig, ax = plt.subplots(figsize=(10, max(6, len(age_corr_df) * 0.3)))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = age_corr_df[['feature', 'category', 'spearman_r']].copy()\n",
    "    heatmap_data = heatmap_data.sort_values(['category', 'spearman_r'], ascending=[True, False])\n",
    "    \n",
    "    # Create color array for significance\n",
    "    colors = ['red' if sig else 'gray' for sig in age_corr_df.sort_values(['category', 'spearman_r'], ascending=[True, False])['significant_pearson']]\n",
    "    \n",
    "    # Horizontal bar plot\n",
    "    y_pos = np.arange(len(heatmap_data))\n",
    "    ax.barh(y_pos, heatmap_data['spearman_r'], color=colors, alpha=0.6, edgecolor='black')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(heatmap_data['feature'], fontsize=8)\n",
    "    ax.set_xlabel('Spearman Correlation with Age', fontweight='bold')\n",
    "    ax.set_title('Age Correlations for All Features\\n(Red = FDR-significant, Gray = Non-significant)', fontweight='bold')\n",
    "    ax.axvline(0, color='black', linewidth=1)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add category separators\n",
    "    category_changes = heatmap_data['category'].ne(heatmap_data['category'].shift())\n",
    "    for idx in category_changes[category_changes].index[1:]:\n",
    "        ax.axhline(y=y_pos[heatmap_data.index.get_loc(idx)] - 0.5, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'age_correlations_all.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Complete age correlations plot saved to {FIGURES_DIR / 'age_correlations_all.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = regional_df['region_label'].unique() if regional_df is not None and 'region_label' in regional_df.columns else None\n",
    "all_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_df_demo = regional_df.merge(demographics, on='subject_id', how='inner')\n",
    "regional_df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if age is available\n",
    "if 'AGE' not in regional_df_demo.columns:\n",
    "    print(\"⚠️  AGE variable not found. Skipping age-related analyses.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGE-RELATED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate correlations with age for all features\n",
    "    age_correlations_region = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Remove missing values\n",
    "        for region in all_regions:\n",
    "            print(f\"Analyzing feature '{feature}' in region '{region}'...\")\n",
    "            valid_data = regional_df_demo[['AGE', feature, 'region_label']][regional_df_demo['region_label'] == region].dropna()\n",
    "            \n",
    "            if len(valid_data) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Calculate Pearson and Spearman correlations\n",
    "            pearson_r, pearson_p = pearsonr(valid_data['AGE'], valid_data[feature])\n",
    "            spearman_r, spearman_p = spearmanr(valid_data['AGE'], valid_data[feature])\n",
    "            \n",
    "            # Linear regression for slope\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(valid_data['AGE'], valid_data[feature])\n",
    "            \n",
    "            age_correlations_region.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'n': len(valid_data),\n",
    "                'pearson_r': pearson_r,\n",
    "                'pearson_p': pearson_p,\n",
    "                'spearman_r': spearman_r,\n",
    "                'spearman_p': spearman_p,\n",
    "                'slope': slope,\n",
    "                'r_squared': r_value**2,\n",
    "                'region': region\n",
    "            })\n",
    "        \n",
    "        age_corr_df_region = pd.DataFrame(age_correlations_region)\n",
    "        \n",
    "        # Multiple testing correction\n",
    "        if len(age_corr_df_region) > 0:\n",
    "            age_corr_df_region['pearson_p_fdr'] = multipletests(age_corr_df_region['pearson_p'], method='fdr_bh')[1]\n",
    "            age_corr_df_region['spearman_p_fdr'] = multipletests(age_corr_df_region['spearman_p'], method='fdr_bh')[1]\n",
    "            age_corr_df_region['significant_pearson'] = age_corr_df_region['pearson_p_fdr'] < 0.05\n",
    "            age_corr_df_region['significant_spearman'] = age_corr_df_region['spearman_p_fdr'] < 0.05\n",
    "\n",
    "            # Sort by absolute correlation strength\n",
    "            age_corr_df_region = age_corr_df_region.sort_values('pearson_r', key=abs, ascending=False)\n",
    "            \n",
    "            # Summary\n",
    "            n_sig_pearson = age_corr_df_region['significant_pearson'].sum()\n",
    "            n_sig_spearman = age_corr_df_region['significant_spearman'].sum()\n",
    "\n",
    "            print(f\"\\nAge Correlation Summary:\")\n",
    "            print(f\"  Total features tested: {len(age_corr_df_region)}\")\n",
    "            print(f\"  Significant (Pearson, FDR<0.05): {n_sig_pearson} ({100*n_sig_pearson/len(age_corr_df_region):.1f}%)\")\n",
    "            print(f\"  Significant (Spearman, FDR<0.05): {n_sig_spearman} ({100*n_sig_spearman/len(age_corr_df_region):.1f}%)\")\n",
    "\n",
    "            print(f\"\\nTop 10 age-correlated features (by absolute Pearson r):\")\n",
    "            display(age_corr_df_region.head(10)[['feature', 'category', 'pearson_r', 'pearson_p_fdr', 'r_squared']])\n",
    "\n",
    "            # Save results\n",
    "            #age_corr_df.to_csv(TABLES_DIR / 'age_correlations.csv', index=False)\n",
    "            #print(f\"\\n✓ Age correlations saved to {TABLES_DIR / 'age_correlations.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_corr_df_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AGE' in regional_df_demo.columns and len(age_corr_df_region) > 0:\n",
    "    # Create heatmap of correlations grouped by category\n",
    "    for region in all_regions:\n",
    "        fig, ax = plt.subplots(figsize=(10, max(6, len(age_corr_df_region) * 0.01)))\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        heatmap_data = age_corr_df_region[['feature', 'category', 'pearson_r']][age_corr_df_region['region'] == region].copy()\n",
    "        heatmap_data = heatmap_data.sort_values(['category', 'pearson_r'], ascending=[True, False])\n",
    "        \n",
    "        # Create color array for significance\n",
    "        colors = ['red' if sig else 'gray' for sig in age_corr_df_region[age_corr_df_region['region'] == region].sort_values(['category', 'pearson_r'], ascending=[True, False])['significant_pearson']]\n",
    "\n",
    "        # Horizontal bar plot\n",
    "        y_pos = np.arange(len(heatmap_data))\n",
    "        ax.barh(y_pos, heatmap_data['pearson_r'], color=colors, alpha=0.6, edgecolor='black')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(heatmap_data['feature'], fontsize=8)\n",
    "        ax.set_xlabel('Pearson Correlation with Age', fontweight='bold')\n",
    "        ax.set_title(f'Age Correlations for All Features in Region: {region}\\n(Red = FDR-significant, Gray = Non-significant)', fontweight='bold')\n",
    "        ax.axvline(0, color='black', linewidth=1)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add category separators\n",
    "        category_changes = heatmap_data['category'].ne(heatmap_data['category'].shift())\n",
    "        for idx in category_changes[category_changes].index[1:]:\n",
    "            ax.axhline(y=y_pos[heatmap_data.index.get_loc(idx)] - 0.5, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'age_correlations_all.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"✓ Complete age correlations plot saved to {FIGURES_DIR / 'age_correlations_all.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6.4 AGE GROUP BOXPLOTS\n",
    "# ============================================================================\n",
    "\n",
    "if 'AGE' in df.columns and len(age_corr_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGE GROUP ANALYSIS - BOXPLOT VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set matplotlib parameters to prevent memory issues\n",
    "    plt.rcParams['figure.max_open_warning'] = 0\n",
    "    plt.rcParams['agg.path.chunksize'] = 10000\n",
    "    \n",
    "    # Create age groups with bins: 20, 40, 60, 80, 100\n",
    "    age_bins = [20, 40, 60, 80, 100]\n",
    "    age_labels = ['20-39', '40-59', '60-79', '80+']\n",
    "    \n",
    "    # Create quartiles for age groups\n",
    "    age_bins = [df['AGE'].min()-1] + list(df['AGE'].quantile([0.25, 0.5, 0.75])) + [df['AGE'].max()+1]\n",
    "    age_labels = [f\"{int(age_bins[i]+1)}-{int(age_bins[i+1])}\" for i in range(len(age_bins)-1)]\n",
    "    \n",
    "    # Create age group column\n",
    "    df['age_group'] = pd.cut(df['AGE'], bins=age_bins, labels=age_labels, right=False)\n",
    "    \n",
    "    # Display age group distribution\n",
    "    print(f\"\\nAge group distribution:\")\n",
    "    age_group_counts = df['age_group'].value_counts().sort_index()\n",
    "    for group, count in age_group_counts.items():\n",
    "        print(f\"  {group}: {count} subjects ({100*count/len(df[df['AGE'].notna()]):.1f}%)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Statistical Testing Between Age Groups\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STATISTICAL TESTING BETWEEN AGE GROUPS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    anova_results = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Get data for each age group\n",
    "        age_group_data = []\n",
    "        for group in age_labels:\n",
    "            group_data = df[df['age_group'] == group][feature].dropna().values\n",
    "            if len(group_data) >= 5:\n",
    "                age_group_data.append(group_data)\n",
    "        \n",
    "        if len(age_group_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Perform ANOVA\n",
    "        f_stat, anova_p = f_oneway(*age_group_data)\n",
    "        \n",
    "        # Perform Kruskal-Wallis (non-parametric alternative)\n",
    "        h_stat, kw_p = kruskal(*age_group_data)\n",
    "        \n",
    "        # Calculate effect size (eta-squared)\n",
    "        grand_mean = np.mean(np.concatenate(age_group_data))\n",
    "        ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in age_group_data)\n",
    "        ss_total = sum(np.sum((d - grand_mean)**2) for d in age_group_data)\n",
    "        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "        \n",
    "        anova_results.append({\n",
    "            'feature': feature,\n",
    "            'category': feature_categories[feature],\n",
    "            'f_statistic': f_stat,\n",
    "            'anova_pvalue': anova_p,\n",
    "            'kruskal_h': h_stat,\n",
    "            'kw_pvalue': kw_p,\n",
    "            'eta_squared': eta_squared\n",
    "        })\n",
    "    \n",
    "    anova_df = pd.DataFrame(anova_results)\n",
    "    \n",
    "    if len(anova_df) > 0:\n",
    "        # Multiple testing correction\n",
    "        anova_df['anova_p_fdr'] = multipletests(anova_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "        anova_df['kw_p_fdr'] = multipletests(anova_df['kw_pvalue'], method='fdr_bh')[1]\n",
    "        anova_df['significant_anova'] = anova_df['anova_p_fdr'] < 0.05\n",
    "        anova_df['significant_kw'] = anova_df['kw_p_fdr'] < 0.05\n",
    "        \n",
    "        anova_df = anova_df.sort_values('eta_squared', ascending=False)\n",
    "        \n",
    "        n_sig = anova_df['significant_anova'].sum()\n",
    "        \n",
    "        print(f\"\\nANOVA results:\")\n",
    "        print(f\"  Features tested: {len(anova_df)}\")\n",
    "        print(f\"  Significant differences (ANOVA, FDR<0.05): {n_sig} ({100*n_sig/len(anova_df):.1f}%)\")\n",
    "        print(f\"  Significant differences (Kruskal-Wallis, FDR<0.05): {anova_df['significant_kw'].sum()}\")\n",
    "        \n",
    "        print(\"\\nTop 10 features with largest age group effects:\")\n",
    "        display(anova_df.head(10)[['feature', 'category', 'f_statistic', 'eta_squared', \n",
    "                         'anova_p_fdr', 'kw_p_fdr']].round(4))\n",
    "        \n",
    "        # Save results\n",
    "        anova_df.to_csv(TABLES_DIR / 'age_group_anova.csv', index=False)\n",
    "        print(f\"\\n✓ Age group ANOVA results saved to {TABLES_DIR / 'age_group_anova.csv'}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # VISUALIZATION\n",
    "        # ====================================================================\n",
    "        \n",
    "        if n_sig > 0:\n",
    "            top_age_features = anova_df[anova_df['significant_anova']].head(6)['feature'].tolist()\n",
    "            \n",
    "            if len(top_age_features) > 0:\n",
    "                print(f\"\\nVisualizing top {len(top_age_features)} age-correlated features...\")\n",
    "                \n",
    "                # Define consistent color mapping for age groups\n",
    "                unique_ages = age_labels\n",
    "                age_colors = dict(zip(unique_ages, \n",
    "                                     sns.color_palette('YlOrRd', n_colors=len(unique_ages))))\n",
    "                \n",
    "                n_plots = min(6, len(top_age_features))\n",
    "                fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "                fig.suptitle('Top Age Group Differences in Vessel Features',\n",
    "                            fontsize=14, fontweight='bold')\n",
    "                axes = axes.flatten()\n",
    "                \n",
    "                for idx, feature in enumerate(top_age_features[:n_plots]):\n",
    "                    ax = axes[idx]\n",
    "                    \n",
    "                    plot_data = df[['age_group', feature]].dropna()\n",
    "                    \n",
    "                    # Age groups are already in natural order\n",
    "                    age_order = age_labels\n",
    "                    \n",
    "                    # Create color palette in the correct order for this plot\n",
    "                    plot_colors = [age_colors[age] for age in age_order]\n",
    "                    \n",
    "                    # Sample data if too large to prevent memory issues\n",
    "                    if len(plot_data) > 500:\n",
    "                        plot_data_sample = plot_data.sample(n=500, random_state=42)\n",
    "                    else:\n",
    "                        plot_data_sample = plot_data\n",
    "                    \n",
    "                    sns.boxplot(data=plot_data, x='age_group', y=feature, ax=ax,\n",
    "                               order=age_order, palette=plot_colors)\n",
    "                    sns.stripplot(data=plot_data_sample, x='age_group', y=feature, ax=ax,\n",
    "                                 color='black', alpha=0.2, size=2, order=age_order)\n",
    "                    \n",
    "                    feat_stats = anova_df[anova_df['feature'] == feature].iloc[0]\n",
    "                    eta_sq = feat_stats['eta_squared']\n",
    "                    p_val = feat_stats['anova_p_fdr']\n",
    "                    \n",
    "                    if p_val < 0.001:\n",
    "                        sig_stars = '***'\n",
    "                        p_text = 'p < 0.001'\n",
    "                    elif p_val < 0.01:\n",
    "                        sig_stars = '**'\n",
    "                        p_text = f'p = {p_val:.3f}'\n",
    "                    elif p_val < 0.05:\n",
    "                        sig_stars = '*'\n",
    "                        p_text = f'p = {p_val:.3f}'\n",
    "                    else:\n",
    "                        sig_stars = 'ns'\n",
    "                        p_text = f'p = {p_val:.2f}'\n",
    "                    \n",
    "                    clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "                    \n",
    "                    ax.set_title(f\"{clean_feature} {sig_stars}\\nη² = {eta_sq:.3f}, {p_text}\",\n",
    "                                fontsize=9)\n",
    "                    ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "                    ax.set_xlabel('Age Group (years)', fontsize=9)\n",
    "                    ax.tick_params(axis='x', labelsize=8)\n",
    "                    ax.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for idx in range(n_plots, 6):\n",
    "                    axes[idx].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(FIGURES_DIR / 'age_groups_boxplots_top6.png', dpi=150, bbox_inches='tight')\n",
    "                print(\"✓ Age group boxplots saved\")\n",
    "                plt.show()\n",
    "                plt.close('all')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ AGE GROUP ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n⚠️  Skipping age group analysis - no age correlation data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sex-Based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SEX_ID' not in df.columns:\n",
    "    print(\"⚠️  SEX_ID variable not found. Skipping sex-based analyses.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEX-BASED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sex comparison for all features\n",
    "    sex_comparisons = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Get data for each sex\n",
    "        male_data = df[(df['SEX_ID'] == 1) & df[feature].notna()][feature]\n",
    "        female_data = df[(df['SEX_ID'] == 2) & df[feature].notna()][feature]\n",
    "        \n",
    "        if len(male_data) < 5 or len(female_data) < 5:\n",
    "            continue\n",
    "        \n",
    "        # T-test and Mann-Whitney U test\n",
    "        t_stat, t_pval = ttest_ind(male_data, female_data)\n",
    "        u_stat, u_pval = mannwhitneyu(male_data, female_data, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(male_data)-1)*male_data.std()**2 + (len(female_data)-1)*female_data.std()**2) / (len(male_data)+len(female_data)-2))\n",
    "        cohens_d = (male_data.mean() - female_data.mean()) / pooled_std\n",
    "        \n",
    "        sex_comparisons.append({\n",
    "            'feature': feature,\n",
    "            'category': feature_categories[feature],\n",
    "            'n_male': len(male_data),\n",
    "            'n_female': len(female_data),\n",
    "            'male_mean': male_data.mean(),\n",
    "            'female_mean': female_data.mean(),\n",
    "            'male_std': male_data.std(),\n",
    "            'female_std': female_data.std(),\n",
    "            't_statistic': t_stat,\n",
    "            't_pvalue': t_pval,\n",
    "            'mwu_pvalue': u_pval,\n",
    "            'cohens_d': cohens_d,\n",
    "            'percent_diff': 100 * (male_data.mean() - female_data.mean()) / female_data.mean()\n",
    "        })\n",
    "    \n",
    "    sex_comp_df = pd.DataFrame(sex_comparisons)\n",
    "    \n",
    "    if len(sex_comp_df) > 0:\n",
    "        # Multiple testing correction\n",
    "        sex_comp_df['t_pvalue_fdr'] = multipletests(sex_comp_df['t_pvalue'], method='fdr_bh')[1]\n",
    "        sex_comp_df['mwu_pvalue_fdr'] = multipletests(sex_comp_df['mwu_pvalue'], method='fdr_bh')[1]\n",
    "        sex_comp_df['significant_ttest'] = sex_comp_df['t_pvalue_fdr'] < 0.05\n",
    "        sex_comp_df['significant_mwu'] = sex_comp_df['mwu_pvalue_fdr'] < 0.05\n",
    "        \n",
    "        # Sort by effect size\n",
    "        sex_comp_df = sex_comp_df.sort_values('cohens_d', key=abs, ascending=False)\n",
    "        \n",
    "        # Summary\n",
    "        n_sig = sex_comp_df['significant_ttest'].sum()\n",
    "        print(f\"\\nSex Comparison Summary:\")\n",
    "        print(f\"  Total features tested: {len(sex_comp_df)}\")\n",
    "        print(f\"  Significant differences (t-test, FDR<0.05): {n_sig} ({100*n_sig/len(sex_comp_df):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTop 10 sex differences (by absolute Cohen's d):\")\n",
    "        display(sex_comp_df.head(10)[['feature', 'category', 'male_mean', 'female_mean', 'cohens_d', 't_pvalue_fdr']])\n",
    "        \n",
    "        # Save results\n",
    "        sex_comp_df.to_csv(TABLES_DIR / 'sex_comparisons.csv', index=False)\n",
    "        print(f\"\\n✓ Sex comparisons saved to {TABLES_DIR / 'sex_comparisons.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Visualize Sex Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SEX_ID' in df.columns and len(sex_comp_df) > 0:\n",
    "    # Select top 6 features with largest effect sizes\n",
    "    top_sex_features = sex_comp_df.head(6)['feature'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Top Sex Differences in Vessel Features', fontsize=14, fontweight='bold')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_sex_features):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Prepare data\n",
    "        plot_data = df[['SEX_ID', feature]].dropna()\n",
    "        plot_data['Sex'] = plot_data['SEX_ID'].map({1: 'Male', 2: 'Female'})\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(data=plot_data, x='Sex', y=feature, ax=ax, palette='Set2')\n",
    "        sns.stripplot(data=plot_data, x='Sex', y=feature, ax=ax, color='black', alpha=0.3, size=3)\n",
    "        \n",
    "        # Get stats\n",
    "        feat_stats = sex_comp_df[sex_comp_df['feature'] == feature].iloc[0]\n",
    "        \n",
    "        # Determine significance asterisks\n",
    "        p_val = feat_stats['t_pvalue']\n",
    "        if p_val < 0.001:\n",
    "            sig_marker = '***'\n",
    "        elif p_val < 0.01:\n",
    "            sig_marker = '**'\n",
    "        elif p_val < 0.05:\n",
    "            sig_marker = '*'\n",
    "        else:\n",
    "            sig_marker = 'ns'\n",
    "        \n",
    "        ax.set_title(f\"{feature}\\nd={feat_stats['cohens_d']:.3f}, p={p_val:.2e} {sig_marker}\")\n",
    "        ax.set_ylabel(feature)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'sex_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Sex differences plot saved to {FIGURES_DIR / 'sex_differences_top6.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. BALANCED Sex-Related Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def balance_sex_distribution(df, age_bin_width=5):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset with equal male/female counts in each age bin.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe with 'AGE' and 'SEX_ID (1=m, 2=f)' columns\n",
    "    age_bin_width : int\n",
    "        Width of age bins in years (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_balanced : pandas.DataFrame\n",
    "        Balanced dataframe with equal male/female representation per age bin\n",
    "    balance_report : pandas.DataFrame\n",
    "        Report showing original and balanced counts per age bin\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the correct sex column name\n",
    "    sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df.columns else 'SEX_ID'\n",
    "    \n",
    "    # Create age bins\n",
    "    min_age = df['AGE'].min()\n",
    "    max_age = df['AGE'].max()\n",
    "    bins = range(int(min_age), int(max_age) + age_bin_width, age_bin_width)\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy['age_bin'] = pd.cut(df_copy['AGE'], bins=bins, include_lowest=True, right=False)\n",
    "    \n",
    "    # Initialize list to store balanced samples\n",
    "    balanced_samples = []\n",
    "    balance_info = []\n",
    "    \n",
    "    # Process each age bin\n",
    "    for age_bin in df_copy['age_bin'].dropna().unique():\n",
    "        bin_data = df_copy[df_copy['age_bin'] == age_bin]\n",
    "        \n",
    "        # Count males and females in this bin\n",
    "        males = bin_data[bin_data[sex_col] == 1]\n",
    "        females = bin_data[bin_data[sex_col] == 2]\n",
    "        \n",
    "        n_males = len(males)\n",
    "        n_females = len(females)\n",
    "        \n",
    "        # Take minimum count\n",
    "        min_count = min(n_males, n_females)\n",
    "        \n",
    "        if min_count > 0:\n",
    "            # Randomly sample min_count from each sex\n",
    "            males_sampled = males.sample(n=min_count, random_state=42)\n",
    "            females_sampled = females.sample(n=min_count, random_state=42)\n",
    "            \n",
    "            balanced_samples.append(males_sampled)\n",
    "            balanced_samples.append(females_sampled)\n",
    "            \n",
    "            balance_info.append({\n",
    "                'age_bin': str(age_bin),\n",
    "                'original_male': n_males,\n",
    "                'original_female': n_females,\n",
    "                'balanced_male': min_count,\n",
    "                'balanced_female': min_count,\n",
    "                'total_kept': 2 * min_count,\n",
    "                'total_removed': (n_males - min_count) + (n_females - min_count)\n",
    "            })\n",
    "    \n",
    "    # Combine all balanced samples\n",
    "    df_balanced = pd.concat(balanced_samples, ignore_index=True)\n",
    "    df_balanced = df_balanced.drop('age_bin', axis=1)\n",
    "    df_balanced = df_balanced.sort_values('AGE').reset_index(drop=True)\n",
    "    \n",
    "    # Create balance report\n",
    "    balance_report = pd.DataFrame(balance_info)\n",
    "    \n",
    "    return df_balanced, balance_report\n",
    "\n",
    "\n",
    "def plot_balanced_comparison(df_original, df_balanced, age_bin_width=5):\n",
    "    \"\"\"\n",
    "    Visualize the original vs balanced sex distribution by age.\n",
    "    \"\"\"\n",
    "    sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df_original.columns else 'SEX_ID'\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Create age bins for visualization\n",
    "    min_age = int(df_original['AGE'].min())\n",
    "    max_age = int(df_original['AGE'].max())\n",
    "    bins = range(min_age, max_age + age_bin_width, age_bin_width)\n",
    "    \n",
    "    for idx, (df_plot, title) in enumerate([(df_original, 'Original Distribution'),\n",
    "                                              (df_balanced, 'Balanced Distribution')]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Separate male and female data\n",
    "        males = df_plot[df_plot[sex_col] == 1]['AGE']\n",
    "        females = df_plot[df_plot[sex_col] == 2]['AGE']\n",
    "        \n",
    "        # Create stacked histogram\n",
    "        ax.hist([males, females], bins=bins, label=['Male', 'Female'],\n",
    "                color=['#ff7f0e', '#ff69b4'], alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Age (years)', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add sample size annotation\n",
    "        n_total = len(df_plot)\n",
    "        n_males = len(males)\n",
    "        n_females = len(females)\n",
    "        ax.text(0.02, 0.98, f'N = {n_total}\\nMale: {n_males}\\nFemale: {n_females}',\n",
    "                transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('age_sex_distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: age_sex_distribution_comparison.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "# Balance the dataset\n",
    "df_balanced, balance_report = balance_sex_distribution(df, age_bin_width=5)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEX DISTRIBUTION BALANCING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOriginal dataset: {len(df)} subjects\")\n",
    "print(f\"Balanced dataset: {len(df_balanced)} subjects\")\n",
    "print(f\"Subjects removed: {len(df) - len(df_balanced)} ({(len(df) - len(df_balanced))/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-AGE-BIN BALANCING DETAILS\")\n",
    "print(\"=\" * 80)\n",
    "print(balance_report.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df.columns else 'SEX_ID'\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BALANCED DATASET CHARACTERISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSex distribution:\")\n",
    "print(f\"  Male:   {(df_balanced[sex_col] == 1).sum()} ({(df_balanced[sex_col] == 1).sum()/len(df_balanced)*100:.1f}%)\")\n",
    "print(f\"  Female: {(df_balanced[sex_col] == 2).sum()} ({(df_balanced[sex_col] == 2).sum()/len(df_balanced)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(f\"  Original - Mean: {df['AGE'].mean():.1f}, SD: {df['AGE'].std():.1f}, Range: [{df['AGE'].min():.0f}, {df['AGE'].max():.0f}]\")\n",
    "print(f\"  Balanced - Mean: {df_balanced['AGE'].mean():.1f}, SD: {df_balanced['AGE'].std():.1f}, Range: [{df_balanced['AGE'].min():.0f}, {df_balanced['AGE'].max():.0f}]\")\n",
    "\n",
    "# Visualize comparison\n",
    "plot_balanced_comparison(df, df_balanced, age_bin_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SEX_ID' not in df_balanced.columns:\n",
    "    print(\"⚠️  SEX_ID variable not found. Skipping sex-based analyses.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEX-BASED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sex comparison for all features\n",
    "    sex_comparisons = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Get data for each sex\n",
    "        male_data = df_balanced[(df_balanced['SEX_ID'] == 1) & df_balanced[feature].notna()][feature]\n",
    "        female_data = df_balanced[(df_balanced['SEX_ID'] == 2) & df_balanced[feature].notna()][feature]\n",
    "        \n",
    "        if len(male_data) < 5 or len(female_data) < 5:\n",
    "            continue\n",
    "        \n",
    "        # T-test and Mann-Whitney U test\n",
    "        t_stat, t_pval = ttest_ind(male_data, female_data)\n",
    "        u_stat, u_pval = mannwhitneyu(male_data, female_data, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(male_data)-1)*male_data.std()**2 + (len(female_data)-1)*female_data.std()**2) / (len(male_data)+len(female_data)-2))\n",
    "        cohens_d = (male_data.mean() - female_data.mean()) / pooled_std\n",
    "        \n",
    "        sex_comparisons.append({\n",
    "            'feature': feature,\n",
    "            'category': feature_categories[feature],\n",
    "            'n_male': len(male_data),\n",
    "            'n_female': len(female_data),\n",
    "            'male_mean': male_data.mean(),\n",
    "            'female_mean': female_data.mean(),\n",
    "            'male_std': male_data.std(),\n",
    "            'female_std': female_data.std(),\n",
    "            't_statistic': t_stat,\n",
    "            't_pvalue': t_pval,\n",
    "            'mwu_pvalue': u_pval,\n",
    "            'cohens_d': cohens_d,\n",
    "            'percent_diff': 100 * (male_data.mean() - female_data.mean()) / female_data.mean()\n",
    "        })\n",
    "    \n",
    "    sex_comp_df = pd.DataFrame(sex_comparisons)\n",
    "    \n",
    "    if len(sex_comp_df) > 0:\n",
    "        # Multiple testing correction\n",
    "        sex_comp_df['t_pvalue_fdr'] = multipletests(sex_comp_df['t_pvalue'], method='fdr_bh')[1]\n",
    "        sex_comp_df['mwu_pvalue_fdr'] = multipletests(sex_comp_df['mwu_pvalue'], method='fdr_bh')[1]\n",
    "        sex_comp_df['significant_ttest'] = sex_comp_df['t_pvalue_fdr'] < 0.05\n",
    "        sex_comp_df['significant_mwu'] = sex_comp_df['mwu_pvalue_fdr'] < 0.05\n",
    "        \n",
    "        # Sort by effect size\n",
    "        sex_comp_df = sex_comp_df.sort_values('cohens_d', key=abs, ascending=False)\n",
    "        \n",
    "        # Summary\n",
    "        n_sig = sex_comp_df['significant_ttest'].sum()\n",
    "        print(f\"\\nSex Comparison Summary:\")\n",
    "        print(f\"  Total features tested: {len(sex_comp_df)}\")\n",
    "        print(f\"  Significant differences (t-test, FDR<0.05): {n_sig} ({100*n_sig/len(sex_comp_df):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTop 10 sex differences (by absolute Cohen's d):\")\n",
    "        display(sex_comp_df.head(10)[['feature', 'category', 'male_mean', 'female_mean', 'cohens_d', 't_pvalue_fdr']])\n",
    "        \n",
    "        # Save results\n",
    "        sex_comp_df.to_csv(TABLES_DIR / 'sex_comparisons.csv', index=False)\n",
    "        print(f\"\\n✓ Sex comparisons saved to {TABLES_DIR / 'sex_comparisons.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'SEX_ID' in df_balanced.columns and len(sex_comp_df) > 0:\n",
    "    # Select top 6 features with largest effect sizes\n",
    "    top_sex_features = sex_comp_df.head(6)['feature'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Top Sex Differences in Vessel Features', fontsize=14, fontweight='bold')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    def get_significance_stars(p_value):\n",
    "        \"\"\"Convert p-value to significance stars\"\"\"\n",
    "        if p_value < 0.001:\n",
    "            return '***'\n",
    "        elif p_value < 0.01:\n",
    "            return '**'\n",
    "        elif p_value < 0.05:\n",
    "            return '*'\n",
    "        else:\n",
    "            return 'ns'\n",
    "    \n",
    "    for idx, feature in enumerate(top_sex_features):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Prepare data\n",
    "        plot_data = df_balanced[['SEX_ID', feature]].dropna()\n",
    "        plot_data['Sex'] = plot_data['SEX_ID'].map({1: 'Male', 2: 'Female'})\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(data=plot_data, x='Sex', y=feature, ax=ax, palette='Set2')\n",
    "        sns.stripplot(data=plot_data, x='Sex', y=feature, ax=ax, color='black', alpha=0.3, size=3)\n",
    "        \n",
    "        # Get stats\n",
    "        feat_stats = sex_comp_df[sex_comp_df['feature'] == feature].iloc[0]\n",
    "        p_value = feat_stats['t_pvalue']\n",
    "        cohens_d = feat_stats['cohens_d']\n",
    "        \n",
    "        # Get significance stars\n",
    "        sig_stars = get_significance_stars(p_value)\n",
    "        sig_indicator = f\" {sig_stars}\" if sig_stars != 'ns' else \" (ns)\"\n",
    "        \n",
    "        # Format p-value\n",
    "        if p_value < 0.001:\n",
    "            p_text = 'p < 0.001'\n",
    "        elif p_value < 0.01:\n",
    "            p_text = f'p = {p_value:.3f}'\n",
    "        else:\n",
    "            p_text = f'p = {p_value:.2f}'\n",
    "        \n",
    "        # Clean feature name for display\n",
    "        clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "        \n",
    "        # Title with significance indicator\n",
    "        ax.set_title(f\"{clean_feature}{sig_indicator}\\nd = {cohens_d:.3f}, {p_text}\", \n",
    "                    fontsize=10)\n",
    "        ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "        ax.set_xlabel('')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'sex_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Sex differences plot saved to {FIGURES_DIR / 'sex_differences_top6.png'}\")\n",
    "    \n",
    "    # Print legend for significance levels\n",
    "    print(\"\\nSignificance levels:\")\n",
    "    print(\"  *** p < 0.001\")\n",
    "    print(\"  **  p < 0.01\")\n",
    "    print(\"  *   p < 0.05\")\n",
    "    print(\"  ns  not significant (p ≥ 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Anthropometric Correlations (Height, Weight, BMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which anthropometric variables are available\n",
    "anthro_vars = [v for v in ['HEIGHT', 'WEIGHT', 'BMI'] if v in df.columns]\n",
    "\n",
    "if len(anthro_vars) == 0:\n",
    "    print(\"⚠️  No anthropometric variables found. Skipping anthropometric analyses.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANTHROPOMETRIC CORRELATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nAvailable anthropometric variables: {anthro_vars}\")\n",
    "    \n",
    "    # Calculate correlations for each anthropometric variable\n",
    "    anthro_results = {}\n",
    "    \n",
    "    for anthro_var in anthro_vars:\n",
    "        correlations = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            valid_data = df[[anthro_var, feature]].dropna()\n",
    "            \n",
    "            if len(valid_data) < 10:\n",
    "                continue\n",
    "            \n",
    "            r, p = pearsonr(valid_data[anthro_var], valid_data[feature])\n",
    "            \n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'n': len(valid_data),\n",
    "                'correlation': r,\n",
    "                'pvalue': p\n",
    "            })\n",
    "        \n",
    "        corr_df = pd.DataFrame(correlations)\n",
    "        \n",
    "        if len(corr_df) > 0:\n",
    "            # Multiple testing correction\n",
    "            corr_df['pvalue_fdr'] = multipletests(corr_df['pvalue'], method='fdr_bh')[1]\n",
    "            corr_df['significant'] = corr_df['pvalue_fdr'] < 0.05\n",
    "            corr_df = corr_df.sort_values('correlation', key=abs, ascending=False)\n",
    "            anthro_results[anthro_var] = corr_df\n",
    "            \n",
    "            print(f\"\\n{anthro_var} Correlations:\")\n",
    "            print(f\"  Significant correlations (FDR<0.05): {corr_df['significant'].sum()}\")\n",
    "            print(f\"  Top 5 correlations:\")\n",
    "            display(corr_df.head(5)[['feature', 'category', 'correlation', 'pvalue_fdr']])\n",
    "            \n",
    "            # Save results\n",
    "            corr_df.to_csv(TABLES_DIR / f'{anthro_var.lower()}_correlations.csv', index=False)\n",
    "    \n",
    "    if len(anthro_results) > 0:\n",
    "        print(f\"\\n✓ Anthropometric correlations saved to {TABLES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANTHROPOMETRIC BOXPLOTS BY GROUPS\n",
    "# ============================================================================\n",
    "\n",
    "if len(anthro_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANTHROPOMETRIC BOXPLOT VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set matplotlib parameters to prevent memory issues\n",
    "    plt.rcParams['figure.max_open_warning'] = 0\n",
    "    plt.rcParams['agg.path.chunksize'] = 10000\n",
    "    \n",
    "    # For each anthropometric variable, create boxplots by groups\n",
    "    for anthro_var in anthro_vars:\n",
    "        if anthro_var not in anthro_results:\n",
    "            continue\n",
    "            \n",
    "        corr_df = anthro_results[anthro_var]\n",
    "        \n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "        print(f\"{anthro_var} GROUP ANALYSIS\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Create groups based on the anthropometric variable\n",
    "        if anthro_var == 'HEIGHT':\n",
    "            # Height groups (in cm)\n",
    "            anthro_bins = [140, 160, 170, 180, 200]\n",
    "            anthro_labels = ['<160cm', '160-169cm', '170-179cm', '≥180cm']\n",
    "            group_name = 'Height Group'\n",
    "        elif anthro_var == 'WEIGHT':\n",
    "            # Weight groups (in kg)\n",
    "            anthro_bins = [40, 60, 75, 90, 150]\n",
    "            anthro_labels = ['<60kg', '60-74kg', '75-89kg', '≥90kg']\n",
    "            group_name = 'Weight Group'\n",
    "        elif anthro_var == 'BMI':\n",
    "            # BMI categories (WHO classification)\n",
    "            #anthro_bins = [10, 18.5, 25, 30, 50]\n",
    "            #anthro_labels = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "            anthro_bins = [18.5, 25, 30, 50]\n",
    "            anthro_labels = ['Normal', 'Overweight', 'Obese']\n",
    "            group_name = 'BMI Category'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Create group column\n",
    "        df[f'{anthro_var}_group'] = pd.cut(df[anthro_var], bins=anthro_bins, \n",
    "                                            labels=anthro_labels, right=False)\n",
    "        \n",
    "        # Display group distribution\n",
    "        group_counts = df[f'{anthro_var}_group'].value_counts().sort_index()\n",
    "        print(f\"\\n{group_name} distribution:\")\n",
    "        for group, count in group_counts.items():\n",
    "            print(f\"  {group}: {count} subjects ({100*count/len(df[df[anthro_var].notna()]):.1f}%)\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Statistical Testing Between Groups\n",
    "        # ====================================================================\n",
    "        print(f\"\\nStatistical testing between {anthro_var} groups...\")\n",
    "        \n",
    "        anova_results = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            group_data = []\n",
    "            for group in anthro_labels:\n",
    "                data = df[df[f'{anthro_var}_group'] == group][feature].dropna().values\n",
    "                if len(data) >= 5:\n",
    "                    group_data.append(data)\n",
    "            \n",
    "            if len(group_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Perform ANOVA\n",
    "            f_stat, anova_p = f_oneway(*group_data)\n",
    "            \n",
    "            # Perform Kruskal-Wallis\n",
    "            h_stat, kw_p = kruskal(*group_data)\n",
    "            \n",
    "            # Calculate effect size (eta-squared)\n",
    "            grand_mean = np.mean(np.concatenate(group_data))\n",
    "            ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in group_data)\n",
    "            ss_total = sum(np.sum((d - grand_mean)**2) for d in group_data)\n",
    "            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "            \n",
    "            anova_results.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'f_statistic': f_stat,\n",
    "                'anova_pvalue': anova_p,\n",
    "                'kruskal_h': h_stat,\n",
    "                'kw_pvalue': kw_p,\n",
    "                'eta_squared': eta_squared\n",
    "            })\n",
    "        \n",
    "        anova_df = pd.DataFrame(anova_results)\n",
    "        \n",
    "        if len(anova_df) > 0:\n",
    "            # Multiple testing correction\n",
    "            anova_df['anova_p_fdr'] = multipletests(anova_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "            anova_df['kw_p_fdr'] = multipletests(anova_df['kw_pvalue'], method='fdr_bh')[1]\n",
    "            anova_df['significant_anova'] = anova_df['anova_p_fdr'] < 0.05\n",
    "            anova_df['significant_kw'] = anova_df['kw_p_fdr'] < 0.05\n",
    "            \n",
    "            anova_df = anova_df.sort_values('eta_squared', ascending=False)\n",
    "            \n",
    "            n_sig = anova_df['significant_anova'].sum()\n",
    "            \n",
    "            print(f\"\\nANOVA results:\")\n",
    "            print(f\"  Features tested: {len(anova_df)}\")\n",
    "            print(f\"  Significant differences (ANOVA, FDR<0.05): {n_sig} ({100*n_sig/len(anova_df):.1f}%)\")\n",
    "            print(f\"  Significant differences (Kruskal-Wallis, FDR<0.05): {anova_df['significant_kw'].sum()}\")\n",
    "            \n",
    "            print(f\"\\nTop 10 features with largest {anthro_var} group effects:\")\n",
    "            display(anova_df.head(10)[['feature', 'category', 'f_statistic', 'eta_squared', \n",
    "                             'anova_p_fdr', 'kw_p_fdr']].round(4))\n",
    "            \n",
    "            # Save results\n",
    "            anova_df.to_csv(TABLES_DIR / f'{anthro_var.lower()}_group_anova.csv', index=False)\n",
    "            print(f\"\\n✓ {anthro_var} group ANOVA results saved to {TABLES_DIR / f'{anthro_var.lower()}_group_anova.csv'}\")\n",
    "            \n",
    "            # ================================================================\n",
    "            # VISUALIZATION\n",
    "            # ================================================================\n",
    "            \n",
    "            if n_sig > 0:\n",
    "                top_features = anova_df[anova_df['significant_anova']].head(6)['feature'].tolist()\n",
    "                \n",
    "                if len(top_features) > 0:\n",
    "                    print(f\"\\nVisualizing top {len(top_features)} {anthro_var}-related features...\")\n",
    "                    \n",
    "                    # Define consistent color mapping\n",
    "                    unique_groups = anthro_labels\n",
    "                    if anthro_var == 'BMI':\n",
    "                        # Use meaningful colors for BMI\n",
    "                        group_colors = dict(zip(unique_groups, ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']))\n",
    "                    elif anthro_var == 'HEIGHT':\n",
    "                        group_colors = dict(zip(unique_groups, \n",
    "                                              sns.color_palette('Blues', n_colors=len(unique_groups))))\n",
    "                    else:  # WEIGHT\n",
    "                        group_colors = dict(zip(unique_groups, \n",
    "                                              sns.color_palette('Oranges', n_colors=len(unique_groups))))\n",
    "                    \n",
    "                    n_plots = min(6, len(top_features))\n",
    "                    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "                    fig.suptitle(f'Top {group_name} Differences in Vessel Features',\n",
    "                                fontsize=14, fontweight='bold')\n",
    "                    axes = axes.flatten()\n",
    "                    \n",
    "                    for idx, feature in enumerate(top_features[:n_plots]):\n",
    "                        ax = axes[idx]\n",
    "                        \n",
    "                        plot_data = df[[f'{anthro_var}_group', feature]].dropna()\n",
    "                        \n",
    "                        # Order groups naturally\n",
    "                        group_order = anthro_labels\n",
    "                        \n",
    "                        # Create color palette in the correct order for this plot\n",
    "                        plot_colors = [group_colors[grp] for grp in group_order]\n",
    "                        \n",
    "                        # Sample data if too large to prevent memory issues\n",
    "                        if len(plot_data) > 500:\n",
    "                            plot_data_sample = plot_data.sample(n=500, random_state=42)\n",
    "                        else:\n",
    "                            plot_data_sample = plot_data\n",
    "                        \n",
    "                        sns.boxplot(data=plot_data, x=f'{anthro_var}_group', y=feature, ax=ax,\n",
    "                                   order=group_order, palette=plot_colors)\n",
    "                        sns.stripplot(data=plot_data_sample, x=f'{anthro_var}_group', y=feature, ax=ax,\n",
    "                                     order=group_order, color='black', alpha=0.2, size=2)\n",
    "                        \n",
    "                        feat_stats = anova_df[anova_df['feature'] == feature].iloc[0]\n",
    "                        eta_sq = feat_stats['eta_squared']\n",
    "                        p_val = feat_stats['anova_p_fdr']\n",
    "                        \n",
    "                        if p_val < 0.001:\n",
    "                            sig_stars = '***'\n",
    "                            p_text = 'p < 0.001'\n",
    "                        elif p_val < 0.01:\n",
    "                            sig_stars = '**'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        elif p_val < 0.05:\n",
    "                            sig_stars = '*'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        else:\n",
    "                            sig_stars = 'ns'\n",
    "                            p_text = f'p = {p_val:.2f}'\n",
    "                        \n",
    "                        clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "                        \n",
    "                        ax.set_title(f\"{clean_feature} {sig_stars}\\nη² = {eta_sq:.3f}, {p_text}\",\n",
    "                                    fontsize=9)\n",
    "                        ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "                        ax.set_xlabel(group_name, fontsize=9)\n",
    "                        ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "                        ax.grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    for idx in range(n_plots, 6):\n",
    "                        axes[idx].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(FIGURES_DIR / f'{anthro_var.lower()}_groups_boxplots.png', \n",
    "                               dpi=150, bbox_inches='tight')\n",
    "                    print(f\"✓ {anthro_var} group boxplots saved\")\n",
    "                    plt.show()\n",
    "                    plt.close('all')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ ANTHROPOMETRIC BOXPLOT ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGE DISTRIBUTION ACROSS BMI CATEGORIES\n",
    "# ============================================================================\n",
    "\n",
    "if 'BMI' in df.columns and 'BMI_group' in df.columns and 'AGE' in df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGE DISTRIBUTION ACROSS BMI CATEGORIES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set matplotlib parameters\n",
    "    plt.rcParams['figure.max_open_warning'] = 0\n",
    "    plt.rcParams['agg.path.chunksize'] = 10000\n",
    "    \n",
    "    # Get data\n",
    "    plot_data = df[['BMI_group', 'AGE']].dropna()\n",
    "    \n",
    "    print(f\"\\nAnalyzing age distribution across BMI categories...\")\n",
    "    print(f\"Total subjects with both BMI and age data: {len(plot_data)}\")\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    bmi_age_stats = plot_data.groupby('BMI_group')['AGE'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "    print(\"\\nAge statistics by BMI category:\")\n",
    "    display(bmi_age_stats.round(2))\n",
    "    \n",
    "    # Statistical test - ANOVA for age differences across BMI groups\n",
    "    bmi_categories = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "    age_by_bmi = [plot_data[plot_data['BMI_group'] == cat]['AGE'].dropna().values \n",
    "                  for cat in bmi_categories]\n",
    "    age_by_bmi = [d for d in age_by_bmi if len(d) >= 5]\n",
    "    \n",
    "    if len(age_by_bmi) >= 2:\n",
    "        f_stat, anova_p = f_oneway(*age_by_bmi)\n",
    "        h_stat, kw_p = kruskal(*age_by_bmi)\n",
    "        \n",
    "        # Calculate effect size\n",
    "        grand_mean = np.mean(np.concatenate(age_by_bmi))\n",
    "        ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in age_by_bmi)\n",
    "        ss_total = sum(np.sum((d - grand_mean)**2) for d in age_by_bmi)\n",
    "        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "        \n",
    "        print(f\"\\nStatistical tests:\")\n",
    "        print(f\"  ANOVA: F = {f_stat:.3f}, p = {anova_p:.4f}\")\n",
    "        print(f\"  Kruskal-Wallis: H = {h_stat:.3f}, p = {kw_p:.4f}\")\n",
    "        print(f\"  Effect size (η²): {eta_squared:.3f}\")\n",
    "        \n",
    "        if anova_p < 0.05:\n",
    "            print(f\"  → Significant age differences across BMI categories\")\n",
    "        else:\n",
    "            print(f\"  → No significant age differences across BMI categories\")\n",
    "    \n",
    "    # Visualization\n",
    "    print(\"\\nGenerating age distribution visualizations...\")\n",
    "    \n",
    "    # Define BMI colors (same as before)\n",
    "    bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "    bmi_colors = dict(zip(bmi_labels, ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Age Distribution Across BMI Categories', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Box plot\n",
    "    ax = axes[0]\n",
    "    plot_colors = [bmi_colors[cat] for cat in bmi_labels]\n",
    "    \n",
    "    # Sample if too many points\n",
    "    if len(plot_data) > 500:\n",
    "        plot_data_sample = plot_data.sample(n=500, random_state=42)\n",
    "    else:\n",
    "        plot_data_sample = plot_data\n",
    "    \n",
    "    sns.boxplot(data=plot_data, x='BMI_group', y='AGE', ax=ax,\n",
    "               order=bmi_labels, palette=plot_colors)\n",
    "    sns.stripplot(data=plot_data_sample, x='BMI_group', y='AGE', ax=ax,\n",
    "                 order=bmi_labels, color='black', alpha=0.2, size=3)\n",
    "    \n",
    "    # Add significance annotation if significant\n",
    "    if 'anova_p' in locals() and anova_p < 0.05:\n",
    "        if anova_p < 0.001:\n",
    "            sig_text = '***'\n",
    "        elif anova_p < 0.01:\n",
    "            sig_text = '**'\n",
    "        elif anova_p < 0.05:\n",
    "            sig_text = '*'\n",
    "        ax.text(0.5, 0.98, f'ANOVA: p = {anova_p:.4f} {sig_text}', \n",
    "               ha='center', va='top', transform=ax.transAxes,\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('BMI Category', fontsize=11)\n",
    "    ax.set_ylabel('Age (years)', fontsize=11)\n",
    "    ax.set_title('Age by BMI Category', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Violin plot with quartiles\n",
    "    ax = axes[1]\n",
    "    \n",
    "    parts = ax.violinplot([plot_data[plot_data['BMI_group'] == cat]['AGE'].dropna().values \n",
    "                           for cat in bmi_labels],\n",
    "                          positions=range(len(bmi_labels)),\n",
    "                          showmeans=True, showmedians=True, showextrema=True)\n",
    "    \n",
    "    # Color the violins\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(plot_colors[i])\n",
    "        pc.set_alpha(0.7)\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_linewidth(1)\n",
    "    \n",
    "    # Style the lines\n",
    "    for partname in ('cbars', 'cmins', 'cmaxes', 'cmedians', 'cmeans'):\n",
    "        if partname in parts:\n",
    "            vp = parts[partname]\n",
    "            vp.set_edgecolor('black')\n",
    "            vp.set_linewidth(1.5)\n",
    "    \n",
    "    ax.set_xticks(range(len(bmi_labels)))\n",
    "    ax.set_xticklabels(bmi_labels, rotation=45)\n",
    "    ax.set_xlabel('BMI Category', fontsize=11)\n",
    "    ax.set_ylabel('Age (years)', fontsize=11)\n",
    "    ax.set_title('Age Distribution (Violin Plot)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'age_by_bmi_categories.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Age by BMI visualization saved to {FIGURES_DIR / 'age_by_bmi_categories.png'}\")\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Additional plot: Age × BMI scatter with category colors\n",
    "    print(\"\\nGenerating age-BMI scatter plot...\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    scatter_data = df[['BMI', 'AGE', 'BMI_group']].dropna()\n",
    "    \n",
    "    for bmi_cat in bmi_labels:\n",
    "        cat_data = scatter_data[scatter_data['BMI_group'] == bmi_cat]\n",
    "        ax.scatter(cat_data['AGE'], cat_data['BMI'], \n",
    "                  label=bmi_cat, alpha=0.5, s=30, color=bmi_colors[bmi_cat])\n",
    "    \n",
    "    # Add BMI category threshold lines\n",
    "    ax.axhline(18.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.axhline(25, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.axhline(30, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Add text labels for thresholds\n",
    "    ax.text(scatter_data['AGE'].max() * 0.98, 18.5, 'Underweight/Normal', \n",
    "           va='bottom', ha='right', fontsize=8, style='italic')\n",
    "    ax.text(scatter_data['AGE'].max() * 0.98, 25, 'Normal/Overweight', \n",
    "           va='bottom', ha='right', fontsize=8, style='italic')\n",
    "    ax.text(scatter_data['AGE'].max() * 0.98, 30, 'Overweight/Obese', \n",
    "           va='bottom', ha='right', fontsize=8, style='italic')\n",
    "    \n",
    "    # Calculate and plot trend line\n",
    "    valid_data = scatter_data[['AGE', 'BMI']].dropna()\n",
    "    if len(valid_data) > 10:\n",
    "        z = np.polyfit(valid_data['AGE'], valid_data['BMI'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        age_range = np.linspace(valid_data['AGE'].min(), valid_data['AGE'].max(), 100)\n",
    "        ax.plot(age_range, p(age_range), 'k--', linewidth=2, alpha=0.8, label='Trend')\n",
    "        \n",
    "        # Calculate correlation\n",
    "        r_corr, p_corr = pearsonr(valid_data['AGE'], valid_data['BMI'])\n",
    "        ax.text(0.02, 0.98, f'r = {r_corr:.3f}, p = {p_corr:.4f}',\n",
    "               transform=ax.transAxes, va='top', ha='left',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('Age (years)', fontsize=11)\n",
    "    ax.set_ylabel('BMI (kg/m²)', fontsize=11)\n",
    "    ax.set_title('Age vs BMI with Category Classification', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='upper right', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'age_bmi_scatter.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Age-BMI scatter plot saved to {FIGURES_DIR / 'age_bmi_scatter.png'}\")\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Save summary statistics\n",
    "    bmi_age_stats.to_csv(TABLES_DIR / 'age_by_bmi_statistics.csv')\n",
    "    print(f\"\\n✓ Age by BMI statistics saved to {TABLES_DIR / 'age_by_bmi_statistics.csv'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ AGE-BMI ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️  Skipping age-BMI analysis - BMI groups or age data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Multi-Center Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to identify site/center from subject ID\n",
    "# IXI naming convention: IXIXXX where XXX is a number\n",
    "# Different centers may have different ID ranges\n",
    "\n",
    "\n",
    "# Build the lookup dictionary once\n",
    "site_lookup = create_site_lookup(COMPLETE_IXI)\n",
    "\n",
    "# Apply the lookup (vectorized operation)\n",
    "df['site'] = df['subject_id'].map(site_lookup).fillna('Unknown')\n",
    "\n",
    "# Remove 'Unknown' site entries\n",
    "df = df[df['site'] != 'Unknown']\n",
    "site_counts = df['site'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-CENTER ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSite distribution:\")\n",
    "for site, count in site_counts.items():\n",
    "    print(f\"  {site}: {count} subjects ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "\n",
    "if len(site_counts) > 1:\n",
    "    # Perform ANOVA for each feature across sites\n",
    "    site_comparisons = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Get data for each site\n",
    "        site_data = [df[df['site'] == site][feature].dropna() for site in site_counts.index]\n",
    "        \n",
    "        # Remove groups with <5 samples\n",
    "        site_data = [d for d in site_data if len(d) >= 5]\n",
    "        \n",
    "        if len(site_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # ANOVA and Kruskal-Wallis test\n",
    "        f_stat, f_pval = f_oneway(*site_data)\n",
    "        h_stat, h_pval = kruskal(*site_data)\n",
    "        \n",
    "        # Effect size (eta-squared)\n",
    "        grand_mean = np.mean(np.concatenate(site_data))\n",
    "        ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in site_data)\n",
    "        ss_total = sum(np.sum((d - grand_mean)**2) for d in site_data)\n",
    "        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "        \n",
    "        site_comparisons.append({\n",
    "            'feature': feature,\n",
    "            'category': feature_categories[feature],\n",
    "            'f_statistic': f_stat,\n",
    "            'anova_pvalue': f_pval,\n",
    "            'kw_pvalue': h_pval,\n",
    "            'eta_squared': eta_squared\n",
    "        })\n",
    "    \n",
    "    site_comp_df = pd.DataFrame(site_comparisons)\n",
    "    \n",
    "    if len(site_comp_df) > 0:\n",
    "        # Multiple testing correction\n",
    "        site_comp_df['anova_pvalue_fdr'] = multipletests(site_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "        site_comp_df['significant'] = site_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "        site_comp_df = site_comp_df.sort_values('eta_squared', ascending=False)\n",
    "        \n",
    "        n_sig = site_comp_df['significant'].sum()\n",
    "        print(f\"\\nMulti-center comparison:\")\n",
    "        print(f\"  Features tested: {len(site_comp_df)}\")\n",
    "        print(f\"  Significant site differences (FDR<0.05): {n_sig} ({100*n_sig/len(site_comp_df):.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTop 10 features with site differences (by eta-squared):\")\n",
    "        display(site_comp_df.head(10)[['feature', 'category', 'eta_squared', 'anova_pvalue_fdr']])\n",
    "        \n",
    "        site_comp_df.to_csv(TABLES_DIR / 'site_comparisons.csv', index=False)\n",
    "        print(f\"\\n✓ Site comparisons saved to {TABLES_DIR / 'site_comparisons.csv'}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Insufficient data for multi-center analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import f_oneway, kruskal, levene, shapiro\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ============================================================================\n",
    "# COMPREHENSIVE MULTI-CENTER ANALYSIS\n",
    "# ============================================================================\n",
    "# Add site information\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MULTI-CENTER ANALYSIS\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. DEMOGRAPHIC CHARACTERISTICS BY SITE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. DEMOGRAPHIC CHARACTERISTICS BY SITE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "site_counts = df['site'].value_counts()\n",
    "print(f\"\\nTotal subjects across sites: {len(df)}\")\n",
    "print(f\"\\nSite distribution:\")\n",
    "for site, count in site_counts.items():\n",
    "    print(f\"  {site}: {count} subjects ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "# Demographic comparison by site\n",
    "sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df.columns else 'SEX_ID'\n",
    "\n",
    "demo_by_site = df.groupby('site').agg({\n",
    "    'subject_id': 'count',\n",
    "    'AGE': ['mean', 'std', 'min', 'max'],\n",
    "    sex_col: lambda x: (x == 1).sum(),  # Count males\n",
    "    'HEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "    'WEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "    'BMI': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan\n",
    "})\n",
    "\n",
    "demo_by_site.columns = ['N', 'Age_Mean', 'Age_SD', 'Age_Min', 'Age_Max', \n",
    "                        'N_Males', 'Height_Mean', 'Weight_Mean', 'BMI_Mean']\n",
    "demo_by_site['Pct_Male'] = 100 * demo_by_site['N_Males'] / demo_by_site['N']\n",
    "\n",
    "print(\"\\nDemographic characteristics by site:\")\n",
    "display(demo_by_site.round(2))\n",
    "\n",
    "# Test for demographic differences across sites\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Testing for demographic differences across sites:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Age differences\n",
    "age_by_site = [df[df['site'] == site]['AGE'].dropna() for site in site_counts.index]\n",
    "f_age, p_age = f_oneway(*age_by_site)\n",
    "print(f\"\\nAge difference across sites:\")\n",
    "print(f\"  ANOVA F-statistic: {f_age:.3f}, p-value: {p_age:.4f}\")\n",
    "\n",
    "# Sex distribution differences (Chi-square)\n",
    "sex_site_table = pd.crosstab(df['site'], df[sex_col])\n",
    "chi2, p_chi2, dof, expected = stats.chi2_contingency(sex_site_table)\n",
    "print(f\"\\nSex distribution across sites:\")\n",
    "print(f\"  Chi-square: {chi2:.3f}, p-value: {p_chi2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. SITE EFFECTS ON VESSEL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. SITE EFFECTS ON VESSEL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "site_comparisons = []\n",
    "\n",
    "for feature in ALL_FEATURES:\n",
    "    # Get data for each site\n",
    "    site_data = [df[df['site'] == site][feature].dropna() for site in site_counts.index]\n",
    "    site_data = [d for d in site_data if len(d) >= 5]\n",
    "    \n",
    "    if len(site_data) < 2:\n",
    "        continue\n",
    "    \n",
    "    # One-way ANOVA\n",
    "    f_stat, f_pval = f_oneway(*site_data)\n",
    "    \n",
    "    # Kruskal-Wallis (non-parametric alternative)\n",
    "    h_stat, h_pval = kruskal(*site_data)\n",
    "    \n",
    "    # Levene's test for homogeneity of variance\n",
    "    levene_stat, levene_pval = levene(*site_data)\n",
    "    \n",
    "    # Effect size (eta-squared)\n",
    "    grand_mean = np.mean(np.concatenate(site_data))\n",
    "    ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in site_data)\n",
    "    ss_total = sum(np.sum((d - grand_mean)**2) for d in site_data)\n",
    "    eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "    \n",
    "    # Omega-squared (less biased than eta-squared)\n",
    "    n_total = sum(len(d) for d in site_data)\n",
    "    k_groups = len(site_data)\n",
    "    omega_squared = (ss_between - (k_groups - 1) * (ss_total - ss_between) / (n_total - k_groups)) / (ss_total + (ss_total - ss_between) / (n_total - k_groups))\n",
    "    omega_squared = max(0, omega_squared)  # Can't be negative\n",
    "    \n",
    "    # Calculate mean and SD for each site\n",
    "    site_means = {site: df[df['site'] == site][feature].mean() \n",
    "                  for site in site_counts.index}\n",
    "    site_stds = {site: df[df['site'] == site][feature].std() \n",
    "                 for site in site_counts.index}\n",
    "    \n",
    "    site_comparisons.append({\n",
    "        'feature': feature,\n",
    "        'category': feature_categories[feature],\n",
    "        'f_statistic': f_stat,\n",
    "        'anova_pvalue': f_pval,\n",
    "        'kw_statistic': h_stat,\n",
    "        'kw_pvalue': h_pval,\n",
    "        'levene_pvalue': levene_pval,\n",
    "        'eta_squared': eta_squared,\n",
    "        'omega_squared': omega_squared,\n",
    "        **{f'{site}_mean': site_means[site] for site in site_counts.index},\n",
    "        **{f'{site}_std': site_stds[site] for site in site_counts.index}\n",
    "    })\n",
    "\n",
    "site_comp_df = pd.DataFrame(site_comparisons)\n",
    "\n",
    "if len(site_comp_df) > 0:\n",
    "    # Multiple testing correction\n",
    "    site_comp_df['anova_pvalue_fdr'] = multipletests(site_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "    site_comp_df['kw_pvalue_fdr'] = multipletests(site_comp_df['kw_pvalue'], method='fdr_bh')[1]\n",
    "    site_comp_df['significant_anova'] = site_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "    site_comp_df['significant_kw'] = site_comp_df['kw_pvalue_fdr'] < 0.05\n",
    "    site_comp_df = site_comp_df.sort_values('omega_squared', ascending=False)\n",
    "    \n",
    "    n_sig_anova = site_comp_df['significant_anova'].sum()\n",
    "    n_sig_kw = site_comp_df['significant_kw'].sum()\n",
    "    \n",
    "    print(f\"\\nFeatures tested: {len(site_comp_df)}\")\n",
    "    print(f\"Significant site differences (ANOVA, FDR<0.05): {n_sig_anova} ({100*n_sig_anova/len(site_comp_df):.1f}%)\")\n",
    "    print(f\"Significant site differences (Kruskal-Wallis, FDR<0.05): {n_sig_kw} ({100*n_sig_kw/len(site_comp_df):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 15 features with largest site effects (by ω²):\")\n",
    "    display_cols = ['feature', 'category', 'omega_squared', 'eta_squared', \n",
    "                    'anova_pvalue_fdr', 'kw_pvalue_fdr']\n",
    "    display(site_comp_df.head(15)[display_cols].round(4))\n",
    "    \n",
    "    # Save detailed results\n",
    "    site_comp_df.to_csv(TABLES_DIR / 'site_comparisons_detailed.csv', index=False)\n",
    "    print(f\"\\n✓ Detailed site comparisons saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. POST-HOC PAIRWISE COMPARISONS (for significant features)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. POST-HOC PAIRWISE COMPARISONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select top 5 features with largest site effects\n",
    "top_site_features = site_comp_df[site_comp_df['significant_anova']].head(5)['feature'].tolist()\n",
    "\n",
    "if len(top_site_features) > 0:\n",
    "    print(f\"\\nPerforming Tukey HSD post-hoc tests for top {len(top_site_features)} features...\")\n",
    "    \n",
    "    posthoc_results = []\n",
    "    \n",
    "    for feature in top_site_features:\n",
    "        # Prepare data\n",
    "        data_for_tukey = df[['site', feature]].dropna()\n",
    "        \n",
    "        # Tukey HSD\n",
    "        tukey = pairwise_tukeyhsd(endog=data_for_tukey[feature], \n",
    "                                   groups=data_for_tukey['site'], \n",
    "                                   alpha=0.05)\n",
    "        \n",
    "        # Parse results\n",
    "        tukey_df = pd.DataFrame(data=tukey.summary().data[1:], \n",
    "                               columns=tukey.summary().data[0])\n",
    "        tukey_df['feature'] = feature\n",
    "        posthoc_results.append(tukey_df)\n",
    "        \n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(tukey)\n",
    "    \n",
    "    # Combine all post-hoc results\n",
    "    posthoc_df = pd.concat(posthoc_results, ignore_index=True)\n",
    "    posthoc_df.to_csv(TABLES_DIR / 'site_posthoc_tukey.csv', index=False)\n",
    "    print(f\"\\n✓ Post-hoc comparisons saved\")\n",
    "else:\n",
    "    print(\"\\nNo significant site differences found for post-hoc testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. SITE AS COVARIATE IN AGE-VESSEL RELATIONSHIPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. SITE AS COVARIATE IN AGE-VESSEL RELATIONSHIPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTesting whether site affects age-vessel correlations...\")\n",
    "\n",
    "# Test interaction between age and site for top features\n",
    "site_age_interactions = []\n",
    "\n",
    "for feature in ALL_FEATURES[:20]:  # Test top 20 for computational efficiency\n",
    "    # Prepare data\n",
    "    model_data = df[['AGE', 'site', sex_col, feature]].dropna()\n",
    "    \n",
    "    if len(model_data) < 30:\n",
    "        continue\n",
    "    \n",
    "    # Model 1: Age + Sex (baseline)\n",
    "    formula1 = f'Q(\"{feature}\") ~ AGE + C(Q(\"{sex_col}\"))'\n",
    "    model1 = ols(formula1, data=model_data).fit()\n",
    "    \n",
    "    # Model 2: Age + Sex + Site (main effect)\n",
    "    formula2 = f'Q(\"{feature}\") ~ AGE + C(Q(\"{sex_col}\")) + C(site)'\n",
    "    model2 = ols(formula2, data=model_data).fit()\n",
    "    \n",
    "    # Model 3: Age + Sex + Site + Age×Site (interaction)\n",
    "    formula3 = f'Q(\"{feature}\") ~ AGE * C(site) + C(Q(\"{sex_col}\"))'\n",
    "    model3 = ols(formula3, data=model_data).fit()\n",
    "    \n",
    "    # F-test for site main effect\n",
    "    f_site = ((model1.ssr - model2.ssr) / (model2.df_resid - model1.df_resid)) / (model2.ssr / model2.df_resid)\n",
    "    p_site = 1 - stats.f.cdf(f_site, model2.df_resid - model1.df_resid, model2.df_resid)\n",
    "    \n",
    "    # F-test for age×site interaction\n",
    "    f_interaction = ((model2.ssr - model3.ssr) / (model3.df_resid - model2.df_resid)) / (model3.ssr / model3.df_resid)\n",
    "    p_interaction = 1 - stats.f.cdf(f_interaction, model3.df_resid - model2.df_resid, model3.df_resid)\n",
    "    \n",
    "    site_age_interactions.append({\n",
    "        'feature': feature,\n",
    "        'category': feature_categories[feature],\n",
    "        'r2_baseline': model1.rsquared,\n",
    "        'r2_with_site': model2.rsquared,\n",
    "        'r2_with_interaction': model3.rsquared,\n",
    "        'delta_r2_site': model2.rsquared - model1.rsquared,\n",
    "        'delta_r2_interaction': model3.rsquared - model2.rsquared,\n",
    "        'p_site_effect': p_site,\n",
    "        'p_age_site_interaction': p_interaction,\n",
    "        'age_coef_model1': model1.params['AGE'],\n",
    "        'age_coef_model2': model2.params['AGE']\n",
    "    })\n",
    "\n",
    "site_age_df = pd.DataFrame(site_age_interactions)\n",
    "\n",
    "if len(site_age_df) > 0:\n",
    "    # Multiple testing correction\n",
    "    site_age_df['p_site_fdr'] = multipletests(site_age_df['p_site_effect'], method='fdr_bh')[1]\n",
    "    site_age_df['p_interaction_fdr'] = multipletests(site_age_df['p_age_site_interaction'], method='fdr_bh')[1]\n",
    "    site_age_df['significant_site'] = site_age_df['p_site_fdr'] < 0.05\n",
    "    site_age_df['significant_interaction'] = site_age_df['p_interaction_fdr'] < 0.05\n",
    "    \n",
    "    site_age_df = site_age_df.sort_values('delta_r2_site', ascending=False)\n",
    "    \n",
    "    n_sig_site = site_age_df['significant_site'].sum()\n",
    "    n_sig_interaction = site_age_df['significant_interaction'].sum()\n",
    "    \n",
    "    print(f\"\\nFeatures tested: {len(site_age_df)}\")\n",
    "    print(f\"Features with significant site effect: {n_sig_site} ({100*n_sig_site/len(site_age_df):.1f}%)\")\n",
    "    print(f\"Features with significant Age×Site interaction: {n_sig_interaction} ({100*n_sig_interaction/len(site_age_df):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 10 features where site explains additional variance:\")\n",
    "    display_cols = ['feature', 'category', 'r2_baseline', 'r2_with_site', \n",
    "                    'delta_r2_site', 'p_site_fdr']\n",
    "    display(site_age_df.head(10)[display_cols].round(4))\n",
    "    \n",
    "    if n_sig_interaction > 0:\n",
    "        print(f\"\\nFeatures with significant Age×Site interaction:\")\n",
    "        interaction_features = site_age_df[site_age_df['significant_interaction']]\n",
    "        display(interaction_features[['feature', 'delta_r2_interaction', 'p_interaction_fdr']].round(4))\n",
    "    \n",
    "    site_age_df.to_csv(TABLES_DIR / 'site_age_interactions.csv', index=False)\n",
    "    print(f\"\\n✓ Site-age interaction analysis saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. GENERATING VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5.0: Site demographics overview (NEW)\n",
    "if 'site' in df.columns:\n",
    "    print(\"\\nGenerating site demographics visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Site Demographics Overview', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Site distribution\n",
    "    ax = axes[0, 0]\n",
    "    site_counts = df['site'].value_counts().sort_values(ascending=False)\n",
    "    bars = ax.barh(range(len(site_counts)), site_counts.values, \n",
    "                  color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_yticks(range(len(site_counts)))\n",
    "    ax.set_yticklabels(site_counts.index, fontsize=9)\n",
    "    ax.set_xlabel('Number of Subjects', fontsize=10)\n",
    "    ax.set_title('Sample Size by Site', fontsize=11, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add counts on bars\n",
    "    for i, v in enumerate(site_counts.values):\n",
    "        ax.text(v + 1, i, f'n={int(v)}', va='center', fontsize=9)\n",
    "    \n",
    "    # 2. Age distribution by site\n",
    "    ax = axes[0, 1]\n",
    "    if 'AGE' in df.columns:\n",
    "        plot_data = df[['site', 'AGE']].dropna()\n",
    "        site_order = site_counts.index.tolist()\n",
    "        sns.boxplot(data=plot_data, y='site', x='AGE', ax=ax,\n",
    "                   order=site_order, palette='Set2')\n",
    "        ax.set_xlabel('Age (years)', fontsize=10)\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title('Age Distribution by Site', fontsize=11, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Age data not available', ha='center', va='center',\n",
    "               transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # 3. Sex distribution by site\n",
    "    ax = axes[1, 0]\n",
    "    sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df.columns else 'SEX_ID'\n",
    "    if sex_col in df.columns:\n",
    "        sex_by_site = df.groupby('site')[sex_col].value_counts(normalize=True).unstack()\n",
    "        sex_by_site = sex_by_site.reindex(site_order)\n",
    "        sex_by_site.plot(kind='barh', stacked=True, ax=ax, \n",
    "                        color=['#ff7f0e', '#ff69b4'], alpha=0.8)\n",
    "        ax.set_xlabel('Proportion', fontsize=10)\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_title('Sex Distribution by Site', fontsize=11, fontweight='bold')\n",
    "        ax.legend(['Male', 'Female'], fontsize=9)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'Sex data not available', ha='center', va='center',\n",
    "               transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # 4. BMI by site (if available)\n",
    "    ax = axes[1, 1]\n",
    "    if 'BMI' in df.columns:\n",
    "        plot_data_bmi = df[df['BMI'] > 0][['site', 'BMI']].dropna()\n",
    "        if len(plot_data_bmi) > 0:\n",
    "            sns.boxplot(data=plot_data_bmi, y='site', x='BMI', ax=ax,\n",
    "                       order=site_order, palette='Set3')\n",
    "            ax.axvline(25, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Overweight')\n",
    "            ax.axvline(30, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Obese')\n",
    "            ax.set_xlabel('BMI (kg/m²)', fontsize=10)\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_title('BMI Distribution by Site', fontsize=11, fontweight='bold')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Insufficient BMI data', ha='center', va='center',\n",
    "                   transform=ax.transAxes)\n",
    "            ax.axis('off')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'BMI data not available', ha='center', va='center',\n",
    "               transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'site_demographics.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Site demographics plot saved\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. GENERATING VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5.1: Site effect sizes visualization\n",
    "if len(site_comp_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Effect size distribution\n",
    "    ax = axes[0]\n",
    "    ax.hist(site_comp_df['omega_squared'], bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(0.01, color='orange', linestyle='--', label='Small effect (0.01)')\n",
    "    ax.axvline(0.06, color='red', linestyle='--', label='Medium effect (0.06)')\n",
    "    ax.axvline(0.14, color='darkred', linestyle='--', label='Large effect (0.14)')\n",
    "    ax.set_xlabel('Omega-squared (ω²)', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title('Distribution of Site Effect Sizes', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Volcano plot\n",
    "    ax = axes[1]\n",
    "    site_comp_df['-log10_p'] = -np.log10(site_comp_df['anova_pvalue_fdr'])\n",
    "    colors = ['red' if sig else 'gray' for sig in site_comp_df['significant_anova']]\n",
    "    ax.scatter(site_comp_df['omega_squared'], site_comp_df['-log10_p'], \n",
    "              c=colors, alpha=0.6, s=30)\n",
    "    ax.axhline(-np.log10(0.05), color='blue', linestyle='--', label='FDR = 0.05')\n",
    "    ax.axvline(0.06, color='orange', linestyle='--', label='Medium effect')\n",
    "    ax.set_xlabel('Effect Size (ω²)', fontsize=11)\n",
    "    ax.set_ylabel('-log₁₀(FDR-adjusted p-value)', fontsize=11)\n",
    "    ax.set_title('Site Effect Volcano Plot', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'site_effects_overview.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Site effects overview saved\")\n",
    "    plt.show()\n",
    "\n",
    "# 5.2: Top site differences visualization\n",
    "if len(top_site_features) > 0:\n",
    "    n_features = min(6, len(top_site_features))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Top Site Differences in Vessel Features', fontsize=14, fontweight='bold')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(top_site_features[:n_features]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Prepare data\n",
    "        plot_data = df[['site', feature]].dropna()\n",
    "        \n",
    "        # Box plot\n",
    "        sns.boxplot(data=plot_data, x='site', y=feature, ax=ax, palette='Set2')\n",
    "        sns.stripplot(data=plot_data, x='site', y=feature, ax=ax, \n",
    "                     color='black', alpha=0.3, size=3)\n",
    "        \n",
    "        # Get stats\n",
    "        feat_stats = site_comp_df[site_comp_df['feature'] == feature].iloc[0]\n",
    "        omega_sq = feat_stats['omega_squared']\n",
    "        p_val = feat_stats['anova_pvalue_fdr']\n",
    "        \n",
    "        # Significance stars\n",
    "        if p_val < 0.001:\n",
    "            sig_stars = '***'\n",
    "            p_text = 'p < 0.001'\n",
    "        elif p_val < 0.01:\n",
    "            sig_stars = '**'\n",
    "            p_text = f'p = {p_val:.3f}'\n",
    "        elif p_val < 0.05:\n",
    "            sig_stars = '*'\n",
    "            p_text = f'p = {p_val:.3f}'\n",
    "        else:\n",
    "            sig_stars = 'ns'\n",
    "            p_text = f'p = {p_val:.2f}'\n",
    "        \n",
    "        clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "        \n",
    "        ax.set_title(f\"{clean_feature} {sig_stars}\\nω² = {omega_sq:.3f}, {p_text}\", \n",
    "                    fontsize=10)\n",
    "        ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "        ax.set_xlabel('Site', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_features, 6):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'site_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Site differences visualization saved\")\n",
    "    plt.show()\n",
    "\n",
    "# 5.3: Age-vessel relationships by site (if interactions found)\n",
    "if len(site_age_df) > 0 and site_age_df['significant_interaction'].any():\n",
    "    interaction_features = site_age_df[site_age_df['significant_interaction']].head(4)['feature'].tolist()\n",
    "    \n",
    "    if len(interaction_features) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        fig.suptitle('Age-Vessel Relationships by Site (Significant Interactions)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feature in enumerate(interaction_features[:4]):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            for site in site_counts.index:\n",
    "                site_data = df[df['site'] == site]\n",
    "                ax.scatter(site_data['AGE'], site_data[feature], \n",
    "                          label=site, alpha=0.5, s=20)\n",
    "                \n",
    "                # Fit line for each site\n",
    "                valid_data = site_data[['AGE', feature]].dropna()\n",
    "                if len(valid_data) > 10:\n",
    "                    z = np.polyfit(valid_data['AGE'], valid_data[feature], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    age_range = np.linspace(valid_data['AGE'].min(), valid_data['AGE'].max(), 100)\n",
    "                    ax.plot(age_range, p(age_range), linewidth=2)\n",
    "            \n",
    "            clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "            ax.set_xlabel('Age (years)', fontsize=10)\n",
    "            ax.set_ylabel(clean_feature, fontsize=10)\n",
    "            ax.set_title(clean_feature, fontsize=11, fontweight='bold')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'age_site_interactions.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"✓ Age×Site interaction visualization saved\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-CENTER ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nKey findings:\")\n",
    "print(f\"1. Site effects detected in {n_sig_anova}/{len(site_comp_df)} features\")\n",
    "print(f\"2. Site explains additional variance in {n_sig_site} age-vessel relationships\")\n",
    "if 'n_sig_interaction' in locals():\n",
    "    print(f\"3. Age×Site interactions found in {n_sig_interaction} features\")\n",
    "print(\"\\nRecommendations for your paper:\")\n",
    "print(\"- Include site as covariate in all age-vessel analyses\")\n",
    "print(\"- Report site effects in supplementary materials\")\n",
    "print(\"- Discuss multi-center design as strength (generalizability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.2 Ethnic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ETHNICITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ETHNICITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'ETHNIC_ID' in df.columns:\n",
    "    # Remove missing values and check distribution\n",
    "    df_ethnic = df[df['ETHNIC_ID'].notna()].copy()\n",
    "    ethnic_counts = df_ethnic['ETHNIC_ID'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nEthnicity distribution:\")\n",
    "    print(f\"  Total with ethnicity data: {len(df_ethnic)} ({100*len(df_ethnic)/len(df):.1f}% of cohort)\")\n",
    "    print(f\"  Missing ethnicity data: {df['ETHNIC_ID'].isna().sum()} ({100*df['ETHNIC_ID'].isna().sum()/len(df):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nEthnicity categories:\")\n",
    "    for ethnic_id, count in ethnic_counts.items():\n",
    "        print(f\"  Category {int(ethnic_id)}: {count} subjects ({100*count/len(df_ethnic):.1f}%)\")\n",
    "    \n",
    "    # Check if we have sufficient data for analysis (at least 2 groups with n≥10)\n",
    "    valid_groups = ethnic_counts[ethnic_counts >= 10]\n",
    "    \n",
    "    if len(valid_groups) >= 2:\n",
    "        print(f\"\\n✓ Sufficient data for analysis: {len(valid_groups)} groups with n≥10\")\n",
    "        \n",
    "        # Filter to valid groups only\n",
    "        df_ethnic = df_ethnic[df_ethnic['ETHNIC_ID'].isin(valid_groups.index)].copy()\n",
    "        \n",
    "        # Check demographic differences by ethnicity\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Demographic characteristics by ethnicity:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df_ethnic.columns else 'SEX_ID'\n",
    "        \n",
    "        ethnic_demo = df_ethnic.groupby('ETHNIC_ID').agg({\n",
    "            'subject_id': 'count',\n",
    "            'AGE': ['mean', 'std', 'min', 'max'],\n",
    "            sex_col: lambda x: (x == 1).sum(),\n",
    "            'HEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'WEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'BMI': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan\n",
    "        })\n",
    "        \n",
    "        ethnic_demo.columns = ['N', 'Age_Mean', 'Age_SD', 'Age_Min', 'Age_Max',\n",
    "                               'N_Males', 'Height_Mean', 'Weight_Mean', 'BMI_Mean']\n",
    "        ethnic_demo['Pct_Male'] = 100 * ethnic_demo['N_Males'] / ethnic_demo['N']\n",
    "        \n",
    "        display(ethnic_demo.round(2))\n",
    "        \n",
    "        # Test for age differences\n",
    "        age_by_ethnic = [df_ethnic[df_ethnic['ETHNIC_ID'] == eid]['AGE'].dropna() \n",
    "                        for eid in valid_groups.index]\n",
    "        f_age, p_age = f_oneway(*age_by_ethnic)\n",
    "        print(f\"\\nAge difference across ethnicities: F={f_age:.3f}, p={p_age:.4f}\")\n",
    "        \n",
    "        # Ethnicity effects on vessel features\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Ethnicity effects on vessel features:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        ethnic_comparisons = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            # Get data for each ethnicity\n",
    "            ethnic_data = [df_ethnic[df_ethnic['ETHNIC_ID'] == eid][feature].dropna() \n",
    "                          for eid in valid_groups.index]\n",
    "            ethnic_data = [d for d in ethnic_data if len(d) >= 5]\n",
    "            \n",
    "            if len(ethnic_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            # ANOVA and Kruskal-Wallis\n",
    "            f_stat, f_pval = f_oneway(*ethnic_data)\n",
    "            h_stat, h_pval = kruskal(*ethnic_data)\n",
    "            \n",
    "            # Levene's test\n",
    "            levene_stat, levene_pval = levene(*ethnic_data)\n",
    "            \n",
    "            # Effect sizes\n",
    "            grand_mean = np.mean(np.concatenate(ethnic_data))\n",
    "            ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in ethnic_data)\n",
    "            ss_total = sum(np.sum((d - grand_mean)**2) for d in ethnic_data)\n",
    "            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "            \n",
    "            n_total = sum(len(d) for d in ethnic_data)\n",
    "            k_groups = len(ethnic_data)\n",
    "            omega_squared = (ss_between - (k_groups - 1) * (ss_total - ss_between) / (n_total - k_groups)) / \\\n",
    "                           (ss_total + (ss_total - ss_between) / (n_total - k_groups))\n",
    "            omega_squared = max(0, omega_squared)\n",
    "            \n",
    "            ethnic_comparisons.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'f_statistic': f_stat,\n",
    "                'anova_pvalue': f_pval,\n",
    "                'kw_statistic': h_stat,\n",
    "                'kw_pvalue': h_pval,\n",
    "                'levene_pvalue': levene_pval,\n",
    "                'eta_squared': eta_squared,\n",
    "                'omega_squared': omega_squared\n",
    "            })\n",
    "        \n",
    "        ethnic_comp_df = pd.DataFrame(ethnic_comparisons)\n",
    "        \n",
    "        if len(ethnic_comp_df) > 0:\n",
    "            # Multiple testing correction\n",
    "            ethnic_comp_df['anova_pvalue_fdr'] = multipletests(ethnic_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "            ethnic_comp_df['kw_pvalue_fdr'] = multipletests(ethnic_comp_df['kw_pvalue'], method='fdr_bh')[1]\n",
    "            ethnic_comp_df['significant_anova'] = ethnic_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "            ethnic_comp_df['significant_kw'] = ethnic_comp_df['kw_pvalue_fdr'] < 0.05\n",
    "            ethnic_comp_df = ethnic_comp_df.sort_values('omega_squared', ascending=False)\n",
    "            \n",
    "            n_sig_anova = ethnic_comp_df['significant_anova'].sum()\n",
    "            n_sig_kw = ethnic_comp_df['significant_kw'].sum()\n",
    "            \n",
    "            print(f\"\\nFeatures tested: {len(ethnic_comp_df)}\")\n",
    "            print(f\"Significant ethnicity differences (ANOVA, FDR<0.05): {n_sig_anova} ({100*n_sig_anova/len(ethnic_comp_df):.1f}%)\")\n",
    "            print(f\"Significant ethnicity differences (Kruskal-Wallis, FDR<0.05): {n_sig_kw} ({100*n_sig_kw/len(ethnic_comp_df):.1f}%)\")\n",
    "            \n",
    "            if n_sig_anova > 0:\n",
    "                print(f\"\\nTop 10 features with ethnicity effects (by ω²):\")\n",
    "                display_cols = ['feature', 'category', 'omega_squared', 'eta_squared', \n",
    "                               'anova_pvalue_fdr', 'kw_pvalue_fdr']\n",
    "                display(ethnic_comp_df.head(10)[display_cols].round(4))\n",
    "            else:\n",
    "                print(\"\\n⚠️  No significant ethnicity effects detected after FDR correction\")\n",
    "            \n",
    "            # Save results\n",
    "            ethnic_comp_df.to_csv(TABLES_DIR / 'ethnicity_comparisons.csv', index=False)\n",
    "            print(f\"\\n✓ Ethnicity comparisons saved\")\n",
    "            \n",
    "            # Visualization if significant effects found\n",
    "            if n_sig_anova > 0:\n",
    "                top_ethnic_features = ethnic_comp_df[ethnic_comp_df['significant_anova']].head(6)['feature'].tolist()\n",
    "                \n",
    "                if len(top_ethnic_features) > 0:\n",
    "                    n_plots = min(6, len(top_ethnic_features))\n",
    "                    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "                    fig.suptitle('Top Ethnicity Differences in Vessel Features', \n",
    "                                fontsize=14, fontweight='bold')\n",
    "                    axes = axes.flatten()\n",
    "                    \n",
    "                    for idx, feature in enumerate(top_ethnic_features[:n_plots]):\n",
    "                        ax = axes[idx]\n",
    "                        \n",
    "                        plot_data = df_ethnic[['ETHNIC_ID', feature]].dropna()\n",
    "                        plot_data['Ethnicity'] = plot_data['ETHNIC_ID'].astype(str)\n",
    "                        \n",
    "                        sns.boxplot(data=plot_data, x='Ethnicity', y=feature, ax=ax, palette='Set3')\n",
    "                        sns.stripplot(data=plot_data, x='Ethnicity', y=feature, ax=ax,\n",
    "                                     color='black', alpha=0.3, size=3)\n",
    "                        \n",
    "                        feat_stats = ethnic_comp_df[ethnic_comp_df['feature'] == feature].iloc[0]\n",
    "                        omega_sq = feat_stats['omega_squared']\n",
    "                        p_val = feat_stats['anova_pvalue_fdr']\n",
    "                        \n",
    "                        if p_val < 0.001:\n",
    "                            sig_stars = '***'\n",
    "                            p_text = 'p < 0.001'\n",
    "                        elif p_val < 0.01:\n",
    "                            sig_stars = '**'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        elif p_val < 0.05:\n",
    "                            sig_stars = '*'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        else:\n",
    "                            sig_stars = 'ns'\n",
    "                            p_text = f'p = {p_val:.2f}'\n",
    "                        \n",
    "                        clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "                        \n",
    "                        ax.set_title(f\"{clean_feature} {sig_stars}\\nω² = {omega_sq:.3f}, {p_text}\", \n",
    "                                    fontsize=10)\n",
    "                        ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "                        ax.set_xlabel('Ethnicity Category', fontsize=9)\n",
    "                        ax.grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    for idx in range(n_plots, 6):\n",
    "                        axes[idx].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(FIGURES_DIR / 'ethnicity_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "                    print(\"✓ Ethnicity differences visualization saved\")\n",
    "                    plt.show()\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Insufficient data for analysis: only {len(valid_groups)} group(s) with n≥10\")\n",
    "        print(\"    Ethnicity analysis skipped\")\n",
    "else:\n",
    "    print(\"\\n⚠️  ETHNIC_ID column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.3 Marital Status Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MARITAL STATUS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MARITAL STATUS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'MARITAL_ID' in df.columns:\n",
    "    df_marital = df[df['MARITAL_ID'].notna()].copy()\n",
    "    marital_counts = df_marital['MARITAL_ID'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nMarital status distribution:\")\n",
    "    print(f\"  Total with marital data: {len(df_marital)} ({100*len(df_marital)/len(df):.1f}% of cohort)\")\n",
    "    print(f\"  Missing marital data: {df['MARITAL_ID'].isna().sum()} ({100*df['MARITAL_ID'].isna().sum()/len(df):.1f}%)\")\n",
    "    \n",
    "    # Map marital status codes (adjust based on your dataset documentation)\n",
    "    marital_labels = {\n",
    "        1: 'Single',\n",
    "        2: 'Married/Partnership',\n",
    "        3: 'Divorced/Separated',\n",
    "        4: 'Widowed'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMarital status categories:\")\n",
    "    for marital_id, count in marital_counts.items():\n",
    "        label = marital_labels.get(int(marital_id), f'Category {int(marital_id)}')\n",
    "        print(f\"  {label}: {count} subjects ({100*count/len(df_marital):.1f}%)\")\n",
    "    \n",
    "    valid_groups = marital_counts[marital_counts >= 10]\n",
    "    \n",
    "    if len(valid_groups) >= 2:\n",
    "        print(f\"\\n✓ Sufficient data for analysis: {len(valid_groups)} groups with n≥10\")\n",
    "        \n",
    "        df_marital = df_marital[df_marital['MARITAL_ID'].isin(valid_groups.index)].copy()\n",
    "        df_marital['MARITAL_LABEL'] = df_marital['MARITAL_ID'].map(marital_labels)\n",
    "        \n",
    "        # Demographic characteristics\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Demographic characteristics by marital status:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df_marital.columns else 'SEX_ID'\n",
    "        \n",
    "        marital_demo = df_marital.groupby('MARITAL_LABEL').agg({\n",
    "            'subject_id': 'count',\n",
    "            'AGE': ['mean', 'std', 'min', 'max'],\n",
    "            sex_col: lambda x: (x == 1).sum(),\n",
    "            'HEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'WEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'BMI': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan\n",
    "        })\n",
    "        \n",
    "        marital_demo.columns = ['N', 'Age_Mean', 'Age_SD', 'Age_Min', 'Age_Max',\n",
    "                                'N_Males', 'Height_Mean', 'Weight_Mean', 'BMI_Mean']\n",
    "        marital_demo['Pct_Male'] = 100 * marital_demo['N_Males'] / marital_demo['N']\n",
    "        \n",
    "        display(marital_demo.round(2))\n",
    "        \n",
    "        # Note: Marital status is strongly confounded with age\n",
    "        print(\"\\n⚠️  NOTE: Marital status is highly correlated with age\")\n",
    "        print(\"    Consider age-adjusted analyses or stratified analyses\")\n",
    "        \n",
    "        # Age differences\n",
    "        age_by_marital = [df_marital[df_marital['MARITAL_ID'] == mid]['AGE'].dropna()\n",
    "                         for mid in valid_groups.index]\n",
    "        f_age, p_age = f_oneway(*age_by_marital)\n",
    "        print(f\"\\nAge difference across marital status: F={f_age:.3f}, p={p_age:.4e}\")\n",
    "        \n",
    "        # Marital status effects on vessel features (unadjusted)\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Marital status effects on vessel features (UNADJUSTED):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        marital_comparisons = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            marital_data = [df_marital[df_marital['MARITAL_ID'] == mid][feature].dropna()\n",
    "                           for mid in valid_groups.index]\n",
    "            marital_data = [d for d in marital_data if len(d) >= 5]\n",
    "            \n",
    "            if len(marital_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            f_stat, f_pval = f_oneway(*marital_data)\n",
    "            h_stat, h_pval = kruskal(*marital_data)\n",
    "            levene_stat, levene_pval = levene(*marital_data)\n",
    "            \n",
    "            grand_mean = np.mean(np.concatenate(marital_data))\n",
    "            ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in marital_data)\n",
    "            ss_total = sum(np.sum((d - grand_mean)**2) for d in marital_data)\n",
    "            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "            \n",
    "            n_total = sum(len(d) for d in marital_data)\n",
    "            k_groups = len(marital_data)\n",
    "            omega_squared = (ss_between - (k_groups - 1) * (ss_total - ss_between) / (n_total - k_groups)) / \\\n",
    "                           (ss_total + (ss_total - ss_between) / (n_total - k_groups))\n",
    "            omega_squared = max(0, omega_squared)\n",
    "            \n",
    "            marital_comparisons.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'f_statistic': f_stat,\n",
    "                'anova_pvalue': f_pval,\n",
    "                'kw_pvalue': h_pval,\n",
    "                'eta_squared': eta_squared,\n",
    "                'omega_squared': omega_squared\n",
    "            })\n",
    "        \n",
    "        marital_comp_df = pd.DataFrame(marital_comparisons)\n",
    "        \n",
    "        if len(marital_comp_df) > 0:\n",
    "            marital_comp_df['anova_pvalue_fdr'] = multipletests(marital_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "            marital_comp_df['significant'] = marital_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "            marital_comp_df = marital_comp_df.sort_values('omega_squared', ascending=False)\n",
    "            \n",
    "            n_sig = marital_comp_df['significant'].sum()\n",
    "            \n",
    "            print(f\"\\nFeatures tested: {len(marital_comp_df)}\")\n",
    "            print(f\"Significant marital status differences (ANOVA, FDR<0.05): {n_sig} ({100*n_sig/len(marital_comp_df):.1f}%)\")\n",
    "            \n",
    "            if n_sig > 0:\n",
    "                print(f\"\\nTop 10 features with marital status effects (by ω²):\")\n",
    "                display(marital_comp_df.head(10)[['feature', 'category', 'omega_squared', 'anova_pvalue_fdr']].round(4))\n",
    "                \n",
    "                print(\"\\n⚠️  WARNING: These differences may be confounded by age!\")\n",
    "                print(\"    Perform age-adjusted analysis below for proper interpretation\")\n",
    "            \n",
    "            marital_comp_df.to_csv(TABLES_DIR / 'marital_comparisons_unadjusted.csv', index=False)\n",
    "            \n",
    "            # AGE-ADJUSTED ANALYSIS\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "            print(\"Marital status effects on vessel features (AGE-ADJUSTED):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            marital_adjusted = []\n",
    "            \n",
    "            for feature in ALL_FEATURES[:30]:  # Test subset for efficiency\n",
    "                model_data = df_marital[['AGE', 'MARITAL_ID', sex_col, feature]].dropna()\n",
    "                \n",
    "                if len(model_data) < 30:\n",
    "                    continue\n",
    "                \n",
    "                # Model 1: Age + Sex only\n",
    "                formula1 = f'Q(\"{feature}\") ~ AGE + C(Q(\"{sex_col}\"))'\n",
    "                model1 = ols(formula1, data=model_data).fit()\n",
    "                \n",
    "                # Model 2: Age + Sex + Marital Status\n",
    "                formula2 = f'Q(\"{feature}\") ~ AGE + C(Q(\"{sex_col}\")) + C(MARITAL_ID)'\n",
    "                model2 = ols(formula2, data=model_data).fit()\n",
    "                \n",
    "                # F-test for marital status effect\n",
    "                f_marital = ((model1.ssr - model2.ssr) / (model2.df_resid - model1.df_resid)) / \\\n",
    "                           (model2.ssr / model2.df_resid)\n",
    "                p_marital = 1 - stats.f.cdf(f_marital, model2.df_resid - model1.df_resid, model2.df_resid)\n",
    "                \n",
    "                marital_adjusted.append({\n",
    "                    'feature': feature,\n",
    "                    'category': feature_categories[feature],\n",
    "                    'r2_baseline': model1.rsquared,\n",
    "                    'r2_with_marital': model2.rsquared,\n",
    "                    'delta_r2': model2.rsquared - model1.rsquared,\n",
    "                    'p_marital_adjusted': p_marital\n",
    "                })\n",
    "            \n",
    "            marital_adj_df = pd.DataFrame(marital_adjusted)\n",
    "            \n",
    "            if len(marital_adj_df) > 0:\n",
    "                marital_adj_df['p_fdr'] = multipletests(marital_adj_df['p_marital_adjusted'], method='fdr_bh')[1]\n",
    "                marital_adj_df['significant_adjusted'] = marital_adj_df['p_fdr'] < 0.05\n",
    "                marital_adj_df = marital_adj_df.sort_values('delta_r2', ascending=False)\n",
    "                \n",
    "                n_sig_adj = marital_adj_df['significant_adjusted'].sum()\n",
    "                \n",
    "                print(f\"\\nFeatures tested: {len(marital_adj_df)}\")\n",
    "                print(f\"Significant age-adjusted marital effects: {n_sig_adj} ({100*n_sig_adj/len(marital_adj_df):.1f}%)\")\n",
    "                \n",
    "                if n_sig_adj > 0:\n",
    "                    print(f\"\\nTop features with age-adjusted marital status effects:\")\n",
    "                    display(marital_adj_df[marital_adj_df['significant_adjusted']][\n",
    "                        ['feature', 'delta_r2', 'p_fdr']].round(4))\n",
    "                else:\n",
    "                    print(\"\\n✓ No significant marital status effects after adjusting for age\")\n",
    "                    print(\"    → Observed differences were primarily age-related\")\n",
    "                \n",
    "                marital_adj_df.to_csv(TABLES_DIR / 'marital_comparisons_age_adjusted.csv', index=False)\n",
    "                print(f\"\\n✓ Age-adjusted marital status analysis saved\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Insufficient data: only {len(valid_groups)} group(s) with n≥10\")\n",
    "else:\n",
    "    print(\"\\n⚠️  MARITAL_ID column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.4 Occupation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OCCUPATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OCCUPATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'OCCUPATION_ID' in df.columns:\n",
    "    df_occup = df[df['OCCUPATION_ID'].notna()].copy()\n",
    "    occup_counts = df_occup['OCCUPATION_ID'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nOccupation distribution:\")\n",
    "    print(f\"  Total with occupation data: {len(df_occup)} ({100*len(df_occup)/len(df):.1f}% of cohort)\")\n",
    "    print(f\"  Missing occupation data: {df['OCCUPATION_ID'].isna().sum()} ({100*df['OCCUPATION_ID'].isna().sum()/len(df):.1f}%)\")\n",
    "    \n",
    "    # Define occupation labels\n",
    "    occupation_labels = {\n",
    "        1: 'Full-time employed',\n",
    "        2: 'Part-time employed',\n",
    "        3: 'Student',\n",
    "        4: 'Housework',\n",
    "        5: 'Retired',\n",
    "        6: 'Unemployed',\n",
    "        7: 'Work at home',\n",
    "        8: 'Other'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nOccupation categories:\")\n",
    "    for occup_id, count in occup_counts.items():\n",
    "        label = occupation_labels.get(int(occup_id), f'Category {int(occup_id)}')\n",
    "        print(f\"  {label}: {count} subjects ({100*count/len(df_occup):.1f}%)\")\n",
    "    \n",
    "    valid_groups = occup_counts[occup_counts >= 10]\n",
    "    \n",
    "    if len(valid_groups) >= 2:\n",
    "        print(f\"\\n✓ Sufficient data for analysis: {len(valid_groups)} groups with n≥10\")\n",
    "        \n",
    "        df_occup = df_occup[df_occup['OCCUPATION_ID'].isin(valid_groups.index)].copy()\n",
    "        df_occup['OCCUPATION_LABEL'] = df_occup['OCCUPATION_ID'].map(occupation_labels)\n",
    "        \n",
    "        # Define consistent color mapping for ALL occupation visualizations\n",
    "        unique_occupations = df_occup['OCCUPATION_LABEL'].unique()\n",
    "        occupation_colors = dict(zip(unique_occupations, \n",
    "                                    sns.color_palette('tab10', n_colors=len(unique_occupations))))\n",
    "        \n",
    "        # Demographic characteristics\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Demographic characteristics by occupation:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df_occup.columns else 'SEX_ID'\n",
    "        \n",
    "        occup_demo = df_occup.groupby('OCCUPATION_LABEL').agg({\n",
    "            'subject_id': 'count',\n",
    "            'AGE': ['mean', 'std', 'min', 'max'],\n",
    "            sex_col: lambda x: (x == 1).sum(),\n",
    "            'HEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'WEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'BMI': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan\n",
    "        })\n",
    "        \n",
    "        occup_demo.columns = ['N', 'Age_Mean', 'Age_SD', 'Age_Min', 'Age_Max',\n",
    "                              'N_Males', 'Height_Mean', 'Weight_Mean', 'BMI_Mean']\n",
    "        occup_demo['Pct_Male'] = 100 * occup_demo['N_Males'] / occup_demo['N']\n",
    "        \n",
    "        display(occup_demo.round(2))\n",
    "        \n",
    "        print(\"\\n⚠️  NOTE: Occupation may be confounded with age, sex, and socioeconomic status\")\n",
    "        \n",
    "        # Occupation effects (unadjusted)\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Occupation effects on vessel features (UNADJUSTED):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        occup_comparisons = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            occup_data = [df_occup[df_occup['OCCUPATION_ID'] == oid][feature].dropna()\n",
    "                         for oid in valid_groups.index]\n",
    "            occup_data = [d for d in occup_data if len(d) >= 5]\n",
    "            \n",
    "            if len(occup_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            f_stat, f_pval = f_oneway(*occup_data)\n",
    "            h_stat, h_pval = kruskal(*occup_data)\n",
    "            \n",
    "            grand_mean = np.mean(np.concatenate(occup_data))\n",
    "            ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in occup_data)\n",
    "            ss_total = sum(np.sum((d - grand_mean)**2) for d in occup_data)\n",
    "            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "            \n",
    "            n_total = sum(len(d) for d in occup_data)\n",
    "            k_groups = len(occup_data)\n",
    "            omega_squared = (ss_between - (k_groups - 1) * (ss_total - ss_between) / (n_total - k_groups)) / \\\n",
    "                           (ss_total + (ss_total - ss_between) / (n_total - k_groups))\n",
    "            omega_squared = max(0, omega_squared)\n",
    "            \n",
    "            occup_comparisons.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'f_statistic': f_stat,\n",
    "                'anova_pvalue': f_pval,\n",
    "                'kw_pvalue': h_pval,\n",
    "                'omega_squared': omega_squared\n",
    "            })\n",
    "        \n",
    "        occup_comp_df = pd.DataFrame(occup_comparisons)\n",
    "        \n",
    "        if len(occup_comp_df) > 0:\n",
    "            occup_comp_df['anova_pvalue_fdr'] = multipletests(occup_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "            occup_comp_df['significant'] = occup_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "            occup_comp_df = occup_comp_df.sort_values('omega_squared', ascending=False)\n",
    "            \n",
    "            n_sig = occup_comp_df['significant'].sum()\n",
    "            \n",
    "            print(f\"\\nFeatures tested: {len(occup_comp_df)}\")\n",
    "            print(f\"Significant occupation differences: {n_sig} ({100*n_sig/len(occup_comp_df):.1f}%)\")\n",
    "            \n",
    "            if n_sig > 0:\n",
    "                print(f\"\\nTop 10 features with occupation effects:\")\n",
    "                display(occup_comp_df.head(10)[['feature', 'category', 'omega_squared', 'anova_pvalue_fdr']].round(4))\n",
    "            \n",
    "            occup_comp_df.to_csv(TABLES_DIR / 'occupation_comparisons.csv', index=False)\n",
    "            print(f\"\\n✓ Occupation analysis saved\")\n",
    "            \n",
    "            # ========================================================================\n",
    "            # VISUALIZATIONS\n",
    "            # ========================================================================\n",
    "            \n",
    "            # Plot 1: Occupation distribution with demographics\n",
    "            print(\"\\nGenerating occupation visualizations...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            fig.suptitle('Occupation Demographics Overview', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # 1. Occupation distribution\n",
    "            ax = axes[0, 0]\n",
    "            occup_demo_sorted = occup_demo.sort_values('N', ascending=False)\n",
    "            occupation_order_demo = occup_demo_sorted.index.tolist()\n",
    "            bar_colors = [occupation_colors[occ] for occ in occupation_order_demo]\n",
    "            bars = ax.barh(range(len(occup_demo_sorted)), occup_demo_sorted['N'], \n",
    "                          color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "            ax.set_yticks(range(len(occup_demo_sorted)))\n",
    "            ax.set_yticklabels(occupation_order_demo, fontsize=9)\n",
    "            ax.set_xlabel('Number of Subjects', fontsize=10)\n",
    "            ax.set_title('Sample Size by Occupation', fontsize=11, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add counts on bars\n",
    "            for i, v in enumerate(occup_demo_sorted['N']):\n",
    "                ax.text(v + 1, i, f'n={int(v)}', va='center', fontsize=9)\n",
    "            \n",
    "            # 2. Age distribution by occupation\n",
    "            ax = axes[0, 1]\n",
    "            plot_data = df_occup[['OCCUPATION_LABEL', 'AGE']].dropna()\n",
    "            box_colors = [occupation_colors[occ] for occ in occupation_order_demo]\n",
    "            sns.boxplot(data=plot_data, y='OCCUPATION_LABEL', x='AGE', ax=ax,\n",
    "                       order=occupation_order_demo, palette=box_colors)\n",
    "            ax.set_xlabel('Age (years)', fontsize=10)\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_title('Age Distribution by Occupation', fontsize=11, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # 3. Sex distribution by occupation\n",
    "            ax = axes[1, 0]\n",
    "            sex_by_occup = df_occup.groupby('OCCUPATION_LABEL')[sex_col].value_counts(normalize=True).unstack()\n",
    "            sex_by_occup = sex_by_occup.reindex(occupation_order_demo)\n",
    "            sex_by_occup.plot(kind='barh', stacked=True, ax=ax, \n",
    "                             color=['#ff7f0e', '#ff69b4'], alpha=0.8)\n",
    "            ax.set_xlabel('Proportion', fontsize=10)\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_title('Sex Distribution by Occupation', fontsize=11, fontweight='bold')\n",
    "            ax.legend(['Male', 'Female'], fontsize=9)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # 4. BMI by occupation (if available)\n",
    "            ax = axes[1, 1]\n",
    "            if 'BMI' in df_occup.columns:\n",
    "                plot_data_bmi = df_occup[df_occup['BMI'] > 0][['OCCUPATION_LABEL', 'BMI']].dropna()\n",
    "                if len(plot_data_bmi) > 0:\n",
    "                    bmi_colors = [occupation_colors[occ] for occ in occupation_order_demo]\n",
    "                    sns.boxplot(data=plot_data_bmi, y='OCCUPATION_LABEL', x='BMI', ax=ax,\n",
    "                               order=occupation_order_demo, palette=bmi_colors)\n",
    "                    ax.axvline(25, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Overweight')\n",
    "                    ax.axvline(30, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Obese')\n",
    "                    ax.set_xlabel('BMI (kg/m²)', fontsize=10)\n",
    "                    ax.set_ylabel('')\n",
    "                    ax.set_title('BMI Distribution by Occupation', fontsize=11, fontweight='bold')\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(axis='x', alpha=0.3)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'Insufficient BMI data', ha='center', va='center',\n",
    "                           transform=ax.transAxes)\n",
    "                    ax.axis('off')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'BMI data not available', ha='center', va='center',\n",
    "                       transform=ax.transAxes)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIGURES_DIR / 'occupation_demographics.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"✓ Occupation demographics plot saved\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot 2: Top vessel feature differences by occupation\n",
    "            if n_sig > 0:\n",
    "                top_occup_features = occup_comp_df[occup_comp_df['significant']].head(6)['feature'].tolist()\n",
    "                \n",
    "                if len(top_occup_features) > 0:\n",
    "                    n_plots = min(6, len(top_occup_features))\n",
    "                    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "                    fig.suptitle('Top Occupation Differences in Vessel Features',\n",
    "                                fontsize=14, fontweight='bold')\n",
    "                    axes = axes.flatten()\n",
    "                    \n",
    "                    for idx, feature in enumerate(top_occup_features[:n_plots]):\n",
    "                        ax = axes[idx]\n",
    "                        \n",
    "                        plot_data = df_occup[['OCCUPATION_LABEL', feature]].dropna()\n",
    "                        \n",
    "                        # Sort by median value for better visualization\n",
    "                        median_values = plot_data.groupby('OCCUPATION_LABEL')[feature].median().sort_values()\n",
    "                        occupation_order_plot = median_values.index.tolist()\n",
    "                        \n",
    "                        # Create color palette in the correct order for this plot\n",
    "                        plot_colors = [occupation_colors[occ] for occ in occupation_order_plot]\n",
    "                        \n",
    "                        sns.boxplot(data=plot_data, y='OCCUPATION_LABEL', x=feature, ax=ax,\n",
    "                                   order=occupation_order_plot, palette=plot_colors)\n",
    "                        sns.stripplot(data=plot_data, y='OCCUPATION_LABEL', x=feature, ax=ax,\n",
    "                                     color='black', alpha=0.3, size=2, order=occupation_order_plot)\n",
    "                        \n",
    "                        feat_stats = occup_comp_df[occup_comp_df['feature'] == feature].iloc[0]\n",
    "                        omega_sq = feat_stats['omega_squared']\n",
    "                        p_val = feat_stats['anova_pvalue_fdr']\n",
    "                        \n",
    "                        if p_val < 0.001:\n",
    "                            sig_stars = '***'\n",
    "                            p_text = 'p < 0.001'\n",
    "                        elif p_val < 0.01:\n",
    "                            sig_stars = '**'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        elif p_val < 0.05:\n",
    "                            sig_stars = '*'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        else:\n",
    "                            sig_stars = 'ns'\n",
    "                            p_text = f'p = {p_val:.2f}'\n",
    "                        \n",
    "                        clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "                        \n",
    "                        ax.set_title(f\"{clean_feature} {sig_stars}\\nω² = {omega_sq:.3f}, {p_text}\",\n",
    "                                    fontsize=9)\n",
    "                        ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "                        ax.set_ylabel('')\n",
    "                        ax.tick_params(axis='y', labelsize=8)\n",
    "                        ax.grid(True, alpha=0.3, axis='x')\n",
    "                    \n",
    "                    for idx in range(n_plots, 6):\n",
    "                        axes[idx].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(FIGURES_DIR / 'occupation_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "                    print(\"✓ Occupation differences visualization saved\")\n",
    "                    plt.show()\n",
    "            \n",
    "            # Plot 3: Effect size heatmap (if multiple significant features)\n",
    "            if n_sig >= 10:\n",
    "                print(\"\\nGenerating occupation effect size heatmap...\")\n",
    "                \n",
    "                # Get top 20 features by effect size\n",
    "                top_features = occup_comp_df.head(20)['feature'].tolist()\n",
    "                \n",
    "                # Create matrix of mean values for each occupation\n",
    "                heatmap_data = []\n",
    "                for feature in top_features:\n",
    "                    feature_means = []\n",
    "                    for occup_id in sorted(valid_groups.index):\n",
    "                        mean_val = df_occup[df_occup['OCCUPATION_ID'] == occup_id][feature].mean()\n",
    "                        feature_means.append(mean_val)\n",
    "                    heatmap_data.append(feature_means)\n",
    "                \n",
    "                heatmap_columns = [occupation_labels.get(oid, str(oid)) for oid in sorted(valid_groups.index)]\n",
    "                heatmap_df = pd.DataFrame(\n",
    "                    heatmap_data,\n",
    "                    index=[f.replace('_', ' ').replace('total ', '').replace('mean ', '').title()[:30] \n",
    "                           for f in top_features],\n",
    "                    columns=heatmap_columns\n",
    "                )\n",
    "                \n",
    "                # Normalize by row (z-score) for better visualization\n",
    "                heatmap_df_norm = heatmap_df.sub(heatmap_df.mean(axis=1), axis=0).div(heatmap_df.std(axis=1), axis=0)\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10, 12))\n",
    "                sns.heatmap(heatmap_df_norm, cmap='RdBu_r', center=0, \n",
    "                           cbar_kws={'label': 'Normalized Value (z-score)'},\n",
    "                           linewidths=0.5, ax=ax, fmt='.2f',\n",
    "                           xticklabels=True, yticklabels=True)\n",
    "                ax.set_title('Vessel Features by Occupation\\n(Top 20 by Effect Size, Z-score normalized)',\n",
    "                            fontsize=12, fontweight='bold', pad=20)\n",
    "                ax.set_xlabel('Occupation', fontsize=11)\n",
    "                ax.set_ylabel('Vessel Feature', fontsize=11)\n",
    "                plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "                plt.yticks(fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(FIGURES_DIR / 'occupation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "                print(\"✓ Occupation heatmap saved\")\n",
    "                plt.show()\n",
    "                \n",
    "    else:\n",
    "        print(f\"\\n⚠️  Insufficient data: only {len(valid_groups)} group(s) with n≥10\")\n",
    "else:\n",
    "    print(\"\\n⚠️  OCCUPATION_ID column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.5 Education/QUALIFICATION ANalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDUCATIONAL QUALIFICATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EDUCATIONAL QUALIFICATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'QUALIFICATION_ID' in df.columns:\n",
    "    df_qual = df[df['QUALIFICATION_ID'].notna()].copy()\n",
    "    qual_counts = df_qual['QUALIFICATION_ID'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nEducational qualification distribution:\")\n",
    "    print(f\"  Total with education data: {len(df_qual)} ({100*len(df_qual)/len(df):.1f}% of cohort)\")\n",
    "    print(f\"  Missing education data: {df['QUALIFICATION_ID'].isna().sum()} ({100*df['QUALIFICATION_ID'].isna().sum()/len(df):.1f}%)\")\n",
    "    \n",
    "    # Define qualification labels\n",
    "    qualification_labels = {\n",
    "        0: 'Missing',\n",
    "        1: 'None',\n",
    "        2: 'O-levels/GCSEs',\n",
    "        3: 'A-levels',\n",
    "        4: 'Further Ed.',\n",
    "        5: 'University'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEducation level categories:\")\n",
    "    for qual_id, count in qual_counts.items():\n",
    "        label = qualification_labels.get(int(qual_id), f'Level {int(qual_id)}')\n",
    "        print(f\"  {label}: {count} subjects ({100*count/len(df_qual):.1f}%)\")\n",
    "    \n",
    "    valid_groups = qual_counts[qual_counts >= 10]\n",
    "    \n",
    "    if len(valid_groups) >= 2:\n",
    "        print(f\"\\n✓ Sufficient data for analysis: {len(valid_groups)} groups with n≥10\")\n",
    "        \n",
    "        df_qual = df_qual[df_qual['QUALIFICATION_ID'].isin(valid_groups.index)].copy()\n",
    "        df_qual['QUALIFICATION_LABEL'] = df_qual['QUALIFICATION_ID'].map(qualification_labels)\n",
    "        \n",
    "        # Demographic characteristics\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Demographic characteristics by education level:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        sex_col = 'SEX_ID (1=m, 2=f)' if 'SEX_ID (1=m, 2=f)' in df_qual.columns else 'SEX_ID'\n",
    "        \n",
    "        qual_demo = df_qual.groupby('QUALIFICATION_LABEL').agg({\n",
    "            'subject_id': 'count',\n",
    "            'AGE': ['mean', 'std', 'min', 'max'],\n",
    "            sex_col: lambda x: (x == 1).sum(),\n",
    "            'HEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'WEIGHT': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan,\n",
    "            'BMI': lambda x: x[x > 0].mean() if (x > 0).any() else np.nan\n",
    "        })\n",
    "        \n",
    "        qual_demo.columns = ['N', 'Age_Mean', 'Age_SD', 'Age_Min', 'Age_Max',\n",
    "                            'N_Males', 'Height_Mean', 'Weight_Mean', 'BMI_Mean']\n",
    "        qual_demo['Pct_Male'] = 100 * qual_demo['N_Males'] / qual_demo['N']\n",
    "        \n",
    "        display(qual_demo.round(2))\n",
    "        \n",
    "        print(\"\\n⚠️  NOTE: Education is often a proxy for socioeconomic status\")\n",
    "        print(\"    May be associated with health behaviors and vascular risk factors\")\n",
    "        \n",
    "        # Education effects\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Education effects on vessel features:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        qual_comparisons = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            qual_data = [df_qual[df_qual['QUALIFICATION_ID'] == qid][feature].dropna()\n",
    "                        for qid in valid_groups.index]\n",
    "            qual_data = [d for d in qual_data if len(d) >= 5]\n",
    "            \n",
    "            if len(qual_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            f_stat, f_pval = f_oneway(*qual_data)\n",
    "            h_stat, h_pval = kruskal(*qual_data)\n",
    "            \n",
    "            # Test for trend (if ordinal)\n",
    "            all_qual_data = df_qual[['QUALIFICATION_ID', feature]].dropna()\n",
    "            if len(all_qual_data) > 20:\n",
    "                spearman_r, spearman_p = stats.spearmanr(all_qual_data['QUALIFICATION_ID'], \n",
    "                                                         all_qual_data[feature])\n",
    "            else:\n",
    "                spearman_r, spearman_p = np.nan, np.nan\n",
    "            \n",
    "            grand_mean = np.mean(np.concatenate(qual_data))\n",
    "            ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in qual_data)\n",
    "            ss_total = sum(np.sum((d - grand_mean)**2) for d in qual_data)\n",
    "            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "            \n",
    "            n_total = sum(len(d) for d in qual_data)\n",
    "            k_groups = len(qual_data)\n",
    "            omega_squared = (ss_between - (k_groups - 1) * (ss_total - ss_between) / (n_total - k_groups)) / \\\n",
    "                           (ss_total + (ss_total - ss_between) / (n_total - k_groups))\n",
    "            omega_squared = max(0, omega_squared)\n",
    "            \n",
    "            qual_comparisons.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'f_statistic': f_stat,\n",
    "                'anova_pvalue': f_pval,\n",
    "                'kw_pvalue': h_pval,\n",
    "                'spearman_r': spearman_r,\n",
    "                'spearman_pvalue': spearman_p,\n",
    "                'omega_squared': omega_squared\n",
    "            })\n",
    "        \n",
    "        qual_comp_df = pd.DataFrame(qual_comparisons)\n",
    "        \n",
    "        if len(qual_comp_df) > 0:\n",
    "            qual_comp_df['anova_pvalue_fdr'] = multipletests(qual_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "            qual_comp_df['spearman_pvalue_fdr'] = multipletests(qual_comp_df['spearman_pvalue'].fillna(1), \n",
    "                                                                 method='fdr_bh')[1]\n",
    "            qual_comp_df['significant_anova'] = qual_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "            qual_comp_df['significant_trend'] = qual_comp_df['spearman_pvalue_fdr'] < 0.05\n",
    "            qual_comp_df = qual_comp_df.sort_values('omega_squared', ascending=False)\n",
    "            \n",
    "            n_sig = qual_comp_df['significant_anova'].sum()\n",
    "            n_sig_trend = qual_comp_df['significant_trend'].sum()\n",
    "            \n",
    "            print(f\"\\nFeatures tested: {len(qual_comp_df)}\")\n",
    "            print(f\"Significant education differences (ANOVA, FDR<0.05): {n_sig} ({100*n_sig/len(qual_comp_df):.1f}%)\")\n",
    "            print(f\"Significant monotonic trends (Spearman): {n_sig_trend} ({100*n_sig_trend/len(qual_comp_df):.1f}%)\")\n",
    "            \n",
    "            if n_sig > 0:\n",
    "                print(f\"\\nTop 10 features with education effects:\")\n",
    "                display(qual_comp_df.head(10)[['feature', 'category', 'omega_squared', 'anova_pvalue_fdr', 'spearman_r']].round(4))\n",
    "            \n",
    "            qual_comp_df.to_csv(TABLES_DIR / 'education_comparisons.csv', index=False)\n",
    "            print(f\"\\n✓ Education analysis saved\")\n",
    "            \n",
    "            # ========================================================================\n",
    "            # VISUALIZATIONS\n",
    "            # ========================================================================\n",
    "            \n",
    "            # Plot 1: Education distribution with demographics\n",
    "            print(\"\\nGenerating education visualizations...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            fig.suptitle('Education Demographics Overview', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # 1. Education distribution\n",
    "            ax = axes[0, 0]\n",
    "            qual_demo_sorted = qual_demo.sort_values('N', ascending=False)\n",
    "            bars = ax.barh(range(len(qual_demo_sorted)), qual_demo_sorted['N'], \n",
    "                          color='steelblue', alpha=0.7, edgecolor='black')\n",
    "            ax.set_yticks(range(len(qual_demo_sorted)))\n",
    "            ax.set_yticklabels(qual_demo_sorted.index, fontsize=9)\n",
    "            ax.set_xlabel('Number of Subjects', fontsize=10)\n",
    "            ax.set_title('Sample Size by Education Level', fontsize=11, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add counts on bars\n",
    "            for i, v in enumerate(qual_demo_sorted['N']):\n",
    "                ax.text(v + 1, i, f'n={int(v)}', va='center', fontsize=9)\n",
    "            \n",
    "            # 2. Age distribution by education\n",
    "            ax = axes[0, 1]\n",
    "            plot_data = df_qual[['QUALIFICATION_LABEL', 'AGE']].dropna()\n",
    "            # Order by education level\n",
    "            education_order = [qualification_labels[i] for i in sorted(df_qual['QUALIFICATION_ID'].unique())]\n",
    "            sns.boxplot(data=plot_data, y='QUALIFICATION_LABEL', x='AGE', ax=ax,\n",
    "                       order=education_order, palette='viridis')\n",
    "            ax.set_xlabel('Age (years)', fontsize=10)\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_title('Age Distribution by Education Level', fontsize=11, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # 3. Sex distribution by education\n",
    "            ax = axes[1, 0]\n",
    "            sex_by_qual = df_qual.groupby('QUALIFICATION_LABEL')[sex_col].value_counts(normalize=True).unstack()\n",
    "            sex_by_qual = sex_by_qual.reindex(education_order)\n",
    "            sex_by_qual.plot(kind='barh', stacked=True, ax=ax, \n",
    "                           color=['#ff7f0e', '#ff69b4'], alpha=0.8)\n",
    "            ax.set_xlabel('Proportion', fontsize=10)\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_title('Sex Distribution by Education Level', fontsize=11, fontweight='bold')\n",
    "            ax.legend(['Male', 'Female'], fontsize=9)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # 4. BMI by education (if available)\n",
    "            ax = axes[1, 1]\n",
    "            if 'BMI' in df_qual.columns:\n",
    "                plot_data_bmi = df_qual[df_qual['BMI'] > 0][['QUALIFICATION_LABEL', 'BMI']].dropna()\n",
    "                if len(plot_data_bmi) > 0:\n",
    "                    sns.boxplot(data=plot_data_bmi, y='QUALIFICATION_LABEL', x='BMI', ax=ax,\n",
    "                               order=education_order, palette='viridis')\n",
    "                    ax.axvline(25, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Overweight')\n",
    "                    ax.axvline(30, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Obese')\n",
    "                    ax.set_xlabel('BMI (kg/m²)', fontsize=10)\n",
    "                    ax.set_ylabel('')\n",
    "                    ax.set_title('BMI Distribution by Education Level', fontsize=11, fontweight='bold')\n",
    "                    ax.legend(fontsize=8)\n",
    "                    ax.grid(axis='x', alpha=0.3)\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'Insufficient BMI data', ha='center', va='center',\n",
    "                           transform=ax.transAxes)\n",
    "                    ax.axis('off')\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'BMI data not available', ha='center', va='center',\n",
    "                       transform=ax.transAxes)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIGURES_DIR / 'education_demographics.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"✓ Education demographics plot saved\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot 2: Top vessel feature differences by education\n",
    "            if n_sig > 0:\n",
    "                top_qual_features = qual_comp_df[qual_comp_df['significant_anova']].head(6)['feature'].tolist()\n",
    "                \n",
    "                if len(top_qual_features) > 0:\n",
    "                    # Define consistent color mapping for education levels\n",
    "                    unique_education = df_qual['QUALIFICATION_LABEL'].unique()\n",
    "                    education_colors = dict(zip(unique_education, \n",
    "                                              sns.color_palette('viridis', n_colors=len(unique_education))))\n",
    "                    \n",
    "                    n_plots = min(6, len(top_qual_features))\n",
    "                    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "                    fig.suptitle('Top Education Differences in Vessel Features',\n",
    "                                fontsize=14, fontweight='bold')\n",
    "                    axes = axes.flatten()\n",
    "                    \n",
    "                    for idx, feature in enumerate(top_qual_features[:n_plots]):\n",
    "                        ax = axes[idx]\n",
    "                        \n",
    "                        plot_data = df_qual[['QUALIFICATION_LABEL', feature]].dropna()\n",
    "                        \n",
    "                        # Order by education level (natural ordering)\n",
    "                        education_order = [qualification_labels[i] for i in sorted(df_qual['QUALIFICATION_ID'].unique())]\n",
    "                        education_order = ['None', 'O-levels/GCSEs', 'A-levels', 'Further Ed.', 'University']\n",
    "\n",
    "                        # Create color palette in the correct order for this plot\n",
    "                        plot_colors = [education_colors[edu] for edu in education_order]\n",
    "                        \n",
    "                        sns.boxplot(data=plot_data, x='QUALIFICATION_LABEL', y=feature, ax=ax,\n",
    "                                order=education_order, palette=plot_colors)\n",
    "                        sns.stripplot(data=plot_data, x='QUALIFICATION_LABEL', y=feature, ax=ax,\n",
    "                                    color='black', alpha=0.3, size=2, order=education_order)\n",
    "                        \n",
    "                        feat_stats = qual_comp_df[qual_comp_df['feature'] == feature].iloc[0]\n",
    "                        omega_sq = feat_stats['omega_squared']\n",
    "                        p_val = feat_stats['anova_pvalue_fdr']\n",
    "                        \n",
    "                        if p_val < 0.001:\n",
    "                            sig_stars = '***'\n",
    "                            p_text = 'p < 0.001'\n",
    "                        elif p_val < 0.01:\n",
    "                            sig_stars = '**'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        elif p_val < 0.05:\n",
    "                            sig_stars = '*'\n",
    "                            p_text = f'p = {p_val:.3f}'\n",
    "                        else:\n",
    "                            sig_stars = 'ns'\n",
    "                            p_text = f'p = {p_val:.2f}'\n",
    "                        \n",
    "                        clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "                        \n",
    "                        # Add trend info if significant\n",
    "                        if feat_stats['significant_trend'] and not pd.isna(feat_stats['spearman_r']):\n",
    "                            spearman_r = feat_stats['spearman_r']\n",
    "                            trend_text = f\", ρ={spearman_r:.3f}\"\n",
    "                        else:\n",
    "                            trend_text = \"\"\n",
    "                        \n",
    "                        ax.set_title(f\"{clean_feature} {sig_stars}\\nω² = {omega_sq:.3f}, {p_text}{trend_text}\",\n",
    "                                    fontsize=9)\n",
    "                        ax.set_ylabel(feature.replace('_', ' ').title(), fontsize=9)\n",
    "                        ax.set_xlabel('')\n",
    "                        ax.tick_params(axis='x', labelsize=8, rotation=45)\n",
    "                        ax.grid(True, alpha=0.3, axis='x')\n",
    "                    \n",
    "                    for idx in range(n_plots, 6):\n",
    "                        axes[idx].axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(FIGURES_DIR / 'education_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "                    print(\"✓ Education differences visualization saved\")\n",
    "                    plt.show()\n",
    "            \n",
    "            # Plot 3: Effect size heatmap (if multiple significant features)\n",
    "            if n_sig >= 10:\n",
    "                print(\"\\nGenerating education effect size heatmap...\")\n",
    "                \n",
    "                # Get top 20 features by effect size\n",
    "                top_features = qual_comp_df.head(20)['feature'].tolist()\n",
    "                \n",
    "                # Create matrix of mean values for each education level\n",
    "                heatmap_data = []\n",
    "                education_order = [qualification_labels[i] for i in sorted(valid_groups.index)]\n",
    "                \n",
    "                for feature in top_features:\n",
    "                    feature_means = []\n",
    "                    for qual_id in sorted(valid_groups.index):\n",
    "                        mean_val = df_qual[df_qual['QUALIFICATION_ID'] == qual_id][feature].mean()\n",
    "                        feature_means.append(mean_val)\n",
    "                    heatmap_data.append(feature_means)\n",
    "                \n",
    "                heatmap_df = pd.DataFrame(\n",
    "                    heatmap_data,\n",
    "                    index=[f.replace('_', ' ').replace('total ', '').replace('mean ', '').title()[:30] \n",
    "                           for f in top_features],\n",
    "                    columns=education_order\n",
    "                )\n",
    "                \n",
    "                # Normalize by row (z-score) for better visualization\n",
    "                heatmap_df_norm = heatmap_df.sub(heatmap_df.mean(axis=1), axis=0).div(heatmap_df.std(axis=1), axis=0)\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10, 12))\n",
    "                sns.heatmap(heatmap_df_norm, cmap='RdBu_r', center=0, \n",
    "                           cbar_kws={'label': 'Normalized Value (z-score)'},\n",
    "                           linewidths=0.5, ax=ax, fmt='.2f',\n",
    "                           xticklabels=True, yticklabels=True)\n",
    "                ax.set_title('Vessel Features by Education Level\\n(Top 20 by Effect Size, Z-score normalized)',\n",
    "                            fontsize=12, fontweight='bold', pad=20)\n",
    "                ax.set_xlabel('Education Level', fontsize=11)\n",
    "                ax.set_ylabel('Vessel Feature', fontsize=11)\n",
    "                plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "                plt.yticks(fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(FIGURES_DIR / 'education_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "                print(\"✓ Education heatmap saved\")\n",
    "                plt.show()\n",
    "                \n",
    "    else:\n",
    "        print(f\"\\n⚠️  Insufficient data: only {len(valid_groups)} group(s) with n≥10\")\n",
    "else:\n",
    "    print(\"\\n⚠️  QUALIFICATION_ID column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.X SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY OF ALL DEMOGRAPHIC FACTORS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: DEMOGRAPHIC FACTORS AND VESSEL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "# Collect results from all analyses\n",
    "demographic_analyses = {\n",
    "    'Sex': ('sex_comp_df', 'significant'),\n",
    "    'Site': ('site_comp_df', 'significant_anova'),\n",
    "    'Ethnicity': ('ethnic_comp_df', 'significant_anova'),\n",
    "    'Marital Status (unadj)': ('marital_comp_df', 'significant'),\n",
    "    'Marital Status (adj)': ('marital_adj_df', 'significant_adjusted'),\n",
    "    'Occupation': ('occup_comp_df', 'significant'),\n",
    "    'Education': ('qual_comp_df', 'significant_anova')\n",
    "}\n",
    "\n",
    "print(\"\\nNumber of vessel features with significant associations:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for demo_name, (df_name, sig_col) in demographic_analyses.items():\n",
    "    if df_name in locals():\n",
    "        df_temp = locals()[df_name]\n",
    "        n_total = len(df_temp)\n",
    "        n_sig = df_temp[sig_col].sum() if sig_col in df_temp.columns else 0\n",
    "        pct_sig = 100 * n_sig / n_total if n_total > 0 else 0\n",
    "        \n",
    "        print(f\"{demo_name:25s}: {n_sig:3d}/{n_total:3d} ({pct_sig:5.1f}%)\")\n",
    "        \n",
    "        summary_results.append({\n",
    "            'demographic_factor': demo_name,\n",
    "            'n_features_tested': n_total,\n",
    "            'n_significant': n_sig,\n",
    "            'percent_significant': pct_sig\n",
    "        })\n",
    "\n",
    "if len(summary_results) > 0:\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    summary_df.to_csv(TABLES_DIR / 'demographic_factors_summary.csv', index=False)\n",
    "    print(f\"\\n✓ Summary saved to {TABLES_DIR / 'demographic_factors_summary.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY RECOMMENDATIONS FOR YOUR PAPER:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. PRIMARY COVARIATES (include in main analyses):\n",
    "   - Age (strongest predictor)\n",
    "   - Sex (biological differences)\n",
    "   - Site/Scanner (technical variation)\n",
    "\n",
    "2. SECONDARY FACTORS (mention in discussion):\n",
    "   - Ethnicity (if significant)\n",
    "   - Education/Occupation (socioeconomic proxies)\n",
    "\n",
    "3. CONFOUNDED FACTORS (interpret cautiously):\n",
    "   - Marital status (age-confounded)\n",
    "   - Analyze with age adjustment\n",
    "\n",
    "4. STATISTICAL APPROACH:\n",
    "   - Use multiple regression including age, sex, and site\n",
    "   - Report both unadjusted and adjusted analyses\n",
    "   - Consider interaction terms for key factors\n",
    "   - Apply FDR correction for multiple comparisons\n",
    "\n",
    "5. LIMITATIONS TO DISCUSS:\n",
    "   - Missing data in some demographic variables\n",
    "   - Potential residual confounding\n",
    "   - Cross-sectional design (no causality)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Regional Analysis (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-quality figure settings\n",
    "plt.rcParams['figure.dpi'] = 1000\n",
    "plt.rcParams['savefig.dpi'] = 1000\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 25\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 25\n",
    "plt.rcParams['ytick.labelsize'] = 25\n",
    "plt.rcParams['legend.fontsize'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_REGIONAL_DATA or regional_df is None:\n",
    "    print(\"\\n⚠️  No regional data available. Skipping regional analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"REGIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify region column ('region' or 'region_id' or 'region_label' or similar)\n",
    "    region_col = 'region' if 'region' in regional_df.columns else 'region_id' if 'region_id' in regional_df.columns else 'region_label' if 'region_label' in regional_df.columns else None\n",
    "    \n",
    "    \n",
    "    if region_col:\n",
    "        n_regions = regional_df[region_col].nunique()\n",
    "        print(f\"\\nNumber of regions: {n_regions}\")\n",
    "        print(f\"Regions: {sorted(regional_df[region_col].unique())}\")\n",
    "        \n",
    "        # Merge demographics for regional analysis\n",
    "        regional_with_demo = regional_df.merge(demographics[['subject_id', 'AGE', 'SEX_ID', 'ETHNIC_ID','MARITAL_ID','OCCUPATION_ID','QUALIFICATION_ID']], on='subject_id', how='left')\n",
    "        \n",
    "        # Regional variability analysis\n",
    "        print(f\"\\nRegional variability (coefficient of variation across regions):\")\n",
    "        \n",
    "        regional_cv = []\n",
    "        for feature in ALL_FEATURES:\n",
    "            if feature in regional_df.columns:\n",
    "                # Calculate mean and std per region\n",
    "                region_stats = regional_df.groupby(region_col)[feature].agg(['mean', 'std'])\n",
    "                \n",
    "                # Coefficient of variation across regions\n",
    "                cv = region_stats['mean'].std() / region_stats['mean'].mean() if region_stats['mean'].mean() != 0 else 0\n",
    "                \n",
    "                regional_cv.append({\n",
    "                    'feature': feature,\n",
    "                    'category': feature_categories[feature],\n",
    "                    'cv_across_regions': cv,\n",
    "                    'min_regional_mean': region_stats['mean'].min(),\n",
    "                    'max_regional_mean': region_stats['mean'].max()\n",
    "                })\n",
    "        \n",
    "        regional_cv_df = pd.DataFrame(regional_cv).sort_values('cv_across_regions', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 features by regional variability:\")\n",
    "        display(regional_cv_df.head(10))\n",
    "        \n",
    "        regional_cv_df.to_csv(TABLES_DIR / 'regional_variability.csv', index=False)\n",
    "        print(f\"\\n✓ Regional variability analysis saved to {TABLES_DIR / 'regional_variability.csv'}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No region identifier column found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10.2 In-depth Regional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_REGIONAL_DATA or regional_df is None:\n",
    "    print(\"\\n⚠️  No regional data available. Skipping regional analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE REGIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify region column\n",
    "    region_col = None\n",
    "    for col_name in ['region', 'region_id', 'region_label', 'Region', 'REGION']:\n",
    "        if col_name in regional_df.columns:\n",
    "            region_col = col_name\n",
    "            break\n",
    "    \n",
    "    if region_col is None:\n",
    "        print(\"\\n⚠️  No region identifier column found\")\n",
    "        print(f\"Available columns: {regional_df.columns.tolist()}\")\n",
    "    else:\n",
    "        n_regions = regional_df[region_col].nunique()\n",
    "        regions = sorted(regional_df[region_col].unique())\n",
    "        print(f\"\\nNumber of regions: {n_regions}\")\n",
    "        print(f\"Regions: {regions}\")\n",
    "        \n",
    "        # Merge demographics for regional analysis\n",
    "        demo_cols = ['subject_id'] + [c for c in ['AGE', 'SEX_ID', 'BMI', 'ETHNIC_ID',\t'MARITAL_ID',\t'OCCUPATION_ID',\t'QUALIFICATION_ID'] if c in demographics.columns]\n",
    "        regional_with_demo = regional_df.merge(demographics[demo_cols], on='subject_id', how='inner')\n",
    "        \n",
    "        print(f\"Regional data with demographics: {len(regional_with_demo)} rows, {regional_with_demo['subject_id'].nunique()} subjects\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 10.1 Regional Feature Distributions\n",
    "        # ====================================================================\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"10.1 REGIONAL FEATURE DISTRIBUTIONS\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Calculate statistics per region for all features\n",
    "        regional_stats = []\n",
    "        for feature in ALL_FEATURES:\n",
    "            if feature not in regional_df.columns:\n",
    "                continue\n",
    "                \n",
    "            for region in regions:\n",
    "                region_data = regional_df[regional_df[region_col] == region][feature].dropna()\n",
    "                \n",
    "                if len(region_data) > 0:\n",
    "                    regional_stats.append({\n",
    "                        'region': region,\n",
    "                        'feature': feature,\n",
    "                        'category': feature_categories[feature],\n",
    "                        'n': len(region_data),\n",
    "                        'mean': region_data.mean(),\n",
    "                        'std': region_data.std(),\n",
    "                        'median': region_data.median(),\n",
    "                        'min': region_data.min(),\n",
    "                        'max': region_data.max(),\n",
    "                        'cv': region_data.std() / region_data.mean() if region_data.mean() != 0 else 0\n",
    "                    })\n",
    "        \n",
    "        regional_stats_df = pd.DataFrame(regional_stats)\n",
    "        \n",
    "        if len(regional_stats_df) > 0:\n",
    "            # Calculate coefficient of variation across regions for each feature\n",
    "            regional_variability = []\n",
    "            for feature in ALL_FEATURES:\n",
    "                if feature not in regional_df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                feature_stats = regional_stats_df[regional_stats_df['feature'] == feature]\n",
    "                if len(feature_stats) > 0:\n",
    "                    cv_across_regions = feature_stats['mean'].std() / feature_stats['mean'].mean() if feature_stats['mean'].mean() != 0 else 0\n",
    "                    \n",
    "                    regional_variability.append({\n",
    "                        'feature': feature,\n",
    "                        'category': feature_categories[feature],\n",
    "                        'cv_across_regions': cv_across_regions,\n",
    "                        'min_regional_mean': feature_stats['mean'].min(),\n",
    "                        'max_regional_mean': feature_stats['mean'].max(),\n",
    "                        'fold_change': feature_stats['mean'].max() / feature_stats['mean'].min() if feature_stats['mean'].min() > 0 else np.inf\n",
    "                    })\n",
    "            \n",
    "            regional_var_df = pd.DataFrame(regional_variability).sort_values('cv_across_regions', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 15 features by regional variability (coefficient of variation):\")\n",
    "            display(regional_var_df.head(15))\n",
    "            \n",
    "            regional_var_df.to_csv(TABLES_DIR / 'regional_variability.csv', index=False)\n",
    "            print(f\"✓ Regional variability saved to {TABLES_DIR / 'regional_variability.csv'}\")\n",
    "            \n",
    "            # Save complete regional statistics\n",
    "            regional_stats_df.to_csv(TABLES_DIR / 'regional_statistics_all.csv', index=False)\n",
    "            print(f\"✓ Complete regional statistics saved to {TABLES_DIR / 'regional_statistics_all.csv'}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 10.2 Regional Heatmaps - Feature Means by Region\n",
    "        # ====================================================================\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"10.2 REGIONAL HEATMAPS\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Create heatmaps for each feature category\n",
    "        for category_name, features_list in [\n",
    "            ('Morphometric', MORPHOMETRIC_FEATURES),\n",
    "            ('Topological', TOPOLOGICAL_FEATURES),\n",
    "            ('Curvature', CURVATURE_FEATURES)\n",
    "        ]:\n",
    "            if len(features_list) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Select top features by variability in this category\n",
    "            cat_var = regional_var_df[regional_var_df['category'] == category_name.lower()]\n",
    "            top_features = cat_var.head(min(20, len(cat_var)))['feature'].tolist()\n",
    "            \n",
    "            if len(top_features) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Create pivot table for heatmap\n",
    "            heatmap_data = []\n",
    "            for feature in top_features:\n",
    "                if feature in regional_df.columns:\n",
    "                    means_by_region = regional_df.groupby(region_col)[feature].mean()\n",
    "                    heatmap_data.append(means_by_region)\n",
    "            \n",
    "            if len(heatmap_data) > 0:\n",
    "                heatmap_df = pd.DataFrame(heatmap_data, index=top_features)\n",
    "                \n",
    "                # Normalize each row (z-score) for better visualization\n",
    "                heatmap_normalized = heatmap_df.sub(heatmap_df.mean(axis=1), axis=0).div(heatmap_df.std(axis=1), axis=0)\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(max(10, n_regions * 0.5), max(8, len(top_features) * 0.3)))\n",
    "                sns.heatmap(heatmap_normalized, cmap='RdBu_r', center=0, \n",
    "                           cbar_kws={'label': 'Z-score'}, ax=ax,\n",
    "                           linewidths=0.5, linecolor='gray')\n",
    "                ax.set_title(f'{category_name} Features Across Regions (Z-scored)', \n",
    "                            fontweight='bold', fontsize=14)\n",
    "                ax.set_xlabel('Region', fontweight='bold')\n",
    "                ax.set_ylabel('Feature', fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(FIGURES_DIR / f'regional_heatmap_{category_name.lower()}.png', \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"✓ {category_name} regional heatmap saved\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 10.3 Regional Comparisons - ANOVA/Kruskal-Wallis\n",
    "        # ====================================================================\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"10.3 REGIONAL COMPARISONS (ANOVA)\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        regional_comparisons = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            if feature not in regional_df.columns:\n",
    "                continue\n",
    "            \n",
    "            # Get data for each region\n",
    "            region_data = [regional_df[regional_df[region_col] == r][feature].dropna() \n",
    "                          for r in regions]\n",
    "            \n",
    "            # Remove groups with <5 samples\n",
    "            region_data = [d for d in region_data if len(d) >= 5]\n",
    "            \n",
    "            if len(region_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # ANOVA and Kruskal-Wallis test\n",
    "                f_stat, f_pval = f_oneway(*region_data)\n",
    "                h_stat, h_pval = kruskal(*region_data)\n",
    "                \n",
    "                # Effect size (eta-squared)\n",
    "                grand_mean = np.mean(np.concatenate(region_data))\n",
    "                ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in region_data)\n",
    "                ss_total = sum(np.sum((d - grand_mean)**2) for d in region_data)\n",
    "                eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "                \n",
    "                regional_comparisons.append({\n",
    "                    'feature': feature,\n",
    "                    'category': feature_categories[feature],\n",
    "                    'n_regions': len(region_data),\n",
    "                    'f_statistic': f_stat,\n",
    "                    'anova_pvalue': f_pval,\n",
    "                    'kruskal_pvalue': h_pval,\n",
    "                    'eta_squared': eta_squared\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        regional_comp_df = pd.DataFrame(regional_comparisons)\n",
    "        \n",
    "        if len(regional_comp_df) > 0:\n",
    "            # Multiple testing correction\n",
    "            regional_comp_df['anova_pvalue_fdr'] = multipletests(regional_comp_df['anova_pvalue'], method='fdr_bh')[1]\n",
    "            regional_comp_df['significant'] = regional_comp_df['anova_pvalue_fdr'] < 0.05\n",
    "            regional_comp_df = regional_comp_df.sort_values('eta_squared', ascending=False)\n",
    "            \n",
    "            n_sig = regional_comp_df['significant'].sum()\n",
    "            print(f\"\\nRegional comparison summary:\")\n",
    "            print(f\"  Features tested: {len(regional_comp_df)}\")\n",
    "            print(f\"  Significant regional differences (ANOVA, FDR<0.05): {n_sig} ({100*n_sig/len(regional_comp_df):.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nTop 20 features with regional differences (by eta-squared):\")\n",
    "            display(regional_comp_df.head(20)[['feature', 'category', 'eta_squared', 'anova_pvalue_fdr']])\n",
    "            \n",
    "            regional_comp_df.to_csv(TABLES_DIR / 'regional_anova_comparisons.csv', index=False)\n",
    "            print(f\"✓ Regional ANOVA results saved to {TABLES_DIR / 'regional_anova_comparisons.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_path = '/home/falcetta/ISBI2025/LIANE/ArterialAtlas.nii.gz'  # Set to your atlas file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10.4 BRAIN ATLAS VISUALIZATION OF REGIONAL DIFFERENCES\n",
    "# ============================================================================\n",
    "\n",
    "if not IS_REGIONAL_DATA or regional_df is None or region_col is None:\n",
    "    print(\"\\n⚠️  Skipping atlas visualization - no regional data available\")\n",
    "else:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"10.4 BRAIN ATLAS VISUALIZATION\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Import required libraries for atlas visualization\n",
    "    from scipy import ndimage\n",
    "    \n",
    "    def plot_atlas_panel(ax, atlas_slice, value_slice, title):\n",
    "        \"\"\"Create a single atlas visualization panel.\"\"\"\n",
    "        # 1. WHITE background\n",
    "        ax.imshow(np.ones_like(atlas_slice), cmap='gray', vmin=0, vmax=1)\n",
    "        # 2. LIGHT GREY fill inside regions\n",
    "        mask = atlas_slice > 0\n",
    "        bg = np.ones_like(atlas_slice)\n",
    "        bg[mask] = 0.85\n",
    "        ax.imshow(bg, cmap='gray', vmin=0, vmax=1)\n",
    "        # 3. SMOOTH GREY contours\n",
    "        smooth = ndimage.gaussian_filter(atlas_slice.astype(float), sigma=0.8)\n",
    "        regions_arr = np.unique(atlas_slice)[1:]\n",
    "        if regions_arr.size:\n",
    "            ax.contour(\n",
    "                atlas_slice.astype(float), levels=regions_arr + 0.5, colors='#BBBBBB',\n",
    "                linewidths=0.5, alpha=0.6, antialiased=True\n",
    "            )\n",
    "        # 4. Value overlay\n",
    "        vals = np.ma.masked_where(value_slice == 0, value_slice)\n",
    "        im = ax.imshow(vals, cmap='RdBu_r', alpha=0.9)\n",
    "        ax.set_title(title, fontsize=14, pad=6)\n",
    "        ax.axis('off')\n",
    "        return im\n",
    "    \n",
    "    def create_regional_atlas_visualization(atlas_path, regional_data_dict, \n",
    "                                            output_prefix='regional_atlas',\n",
    "                                            title='Regional Feature Distribution',\n",
    "                                            vmin=None, vmax=None,\n",
    "                                            colorbar_label='Feature Value',\n",
    "                                            use_diverging=False):\n",
    "        \"\"\"\n",
    "        Create atlas visualization for regional data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        atlas_path : str or None\n",
    "            Path to atlas NIfTI file (if available)\n",
    "        regional_data_dict : dict\n",
    "            Dictionary mapping region IDs to values\n",
    "        output_prefix : str\n",
    "            Prefix for output files\n",
    "        title : str\n",
    "            Figure title\n",
    "        vmin, vmax : float or None\n",
    "            Value range for colormap\n",
    "        colorbar_label : str\n",
    "            Label for colorbar\n",
    "        use_diverging : bool\n",
    "            If True, use diverging colormap centered at 0 (for z-scores, correlations, effect sizes)\n",
    "        \"\"\"\n",
    "        if len(regional_data_dict) == 0:\n",
    "            print(f\"⚠️  No data to visualize for {output_prefix}\")\n",
    "            return None\n",
    "\n",
    "        # Check if atlas file exists\n",
    "        if atlas_path is None or not Path(atlas_path).exists():\n",
    "            print(f\"⚠️  Atlas file not found at {atlas_path}\")\n",
    "            print(\"   Creating bar plot visualization instead...\")\n",
    "            \n",
    "            # Create bar plot as alternative\n",
    "            fig, ax = plt.subplots(figsize=(12, max(6, len(regional_data_dict) * 0.3)))\n",
    "            \n",
    "            regions_list = sorted(regional_data_dict.keys())\n",
    "            values = [regional_data_dict[r] for r in regions_list]\n",
    "            \n",
    "            # Color bars by value\n",
    "            if vmin is None:\n",
    "                vmin = min(values)\n",
    "            if vmax is None:\n",
    "                vmax = max(values)\n",
    "            \n",
    "            # COLORS\n",
    "            if use_diverging:\n",
    "                # Center at 0 for z-scores\n",
    "                max_abs = max(abs(vmin), abs(vmax))\n",
    "                vmin, vmax = -max_abs, max_abs\n",
    "                norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "                cmap = plt.cm.RdBu_r\n",
    "            else:\n",
    "                norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "                cmap = plt.cm.OrRd\n",
    "            \n",
    "            colors = [cmap(norm(val)) for val in values]\n",
    "            bars = ax.barh(range(len(regions_list)), values, color=colors, alpha=0.8, edgecolor='black')\n",
    "            ax.set_yticks(range(len(regions_list)))\n",
    "            ax.set_yticklabels([f'Region {r}' for r in regions_list], fontsize=9)\n",
    "            ax.set_xlabel(colorbar_label, fontweight='bold', fontsize=11)\n",
    "            ax.set_title(title, fontweight='bold', fontsize=13)\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Add zero line for z-scores\n",
    "            if use_diverging:\n",
    "                ax.axvline(0, color='black', linestyle='-', linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "            # Add colorbar\n",
    "            sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "            sm.set_array([])\n",
    "            cbar = plt.colorbar(sm, ax=ax, pad=0.02)\n",
    "            cbar.set_label(colorbar_label, fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIGURES_DIR / f'{output_prefix}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            \n",
    "            print(f\"Loading atlas from {atlas_path}...\")\n",
    "            atlas_img = nib.load(atlas_path)\n",
    "            atlas_data = atlas_img.get_fdata()\n",
    "            \n",
    "            # Create value map\n",
    "            value_map = np.zeros_like(atlas_data)\n",
    "            \n",
    "            print(\"Mapping values to regions...\")\n",
    "            for region_id, value in regional_data_dict.items():\n",
    "                mask = atlas_data == region_id\n",
    "                if np.any(mask):\n",
    "                    value_map[mask] = value\n",
    "                    print(f\"  Region {region_id}: value={value:.3f}, voxels={np.sum(mask)}\")\n",
    "            \n",
    "            # Crop to brain region\n",
    "            atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "            value_cropped = value_map[50:-50, 40:-40, 50:-50]\n",
    "            \n",
    "            # Get slice positions\n",
    "            x, y, z = atlas_cropped.shape\n",
    "            x_slice, y_slice, z_slice = x//2, y//2, z//2\n",
    "            \n",
    "            slices = {\n",
    "                'Sagittal': (np.rot90(atlas_cropped[x_slice, :, :]), np.rot90(value_cropped[x_slice, :, :])),\n",
    "                'Coronal': (np.rot90(atlas_cropped[:, y_slice, :]), np.rot90(value_cropped[:, y_slice, :])),\n",
    "                'Axial': (np.rot90(atlas_cropped[:, :, z_slice]), np.rot90(value_cropped[:, :, z_slice]))\n",
    "            }\n",
    "            \n",
    "            # Create figure\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "            fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Determine value range if not provided\n",
    "            non_zero_values = value_map[value_map != 0]\n",
    "            if vmin is None and len(non_zero_values) > 0:\n",
    "                vmin = np.percentile(non_zero_values, 5)\n",
    "            if vmax is None and len(non_zero_values) > 0:\n",
    "                vmax = np.percentile(non_zero_values, 95)\n",
    "            \n",
    "            # For diverging colormaps, ensure symmetric range around 0\n",
    "            if use_diverging and vmin is not None and vmax is not None:\n",
    "                max_abs = max(abs(vmin), abs(vmax))\n",
    "                vmin, vmax = -max_abs, max_abs\n",
    "            \n",
    "            # Plot each view\n",
    "            for ax, (view_name, (atlas_sl, val_sl)) in zip(axes, slices.items()):\n",
    "                im = plot_atlas_panel(ax, atlas_sl, val_sl, view_name)\n",
    "                im.set_clim(vmin, vmax)\n",
    "            \n",
    "            # Use appropriate colormap\n",
    "            if use_diverging:\n",
    "                im.set_cmap('RdBu_r')\n",
    "            else:\n",
    "                im.set_cmap('OrRd')\n",
    "            \n",
    "            # Colorbar\n",
    "            cbar = fig.colorbar(im, ax=axes, orientation='horizontal', \n",
    "                                fraction=0.05, pad=0.05)\n",
    "            cbar.set_label(colorbar_label, fontsize=12)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIGURES_DIR / f'{output_prefix}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            plt.close('all')\n",
    "            \n",
    "            print(f\"✓ Atlas visualization saved to {FIGURES_DIR / f'{output_prefix}.png'}\")\n",
    "            \n",
    "            return value_map\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"⚠️  nibabel not installed. Install with: pip install nibabel\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error creating atlas visualization: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Check if we have significant regional differences to visualize\n",
    "    # ========================================================================\n",
    "    \n",
    "    if 'regional_comp_df' in locals() and len(regional_comp_df) > 0 and regional_comp_df['significant'].sum() > 0:\n",
    "        print(\"\\nGenerating brain atlas visualizations for top features with regional differences...\")\n",
    "        print(f\"Using atlas file: {atlas_path}\")\n",
    "        \n",
    "        # Get top features with significant regional differences\n",
    "        top_regional_features = regional_comp_df[regional_comp_df['significant']].head(6)['feature'].tolist()\n",
    "        \n",
    "        if len(top_regional_features) > 0:\n",
    "            # Visualize top 6 features with Z-SCORES\n",
    "            for idx, feature in enumerate(top_regional_features[:6]):\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Visualizing feature {idx+1}/6: {feature}\")\n",
    "                print('='*60)\n",
    "                \n",
    "                # Get regional means for this feature\n",
    "                regional_means = regional_df.groupby(region_col)[feature].mean()\n",
    "                \n",
    "                # CALCULATE Z-SCORES (normalize across regions)\n",
    "                mean_val = regional_means.mean()\n",
    "                std_val = regional_means.std()\n",
    "                \n",
    "                if std_val > 0:\n",
    "                    regional_zscores = (regional_means - mean_val) / std_val\n",
    "                else:\n",
    "                    regional_zscores = regional_means - mean_val\n",
    "                \n",
    "                # Create dictionary mapping region to Z-SCORE\n",
    "                regional_data_dict = regional_zscores.to_dict()\n",
    "                \n",
    "                print(f\"Regional z-scores for {feature}:\")\n",
    "                for region, zscore in sorted(regional_data_dict.items()):\n",
    "                    print(f\"  Region {region}: z={zscore:.3f}\")\n",
    "                \n",
    "                # Clean feature name\n",
    "                clean_feature = feature.replace('_', ' ').replace('total ', '').replace('mean ', '').title()\n",
    "                \n",
    "                # Get statistics\n",
    "                feat_stats = regional_comp_df[regional_comp_df['feature'] == feature].iloc[0]\n",
    "                eta_sq = feat_stats['eta_squared']\n",
    "                p_val = feat_stats['anova_pvalue_fdr']\n",
    "                \n",
    "                if p_val < 0.001:\n",
    "                    sig_text = '***'\n",
    "                    p_full = 'p < 0.001'\n",
    "                elif p_val < 0.01:\n",
    "                    sig_text = '**'\n",
    "                    p_full = f'p = {p_val:.3f}'\n",
    "                else:\n",
    "                    sig_text = '*'\n",
    "                    p_full = f'p = {p_val:.3f}'\n",
    "                \n",
    "                # Create visualization with Z-SCORES\n",
    "                create_regional_atlas_visualization(\n",
    "                    atlas_path=atlas_path,\n",
    "                    regional_data_dict=regional_data_dict,\n",
    "                    output_prefix=f'regional_atlas_{feature}_zscore',\n",
    "                    title=f'{clean_feature} (Z-scored) {sig_text}\\nη² = {eta_sq:.3f}, {p_full}',\n",
    "                    colorbar_label='Z-score',\n",
    "                    use_diverging=True  # Use RdBu_r colormap centered at 0\n",
    "                )\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"✓ BRAIN ATLAS VISUALIZATION COMPLETE\")\n",
    "            print(\"=\"*80)\n",
    "        else:\n",
    "            print(\"\\n⚠️  No significant regional differences found to visualize\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No regional comparison data available or no significant differences to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_it =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_REGIONAL_DATA or regional_df is None:\n",
    "    print(\"\\n⚠️  No regional data available. Skipping regional analysis.\")\n",
    "elif skip_it:\n",
    "    print(f\"This cell take too long: skip it\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE REGIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    # Identify region column\n",
    "    region_col = None\n",
    "    for col_name in ['region', 'region_id', 'region_label', 'Region', 'REGION']:\n",
    "        if col_name in regional_df.columns:\n",
    "            region_col = col_name\n",
    "            break\n",
    "    \n",
    "    if region_col is None:\n",
    "        print(\"\\n⚠️  No region identifier column found\")\n",
    "        print(f\"Available columns: {regional_df.columns.tolist()}\")\n",
    "    else:\n",
    "        # ====================================================================\n",
    "        # 10.4 Age-Region Interactions\n",
    "        # ====================================================================\n",
    "        if 'AGE' in regional_with_demo.columns:\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"10.4 AGE × REGION INTERACTIONS\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            age_region_interactions = []\n",
    "            \n",
    "            for feature in ALL_FEATURES:\n",
    "                if feature not in regional_with_demo.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare data\n",
    "                data_for_model = regional_with_demo[[region_col, 'AGE', feature]].dropna()\n",
    "                \n",
    "                if len(data_for_model) < 50:  # Need sufficient data\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Fit model with interaction\n",
    "                    formula = f'{feature} ~ AGE + C({region_col}) + AGE:C({region_col})'\n",
    "                    model = smf.ols(formula, data=data_for_model).fit()\n",
    "                    \n",
    "                    # Check if any interaction terms are significant\n",
    "                    interaction_terms = [p for p in model.pvalues.index if 'AGE:C(' in p]\n",
    "                    \n",
    "                    if len(interaction_terms) > 0:\n",
    "                        min_interaction_p = model.pvalues[interaction_terms].min()\n",
    "                        \n",
    "                        age_region_interactions.append({\n",
    "                            'feature': feature,\n",
    "                            'category': feature_categories[feature],\n",
    "                            'n': len(data_for_model),\n",
    "                            'min_interaction_pvalue': min_interaction_p,\n",
    "                            'model_r_squared': model.rsquared,\n",
    "                            'n_regions_tested': len(data_for_model[region_col].unique())\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if len(age_region_interactions) > 0:\n",
    "                age_region_df = pd.DataFrame(age_region_interactions)\n",
    "                age_region_df['min_interaction_pvalue_fdr'] = multipletests(\n",
    "                    age_region_df['min_interaction_pvalue'], method='fdr_bh')[1]\n",
    "                age_region_df['significant'] = age_region_df['min_interaction_pvalue_fdr'] < 0.05\n",
    "                age_region_df = age_region_df.sort_values('min_interaction_pvalue')\n",
    "                \n",
    "                n_sig = age_region_df['significant'].sum()\n",
    "                print(f\"\\nAge×Region interaction summary:\")\n",
    "                print(f\"  Features tested: {len(age_region_df)}\")\n",
    "                print(f\"  Significant interactions (FDR<0.05): {n_sig}\")\n",
    "                \n",
    "                if n_sig > 0:\n",
    "                    print(f\"\\nFeatures with significant Age×Region interaction:\")\n",
    "                    display(age_region_df[age_region_df['significant']][\n",
    "                        ['feature', 'category', 'min_interaction_pvalue_fdr', 'model_r_squared']\n",
    "                    ].head(15))\n",
    "                \n",
    "                age_region_df.to_csv(TABLES_DIR / 'age_region_interactions.csv', index=False)\n",
    "                print(f\"✓ Age×Region interactions saved to {TABLES_DIR / 'age_region_interactions.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_REGIONAL_DATA or regional_df is None:\n",
    "    print(\"\\n⚠️  No regional data available. Skipping regional analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE REGIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify region column\n",
    "    region_col = None\n",
    "    for col_name in ['region', 'region_id', 'region_label', 'Region', 'REGION']:\n",
    "        if col_name in regional_df.columns:\n",
    "            region_col = col_name\n",
    "            break\n",
    "    \n",
    "    if region_col is None:\n",
    "        print(\"\\n⚠️  No region identifier column found\")\n",
    "        print(f\"Available columns: {regional_df.columns.tolist()}\")\n",
    "    else:\n",
    "        # ====================================================================\n",
    "        # 10.5 Regional Age Correlations\n",
    "        # ====================================================================\n",
    "        if 'AGE' in regional_with_demo.columns:\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"10.5 REGIONAL AGE CORRELATIONS\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            regional_age_corr = []\n",
    "            \n",
    "            for region in regions:\n",
    "                region_data = regional_with_demo[regional_with_demo[region_col] == region]\n",
    "                \n",
    "                for feature in ALL_FEATURES:\n",
    "                    if feature not in region_data.columns:\n",
    "                        continue\n",
    "                    \n",
    "                    valid_data = region_data[['AGE', feature]].dropna()\n",
    "                    \n",
    "                    if len(valid_data) < 10:\n",
    "                        continue\n",
    "                    \n",
    "                    r, p = pearsonr(valid_data['AGE'], valid_data[feature])\n",
    "                    \n",
    "                    regional_age_corr.append({\n",
    "                        'region': region,\n",
    "                        'feature': feature,\n",
    "                        'category': feature_categories[feature],\n",
    "                        'n': len(valid_data),\n",
    "                        'correlation': r,\n",
    "                        'pvalue': p\n",
    "                    })\n",
    "            \n",
    "            if len(regional_age_corr) > 0:\n",
    "                regional_age_corr_df = pd.DataFrame(regional_age_corr)\n",
    "                \n",
    "                # FDR correction within each region\n",
    "                for region in regions:\n",
    "                    region_mask = regional_age_corr_df['region'] == region\n",
    "                    if region_mask.sum() > 0:\n",
    "                        regional_age_corr_df.loc[region_mask, 'pvalue_fdr'] = multipletests(\n",
    "                            regional_age_corr_df.loc[region_mask, 'pvalue'], method='fdr_bh')[1]\n",
    "                \n",
    "                regional_age_corr_df['significant'] = regional_age_corr_df['pvalue_fdr'] < 0.05\n",
    "                \n",
    "                # Summary by region\n",
    "                print(\"\\nSignificant age correlations by region:\")\n",
    "                for region in regions:\n",
    "                    region_data = regional_age_corr_df[regional_age_corr_df['region'] == region]\n",
    "                    n_sig = region_data['significant'].sum()\n",
    "                    print(f\"  Region {region}: {n_sig} significant correlations\")\n",
    "                \n",
    "                # Find features with consistent age effects across regions\n",
    "                feature_consistency = []\n",
    "                for feature in ALL_FEATURES:\n",
    "                    feature_data = regional_age_corr_df[regional_age_corr_df['feature'] == feature]\n",
    "                    if len(feature_data) >= n_regions * 0.5:  # Present in at least half the regions\n",
    "                        n_sig = feature_data['significant'].sum()\n",
    "                        mean_r = feature_data['correlation'].mean()\n",
    "                        std_r = feature_data['correlation'].std()\n",
    "                        \n",
    "                        feature_consistency.append({\n",
    "                            'feature': feature,\n",
    "                            'category': feature_categories[feature],\n",
    "                            'n_regions': len(feature_data),\n",
    "                            'n_significant': n_sig,\n",
    "                            'mean_correlation': mean_r,\n",
    "                            'std_correlation': std_r,\n",
    "                            'consistency': 1 - std_r  # Higher = more consistent\n",
    "                        })\n",
    "                \n",
    "                if len(feature_consistency) > 0:\n",
    "                    consistency_df = pd.DataFrame(feature_consistency)\n",
    "                    consistency_df = consistency_df.sort_values('n_significant', ascending=False)\n",
    "                    \n",
    "                    print(f\"\\nFeatures with most consistent age effects across regions:\")\n",
    "                    display(consistency_df.head(15))\n",
    "                    \n",
    "                    consistency_df.to_csv(TABLES_DIR / 'regional_age_correlation_consistency.csv', index=False)\n",
    "                    print(f\"✓ Regional age correlation consistency saved\")\n",
    "                \n",
    "                regional_age_corr_df.to_csv(TABLES_DIR / 'regional_age_correlations.csv', index=False)\n",
    "                print(f\"✓ Regional age correlations saved to {TABLES_DIR / 'regional_age_correlations.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_REGIONAL_DATA or regional_df is None:\n",
    "    print(\"\\n⚠️  No regional data available. Skipping regional analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE REGIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify region column\n",
    "    region_col = None\n",
    "    for col_name in ['region', 'region_id', 'region_label', 'Region', 'REGION']:\n",
    "        if col_name in regional_df.columns:\n",
    "            region_col = col_name\n",
    "            break\n",
    "    \n",
    "    if region_col is None:\n",
    "        print(\"\\n⚠️  No region identifier column found\")\n",
    "        print(f\"Available columns: {regional_df.columns.tolist()}\")\n",
    "    else:\n",
    "        # ====================================================================\n",
    "        # 10.6 Visualize Top Regional Differences\n",
    "        # ====================================================================\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"10.6 VISUALIZING TOP REGIONAL DIFFERENCES\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        if 'regional_comp_df' in locals() and len(regional_comp_df) > 0:\n",
    "            # Select top 6 features with largest regional differences\n",
    "            top_regional_features = regional_comp_df.head(6)['feature'].tolist()\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            fig.suptitle('Top Features with Regional Differences', fontsize=16, fontweight='bold')\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for idx, feature in enumerate(top_regional_features):\n",
    "                ax = axes[idx]\n",
    "                \n",
    "                # Extract data as numpy arrays first, then rebuild DataFrame\n",
    "                region_values = regional_df[region_col].values\n",
    "                feature_values = regional_df[feature].values\n",
    "                \n",
    "                # Create a clean DataFrame from scratch\n",
    "                plot_data = pd.DataFrame({\n",
    "                    'region': region_values,\n",
    "                    'feature_value': feature_values\n",
    "                })\n",
    "                \n",
    "                # Remove NaN values\n",
    "                plot_data = plot_data.dropna()\n",
    "                \n",
    "                # Convert region to string\n",
    "                plot_data['region'] = plot_data['region'].astype(str)\n",
    "                \n",
    "                # Get unique regions and sort\n",
    "                unique_regions = plot_data['region'].unique()\n",
    "                \n",
    "                try:\n",
    "                    region_order = sorted(unique_regions, \n",
    "                                        key=lambda x: int(x) if x.isdigit() else x)\n",
    "                except:\n",
    "                    region_order = sorted(unique_regions, key=str)\n",
    "                \n",
    "                # Box plot with explicit order\n",
    "                sns.boxplot(data=plot_data, x='region', y='feature_value', \n",
    "                        order=region_order, ax=ax, palette='Set3')\n",
    "                \n",
    "                # Get stats\n",
    "                feat_stats = regional_comp_df[regional_comp_df['feature'] == feature].iloc[0]\n",
    "                \n",
    "                ax.set_title(f\"{feature}\\nη²={feat_stats['eta_squared']:.3f}, p={feat_stats['anova_pvalue_fdr']:.2e}\",\n",
    "                        fontsize=10)\n",
    "                ax.set_xlabel('Region', fontweight='bold')\n",
    "                ax.set_ylabel(feature, fontsize=9)\n",
    "                ax.tick_params(axis='x', rotation=45)\n",
    "                ax.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # Reduce number of x-tick labels if too many regions\n",
    "                if len(region_order) > 15:\n",
    "                    current_labels = ax.get_xticklabels()\n",
    "                    for i, label in enumerate(current_labels):\n",
    "                        if i % 2 != 0:  # Hide every other label\n",
    "                            label.set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIGURES_DIR / 'regional_differences_top6.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"✓ Regional differences visualization saved to {FIGURES_DIR / 'regional_differences_top6.png'}\")\n",
    "        else:\n",
    "            print(\"⚠️  No regional comparison results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_REGIONAL_DATA or regional_df is None:\n",
    "    print(\"\\n⚠️  No regional data available. Skipping regional analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE REGIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify region column\n",
    "    region_col = None\n",
    "    for col_name in ['region', 'region_id', 'region_label', 'Region', 'REGION']:\n",
    "        if col_name in regional_df.columns:\n",
    "            region_col = col_name\n",
    "            break\n",
    "    \n",
    "    if region_col is None:\n",
    "        print(\"\\n⚠️  No region identifier column found\")\n",
    "        print(f\"Available columns: {regional_df.columns.tolist()}\")\n",
    "    else:\n",
    "        # ====================================================================\n",
    "        # 10.7 Regional Profiles - Feature Fingerprints\n",
    "        # ====================================================================\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"10.7 REGIONAL FEATURE PROFILES\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Create \"fingerprint\" plots showing normalized feature values per region\n",
    "        if 'regional_var_df' in locals() and len(regional_var_df) > 0:\n",
    "            # Select most variable features\n",
    "            top_variable_features = regional_var_df.head(min(30, len(regional_var_df)))['feature'].tolist()\n",
    "            \n",
    "            # Create normalized profiles\n",
    "            profile_data = []\n",
    "            for region in regions:\n",
    "                region_data = regional_df[regional_df[region_col] == region]\n",
    "                profile = {}\n",
    "                for feature in top_variable_features:\n",
    "                    if feature in region_data.columns:\n",
    "                        profile[feature] = region_data[feature].mean()\n",
    "                profile['region'] = region\n",
    "                profile_data.append(profile)\n",
    "            \n",
    "            profile_df = pd.DataFrame(profile_data).set_index('region')\n",
    "            \n",
    "            # Z-score normalization\n",
    "            profile_normalized = (profile_df - profile_df.mean()) / profile_df.std()\n",
    "            \n",
    "            # Plot\n",
    "            fig, ax = plt.subplots(figsize=(14, max(8, n_regions * 0.1)))\n",
    "            sns.heatmap(profile_normalized.T, cmap='RdBu_r', center=0, \n",
    "                       cbar_kws={'label': 'Z-score'}, ax=ax,\n",
    "                       linewidths=0.5, linecolor='gray')\n",
    "            ax.set_title('Regional Feature Profiles (Top Variable Features)', \n",
    "                        fontweight='bold', fontsize=14)\n",
    "            ax.set_xlabel('Region', fontweight='bold')\n",
    "            ax.set_ylabel('Feature', fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(FIGURES_DIR / 'regional_feature_profiles.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"✓ Regional feature profiles saved to {FIGURES_DIR / 'regional_feature_profiles.png'}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ COMPREHENSIVE REGIONAL ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nGenerated outputs:\")\n",
    "        print(f\"  • Regional variability analysis\")\n",
    "        print(f\"  • Regional heatmaps by feature category\")\n",
    "        print(f\"  • Regional ANOVA comparisons\")\n",
    "        if 'AGE' in regional_with_demo.columns:\n",
    "            print(f\"  • Age×Region interaction analysis\")\n",
    "            print(f\"  • Regional age correlations\")\n",
    "            print(f\"  • Regional age correlation consistency\")\n",
    "        print(f\"  • Top regional differences visualization\")\n",
    "        print(f\"  • Regional feature profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 10.8 Atlas Visualization of Regional Results - All Demographics\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10.8 ATLAS VISUALIZATION OF REGIONAL RESULTS - ALL DEMOGRAPHICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Import required libraries for atlas visualization\n",
    "from scipy import ndimage\n",
    "\n",
    "def plot_atlas_panel(ax, atlas_slice, value_slice, title):\n",
    "    \"\"\"Create a single atlas visualization panel.\"\"\"\n",
    "    # 1. WHITE background\n",
    "    ax.imshow(np.ones_like(atlas_slice), cmap='gray', vmin=0, vmax=1)\n",
    "    # 2. LIGHT GREY fill inside regions\n",
    "    mask = atlas_slice > 0\n",
    "    bg = np.ones_like(atlas_slice)\n",
    "    bg[mask] = 0.85\n",
    "    ax.imshow(bg, cmap='gray', vmin=0, vmax=1)\n",
    "    # 3. SMOOTH GREY contours\n",
    "    smooth = ndimage.gaussian_filter(atlas_slice.astype(float), sigma=0.8)\n",
    "    regions_arr = np.unique(atlas_slice)[1:]\n",
    "    if regions_arr.size:\n",
    "        ax.contour(\n",
    "            atlas_slice.astype(float), levels=regions_arr + 0.5, colors='#BBBBBB',\n",
    "            linewidths=0.5, alpha=0.6, antialiased=True\n",
    "        )\n",
    "    # 4. Value overlay\n",
    "    vals = np.ma.masked_where(value_slice == 0, value_slice)\n",
    "    im = ax.imshow(vals, cmap='OrRd', alpha=0.9)\n",
    "    ax.set_title(title, fontsize=14, pad=6)\n",
    "    ax.axis('off')\n",
    "    return im\n",
    "\n",
    "def create_regional_value_map(regional_stats_df, feature_name, region_col='region', \n",
    "                               aggregation='mean'):\n",
    "    \"\"\"\n",
    "    Create a dictionary mapping region IDs to feature values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    regional_stats_df : pd.DataFrame\n",
    "        DataFrame with regional statistics\n",
    "    feature_name : str\n",
    "        Name of the feature to map\n",
    "    region_col : str\n",
    "        Name of the region column\n",
    "    aggregation : str\n",
    "        How to aggregate ('mean', 'median', 'std', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Region ID -> feature value mapping\n",
    "    \"\"\"\n",
    "    feature_data = regional_stats_df[regional_stats_df['feature'] == feature_name]\n",
    "    \n",
    "    if len(feature_data) == 0:\n",
    "        return {}\n",
    "    \n",
    "    if aggregation == 'mean':\n",
    "        return dict(zip(feature_data['region'], feature_data['mean']))\n",
    "    elif aggregation == 'median':\n",
    "        return dict(zip(feature_data['region'], feature_data['median']))\n",
    "    elif aggregation == 'std':\n",
    "        return dict(zip(feature_data['region'], feature_data['std']))\n",
    "    elif aggregation == 'cv':\n",
    "        return dict(zip(feature_data['region'], feature_data['cv']))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "\n",
    "def create_regional_atlas_visualization(atlas_path, regional_data_dict, \n",
    "                                        output_prefix='regional_atlas',\n",
    "                                        title='Regional Feature Distribution',\n",
    "                                        vmin=None, vmax=None,\n",
    "                                        colorbar_label='Feature Value',\n",
    "                                        use_diverging=False):\n",
    "    \"\"\"\n",
    "    Create atlas visualization for regional data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    atlas_path : str or None\n",
    "        Path to atlas NIfTI file (if available)\n",
    "    regional_data_dict : dict\n",
    "        Dictionary mapping region IDs to values\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "    title : str\n",
    "        Figure title\n",
    "    vmin, vmax : float or None\n",
    "        Value range for colormap\n",
    "    colorbar_label : str\n",
    "        Label for colorbar\n",
    "    use_diverging : bool\n",
    "        If True, use diverging colormap centered at 0 (for correlations, effect sizes)\n",
    "    \"\"\"\n",
    "    if len(regional_data_dict) == 0:\n",
    "        print(f\"⚠️  No data to visualize for {output_prefix}\")\n",
    "        return None\n",
    "\n",
    "    # Check if atlas file exists\n",
    "    if atlas_path is None or not Path(atlas_path).exists():\n",
    "        print(f\"⚠️  Atlas file not found at {atlas_path}\")\n",
    "        print(\"   Creating bar plot visualization instead...\")\n",
    "        \n",
    "        # Create bar plot as alternative\n",
    "        fig, ax = plt.subplots(figsize=(12, max(6, len(regional_data_dict) * 0.3)))\n",
    "        \n",
    "        regions_list = sorted(regional_data_dict.keys())\n",
    "        values = [regional_data_dict[r] for r in regions_list]\n",
    "        \n",
    "        bars = ax.barh(range(len(regions_list)), values, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax.set_yticks(range(len(regions_list)))\n",
    "        ax.set_yticklabels([f'Region {r}' for r in regions_list])\n",
    "        ax.set_xlabel(colorbar_label, fontweight='bold')\n",
    "        ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Color bars by value\n",
    "        if vmin is None:\n",
    "            vmin = min(values)\n",
    "        if vmax is None:\n",
    "            vmax = max(values)\n",
    "        \n",
    "        # COLORSSS\n",
    "        if use_diverging:\n",
    "            # Center at 0 for correlations\n",
    "            max_abs = max(abs(vmin), abs(vmax))\n",
    "            vmin, vmax = -max_abs, max_abs\n",
    "            norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "            cmap = plt.cm.RdBu_r\n",
    "        else:\n",
    "            norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "            cmap = plt.cm.OrRd\n",
    "        \n",
    "        for bar, val in zip(bars, values):\n",
    "            bar.set_color(cmap(norm(val)))\n",
    "        \n",
    "        # Add colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax, pad=0.02)\n",
    "        cbar.set_label(colorbar_label)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / f'{output_prefix}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        import nibabel as nib\n",
    "        \n",
    "        print(f\"Loading atlas from {atlas_path}...\")\n",
    "        atlas_img = nib.load(atlas_path)\n",
    "        atlas_data = atlas_img.get_fdata()\n",
    "        \n",
    "        # Create value map\n",
    "        value_map = np.zeros_like(atlas_data)\n",
    "        \n",
    "        print(\"Mapping values to regions...\")\n",
    "        for region_id, value in regional_data_dict.items():\n",
    "            mask = atlas_data == region_id\n",
    "            if np.any(mask):\n",
    "                value_map[mask] = value\n",
    "                print(f\"  Region {region_id}: value={value:.3f}, voxels={np.sum(mask)}\")\n",
    "        \n",
    "        # Crop to brain region\n",
    "        atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "        value_cropped = value_map[50:-50, 40:-40, 50:-50]\n",
    "        \n",
    "        # Get slice positions\n",
    "        x, y, z = atlas_cropped.shape\n",
    "        x_slice, y_slice, z_slice = x//2, y//2, z//2\n",
    "        \n",
    "        slices = {\n",
    "            'Sagittal': (np.rot90(atlas_cropped[x_slice, :, :]), np.rot90(value_cropped[x_slice, :, :])),\n",
    "            'Coronal': (np.rot90(atlas_cropped[:, y_slice, :]), np.rot90(value_cropped[:, y_slice, :])),\n",
    "            'Axial': (np.rot90(atlas_cropped[:, :, z_slice]), np.rot90(value_cropped[:, :, z_slice]))\n",
    "        }\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Determine value range if not provided\n",
    "        non_zero_values = value_map[value_map != 0]\n",
    "        if vmin is None and len(non_zero_values) > 0:\n",
    "            vmin = np.percentile(non_zero_values, 5)\n",
    "        if vmax is None and len(non_zero_values) > 0:\n",
    "            vmax = np.percentile(non_zero_values, 95)\n",
    "        \n",
    "        # For diverging colormaps, ensure symmetric range around 0\n",
    "        if use_diverging and vmin is not None and vmax is not None:\n",
    "            max_abs = max(abs(vmin), abs(vmax))\n",
    "            vmin, vmax = -max_abs, max_abs\n",
    "        \n",
    "        # Plot each view\n",
    "        for ax, (view_name, (atlas_sl, val_sl)) in zip(axes, slices.items()):\n",
    "            im = plot_atlas_panel(ax, atlas_sl, val_sl, view_name)\n",
    "            im.set_clim(vmin, vmax)\n",
    "        \n",
    "        # Use appropriate colormap\n",
    "        if use_diverging:\n",
    "            im.set_cmap('OrRd')\n",
    "        \n",
    "        # Colorbar\n",
    "        cbar = fig.colorbar(im, ax=axes, orientation='horizontal', \n",
    "                            fraction=0.05, pad=0.05)\n",
    "        cbar.set_label(colorbar_label, fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / f'{output_prefix}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Atlas visualization saved to {FIGURES_DIR / f'{output_prefix}.png'}\")\n",
    "        \n",
    "        return value_map\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️  nibabel not installed. Install with: pip install nibabel\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error creating atlas visualization: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_PATH = '/home/falcetta/ISBI2025/LIANE/ArterialAtlas.nii.gz'  # Set to your atlas file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_it=False\n",
    "# ====================================================================\n",
    "# Create atlas visualizations for all demographic analyses\n",
    "# ====================================================================\n",
    "if skip_it:\n",
    "    print(\"Skipping\")\n",
    "else:\n",
    "    if len(regional_stats_df) > 0:\n",
    "        print(\"\\nCreating comprehensive atlas visualizations...\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # 1. Regional Feature Mean Values\n",
    "        # ================================================================\n",
    "        print(\"\\n1. REGIONAL FEATURE MEAN VALUES\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Select all features sorted by variability\n",
    "        top_variable = regional_var_df['feature'].tolist()\n",
    "        top_n_features = len(top_variable)\n",
    "        \n",
    "        for feature in top_variable:\n",
    "            print(f\"\\n  Creating visualization for {feature}...\")\n",
    "            \n",
    "            # Get regional means for this feature\n",
    "            regional_values = create_regional_value_map(\n",
    "                regional_stats_df, \n",
    "                feature, \n",
    "                region_col=region_col, \n",
    "                aggregation='mean'\n",
    "            )\n",
    "            \n",
    "            # Create visualization\n",
    "            create_regional_atlas_visualization(\n",
    "                atlas_path=ATLAS_PATH,\n",
    "                regional_data_dict=regional_values,\n",
    "                output_prefix=f'regional_atlas_{feature}',\n",
    "                title=f'{feature} - Mean by Region',\n",
    "                colorbar_label=f'{feature} (mean)'\n",
    "            )\n",
    "        \n",
    "        # ================================================================\n",
    "        # 2. Demographic Correlations by Region\n",
    "        # ================================================================\n",
    "        if 'regional_demo_correlations' in locals() and len(regional_demo_correlations) > 0:\n",
    "            print(\"\\n2. DEMOGRAPHIC CORRELATIONS BY REGION\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            for demo_var, regional_corr_df in regional_demo_correlations.items():\n",
    "                print(f\"\\n  Processing {demo_var} correlations...\")\n",
    "                \n",
    "                # Find features with strongest average correlation\n",
    "                avg_corr = regional_corr_df.groupby('feature')['correlation'].agg(['mean', 'std'])\n",
    "                avg_corr['abs_mean'] = avg_corr['mean'].abs()\n",
    "                top_corr_features = avg_corr.nlargest(min(10, len(avg_corr)), 'abs_mean').index.tolist()\n",
    "                \n",
    "                for feature in top_corr_features:\n",
    "                    print(f\"    Creating {demo_var} correlation map for {feature}...\")\n",
    "                    \n",
    "                    # Get correlations by region\n",
    "                    feature_corr = regional_corr_df[regional_corr_df['feature'] == feature]\n",
    "                    corr_dict = dict(zip(feature_corr['region'], feature_corr['correlation']))\n",
    "                    \n",
    "                    create_regional_atlas_visualization(\n",
    "                        atlas_path=ATLAS_PATH,\n",
    "                        regional_data_dict=corr_dict,\n",
    "                        output_prefix=f'regional_{demo_var.lower()}_corr_{feature}',\n",
    "                        title=f'{feature} - {demo_var} Correlation by Region',\n",
    "                        vmin=-1, vmax=1,\n",
    "                        colorbar_label=f'Pearson r with {demo_var}',\n",
    "                        use_diverging=True\n",
    "                    )\n",
    "        \n",
    "        # ================================================================\n",
    "        # 3. Visualize Sex Differences (Cohen's d) by Region\n",
    "        # ================================================================\n",
    "        if 'regional_sex_df' in locals() and len(regional_sex_df) > 0:\n",
    "            print(\"\\n3. SEX DIFFERENCES (COHEN'S D) BY REGION\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            # Find features with largest average effect size\n",
    "            avg_effect = regional_sex_df.groupby('feature')['cohens_d'].agg(['mean', 'std'])\n",
    "            avg_effect['abs_mean'] = avg_effect['mean'].abs()\n",
    "            top_sex_features = avg_effect.nlargest(min(10, len(avg_effect)), 'abs_mean').index.tolist()\n",
    "            \n",
    "            for feature in top_sex_features:\n",
    "                print(f\"\\n  Creating sex difference map for {feature}...\")\n",
    "                \n",
    "                # Get Cohen's d by region\n",
    "                feature_sex = regional_sex_df[regional_sex_df['feature'] == feature]\n",
    "                sex_dict = dict(zip(feature_sex['region'], feature_sex['cohens_d']))\n",
    "                \n",
    "                create_regional_atlas_visualization(\n",
    "                    atlas_path=ATLAS_PATH,\n",
    "                    regional_data_dict=sex_dict,\n",
    "                    output_prefix=f'regional_sex_cohens_d_{feature}',\n",
    "                    title=f'{feature} - Sex Difference (Cohen\\'s d) by Region',\n",
    "                    vmin=-2, vmax=2,\n",
    "                    colorbar_label='Cohen\\'s d (Male - Female)',\n",
    "                    use_diverging=True\n",
    "                )\n",
    "        \n",
    "        # ================================================================\n",
    "        # 4. Export Comprehensive Regional Data\n",
    "        # ================================================================\n",
    "        print(\"\\n4. EXPORTING COMPREHENSIVE REGIONAL DATA\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Create comprehensive export with all demographics\n",
    "        comprehensive_export = []\n",
    "        \n",
    "        for feature in ALL_FEATURES:\n",
    "            if feature not in regional_df.columns:\n",
    "                continue\n",
    "                \n",
    "            for region in regions:\n",
    "                region_data = regional_df[regional_df[region_col] == region][feature].dropna()\n",
    "                \n",
    "                if len(region_data) > 0:\n",
    "                    export_row = {\n",
    "                        'region': region,\n",
    "                        'feature': feature,\n",
    "                        'category': feature_categories[feature],\n",
    "                        'mean': region_data.mean(),\n",
    "                        'std': region_data.std(),\n",
    "                        'median': region_data.median(),\n",
    "                        'n_subjects': len(region_data)\n",
    "                    }\n",
    "                    \n",
    "                    # Add correlations for all demographic variables\n",
    "                    if 'regional_demo_correlations' in locals():\n",
    "                        for demo_var, regional_corr_df in regional_demo_correlations.items():\n",
    "                            corr_match = regional_corr_df[\n",
    "                                (regional_corr_df['region'] == region) & \n",
    "                                (regional_corr_df['feature'] == feature)\n",
    "                            ]\n",
    "                            if len(corr_match) > 0:\n",
    "                                export_row[f'{demo_var}_correlation'] = corr_match.iloc[0]['correlation']\n",
    "                                export_row[f'{demo_var}_pvalue'] = corr_match.iloc[0]['pvalue']\n",
    "                                export_row[f'{demo_var}_significant'] = corr_match.iloc[0]['significant']\n",
    "                    \n",
    "                    # Add sex differences\n",
    "                    if 'regional_sex_df' in locals():\n",
    "                        sex_match = regional_sex_df[\n",
    "                            (regional_sex_df['region'] == region) & \n",
    "                            (regional_sex_df['feature'] == feature)\n",
    "                        ]\n",
    "                        if len(sex_match) > 0:\n",
    "                            export_row['sex_cohens_d'] = sex_match.iloc[0]['cohens_d']\n",
    "                            export_row['sex_pvalue'] = sex_match.iloc[0]['pvalue']\n",
    "                            export_row['sex_significant'] = sex_match.iloc[0]['significant']\n",
    "                    \n",
    "                    comprehensive_export.append(export_row)\n",
    "        \n",
    "        comprehensive_export_df = pd.DataFrame(comprehensive_export)\n",
    "        comprehensive_export_df.to_csv(TABLES_DIR / 'regional_comprehensive_demographics.csv', index=False)\n",
    "        print(f\"✓ Comprehensive regional data exported to {TABLES_DIR / 'regional_comprehensive_demographics.csv'}\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # 5. Create JSON Export for Custom Atlas Tools\n",
    "        # ================================================================\n",
    "        print(\"\\n5. CREATING JSON EXPORTS\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        json_exports = {}\n",
    "        \n",
    "        # Export mean values for top features\n",
    "        json_exports['feature_means'] = {}\n",
    "        for feature in top_variable[:10]:\n",
    "            regional_values = create_regional_value_map(\n",
    "                regional_stats_df, feature, region_col='region', aggregation='mean'\n",
    "            )\n",
    "            json_exports['feature_means'][feature] = {str(k): float(v) for k, v in regional_values.items()}\n",
    "        \n",
    "        # Export demographic correlations\n",
    "        if 'regional_demo_correlations' in locals():\n",
    "            json_exports['demographic_correlations'] = {}\n",
    "            for demo_var, regional_corr_df in regional_demo_correlations.items():\n",
    "                json_exports['demographic_correlations'][demo_var] = {}\n",
    "                \n",
    "                # Top 5 features for each demographic variable\n",
    "                avg_corr = regional_corr_df.groupby('feature')['correlation'].agg('mean')\n",
    "                top_features = avg_corr.abs().nlargest(5).index.tolist()\n",
    "                \n",
    "                for feature in top_features:\n",
    "                    feature_corr = regional_corr_df[regional_corr_df['feature'] == feature]\n",
    "                    corr_dict = dict(zip(feature_corr['region'], feature_corr['correlation']))\n",
    "                    json_exports['demographic_correlations'][demo_var][feature] = {\n",
    "                        str(k): float(v) for k, v in corr_dict.items()\n",
    "                    }\n",
    "        \n",
    "        # Export sex differences\n",
    "        if 'regional_sex_df' in locals():\n",
    "            json_exports['sex_differences'] = {}\n",
    "            avg_effect = regional_sex_df.groupby('feature')['cohens_d'].agg('mean')\n",
    "            top_sex_features = avg_effect.abs().nlargest(5).index.tolist()\n",
    "            \n",
    "            for feature in top_sex_features:\n",
    "                feature_sex = regional_sex_df[regional_sex_df['feature'] == feature]\n",
    "                sex_dict = dict(zip(feature_sex['region'], feature_sex['cohens_d']))\n",
    "                json_exports['sex_differences'][feature] = {\n",
    "                    str(k): float(v) for k, v in sex_dict.items()\n",
    "                }\n",
    "        \n",
    "        # Save JSON\n",
    "        import json\n",
    "        with open(TABLES_DIR / 'regional_atlas_data_all_demographics.json', 'w') as f:\n",
    "            json.dump(json_exports, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ JSON export saved to {TABLES_DIR / 'regional_atlas_data_all_demographics.json'}\")\n",
    "        \n",
    "        # ================================================================\n",
    "        # Summary\n",
    "        # ================================================================\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ COMPREHENSIVE ATLAS VISUALIZATION COMPLETE\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nGenerated outputs:\")\n",
    "        print(f\"  • Regional mean value visualizations for top {top_n_features} features\")\n",
    "        if 'regional_demo_correlations' in locals():\n",
    "            print(f\"  • Demographic correlation maps for {len(regional_demo_correlations)} variables:\")\n",
    "            for demo_var in regional_demo_correlations.keys():\n",
    "                print(f\"    - {demo_var}\")\n",
    "        if 'regional_sex_df' in locals():\n",
    "            print(f\"  • Sex difference (Cohen's d) maps\")\n",
    "        print(f\"  • Comprehensive CSV export with all demographics\")\n",
    "        print(f\"  • JSON export for custom atlas visualization\")\n",
    "        print(\"\\n💡 TIPS:\")\n",
    "        print(\"  • Set ATLAS_PATH to your atlas NIfTI file for proper brain overlays\")\n",
    "        print(\"  • All correlation maps use diverging colormaps centered at 0\")\n",
    "        print(\"  • Effect sizes (Cohen's d) also use diverging colormaps\")\n",
    "        print(\"  • CSV file contains correlations with ALL demographic variables\")\n",
    "        print(\"  • JSON file can be used with custom atlas visualization scripts\")\n",
    "        print(\"  • Use the JSON/CSV exports with 3D Slicer, FSLeyes, or other tools\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n⚠️  No regional statistics available for atlas visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for atlas visualization\n",
    "from scipy import ndimage\n",
    "\n",
    "def plot_atlas_panel(ax, atlas_slice, value_slice, title, cmap='OrRd', vmin=None, vmax=None):\n",
    "    \"\"\"Create a single atlas visualization panel.\"\"\"\n",
    "    # 1. WHITE background\n",
    "    ax.imshow(np.ones_like(atlas_slice), cmap='gray', vmin=0, vmax=1)\n",
    "    # 2. LIGHT GREY fill inside regions\n",
    "    mask = atlas_slice > 0\n",
    "    bg = np.ones_like(atlas_slice)\n",
    "    bg[mask] = 0.85\n",
    "    ax.imshow(bg, cmap='gray', vmin=0, vmax=1)\n",
    "    # 3. SMOOTH GREY contours\n",
    "    smooth = ndimage.gaussian_filter(atlas_slice.astype(float), sigma=0.8)\n",
    "    regions_arr = np.unique(atlas_slice)[1:]\n",
    "    if regions_arr.size:\n",
    "        ax.contour(\n",
    "            atlas_slice.astype(float), levels=regions_arr + 0.5, colors='#BBBBBB',\n",
    "            linewidths=0.5, alpha=0.6, antialiased=True\n",
    "        )\n",
    "    # 4. Value overlay\n",
    "    vals = np.ma.masked_where(value_slice == 0, value_slice)\n",
    "    im = ax.imshow(vals, cmap=cmap, vmin=vmin, vmax=vmax, alpha=0.9)\n",
    "    ax.set_title(title, fontsize=12, pad=4)\n",
    "    ax.axis('off')\n",
    "    return im\n",
    "\n",
    "def calculate_regional_means_by_group(regional_df, feature, region_col, group_col, group_value):\n",
    "    \"\"\"Calculate mean feature values by region for a specific group.\"\"\"\n",
    "    group_data = regional_df[regional_df[group_col] == group_value]\n",
    "    regional_means = {}\n",
    "    \n",
    "    for region in group_data[region_col].unique():\n",
    "        region_data = group_data[group_data[region_col] == region][feature].dropna()\n",
    "        if len(region_data) > 0:\n",
    "            regional_means[region] = region_data.mean()\n",
    "    \n",
    "    return regional_means\n",
    "\n",
    "def create_group_comparison_atlas(atlas_path, regional_df, feature, region_col, \n",
    "                                  group_col, group_labels, output_prefix,\n",
    "                                  main_title, colorbar_label):\n",
    "    \"\"\"\n",
    "    Create side-by-side atlas comparison for different demographic groups.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    atlas_path : str\n",
    "        Path to atlas NIfTI file\n",
    "    regional_df : pd.DataFrame\n",
    "        Regional data with demographics\n",
    "    feature : str\n",
    "        Feature to visualize\n",
    "    region_col : str\n",
    "        Column name for regions\n",
    "    group_col : str\n",
    "        Column name for grouping variable\n",
    "    group_labels : dict\n",
    "        Dictionary mapping group values to display labels\n",
    "    output_prefix : str\n",
    "        Prefix for output filename\n",
    "    main_title : str\n",
    "        Main figure title\n",
    "    colorbar_label : str\n",
    "        Label for colorbar\n",
    "    \"\"\"\n",
    "    \n",
    "    if atlas_path is None or not Path(atlas_path).exists():\n",
    "        print(f\"⚠️  Atlas file not found. Creating bar plots instead...\")\n",
    "        \n",
    "        # Create bar plot comparison\n",
    "        n_groups = len(group_labels)\n",
    "        fig, axes = plt.subplots(1, n_groups, figsize=(6*n_groups, 8))\n",
    "        if n_groups == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        fig.suptitle(main_title, fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Get all regions\n",
    "        all_regions = sorted(regional_df[region_col].unique())\n",
    "        \n",
    "        # Determine global value range\n",
    "        all_values = []\n",
    "        for group_val in group_labels.keys():\n",
    "            regional_means = calculate_regional_means_by_group(\n",
    "                regional_df, feature, region_col, group_col, group_val\n",
    "            )\n",
    "            all_values.extend(regional_means.values())\n",
    "        \n",
    "        vmin, vmax = min(all_values), max(all_values)\n",
    "        norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "        cmap = plt.cm.OrRd\n",
    "        \n",
    "        for idx, (group_val, group_label) in enumerate(group_labels.items()):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            regional_means = calculate_regional_means_by_group(\n",
    "                regional_df, feature, region_col, group_col, group_val\n",
    "            )\n",
    "            \n",
    "            regions = sorted(regional_means.keys())\n",
    "            values = [regional_means[r] for r in regions]\n",
    "            \n",
    "            bars = ax.barh(range(len(regions)), values, edgecolor='black')\n",
    "            ax.set_yticks(range(len(regions)))\n",
    "            ax.set_yticklabels([f'R{r}' for r in regions], fontsize=8)\n",
    "            ax.set_xlabel(colorbar_label, fontweight='bold')\n",
    "            ax.set_title(group_label, fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Color bars\n",
    "            for bar, val in zip(bars, values):\n",
    "                bar.set_color(cmap(norm(val)))\n",
    "        \n",
    "        # Add shared colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=axes, orientation='horizontal', \n",
    "                           fraction=0.05, pad=0.08)\n",
    "        cbar.set_label(colorbar_label, fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / f'{output_prefix}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        import nibabel as nib\n",
    "        \n",
    "        print(f\"Loading atlas from {atlas_path}...\")\n",
    "        atlas_img = nib.load(atlas_path)\n",
    "        atlas_data = atlas_img.get_fdata()\n",
    "        \n",
    "        # Calculate regional means for each group\n",
    "        group_value_maps = {}\n",
    "        all_values = []\n",
    "        \n",
    "        for group_val, group_label in group_labels.items():\n",
    "            regional_means = calculate_regional_means_by_group(\n",
    "                regional_df, feature, region_col, group_col, group_val\n",
    "            )\n",
    "            \n",
    "            # Create value map\n",
    "            value_map = np.zeros_like(atlas_data)\n",
    "            for region_id, value in regional_means.items():\n",
    "                mask = atlas_data == region_id\n",
    "                if np.any(mask):\n",
    "                    value_map[mask] = value\n",
    "            \n",
    "            group_value_maps[group_label] = value_map\n",
    "            non_zero = value_map[value_map != 0]\n",
    "            if len(non_zero) > 0:\n",
    "                all_values.extend(non_zero)\n",
    "        \n",
    "        # Determine global value range\n",
    "        vmin = np.percentile(all_values, 5)\n",
    "        vmax = np.percentile(all_values, 95)\n",
    "        \n",
    "        # Create figure with subplots for each group\n",
    "        n_groups = len(group_labels)\n",
    "        fig, axes = plt.subplots(n_groups, 3, figsize=(15, 5*n_groups))\n",
    "        if n_groups == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        fig.suptitle(main_title, fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Sort groups by label (if numbers, sort numerically)\n",
    "        sorted_groups = sorted(group_value_maps.items(), key=lambda x: x[0])\n",
    "        \n",
    "        for group_idx, (group_label, value_map) in enumerate(sorted_groups):\n",
    "            print(f\"  Plotting group: {group_label}...!!!\")\n",
    "            # Crop to brain region\n",
    "            atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "            value_cropped = value_map[50:-50, 40:-40, 50:-50]\n",
    "            \n",
    "            # Get slice positions\n",
    "            x, y, z = atlas_cropped.shape\n",
    "            x_slice, y_slice, z_slice = x//2, y//2, z//2\n",
    "            \n",
    "            slices = {\n",
    "                'Sagittal': (np.rot90(atlas_cropped[x_slice, :, :]), \n",
    "                           np.rot90(value_cropped[x_slice, :, :])),\n",
    "                'Coronal': (np.rot90(atlas_cropped[:, y_slice, :]), \n",
    "                          np.rot90(value_cropped[:, y_slice, :])),\n",
    "                'Axial': (np.rot90(atlas_cropped[:, :, z_slice]), \n",
    "                        np.rot90(value_cropped[:, :, z_slice]))\n",
    "            }\n",
    "            \n",
    "            # Plot each view for this group\n",
    "            for view_idx, (view_name, (atlas_sl, val_sl)) in enumerate(slices.items()):\n",
    "                ax = axes[group_idx, view_idx]\n",
    "                title = f\"{group_label} - {view_name}\"\n",
    "                im = plot_atlas_panel(ax, atlas_sl, val_sl, title, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        # Add shared colorbar\n",
    "        cbar = fig.colorbar(im, ax=axes, orientation='horizontal', \n",
    "                           fraction=0.03, pad=0.04)\n",
    "        cbar.set_label(colorbar_label, fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / f'{output_prefix}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Group comparison atlas saved to {FIGURES_DIR / f'{output_prefix}.png'}\")\n",
    "        \n",
    "        return group_value_maps\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️  nibabel not installed. Install with: pip install nibabel\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error creating atlas visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# ENHANCED ATLAS VISUALIZATION - ALL DEMOGRAPHICS WITH GROUP COMPARISONS\n",
    "# ====================================================================\n",
    "\"\"\"\n",
    "This enhanced version automatically:\n",
    "1. Detects all available demographic variables\n",
    "2. Creates visualizations for each demographic\n",
    "3. Shows group differences with statistical tests\n",
    "4. Handles both categorical and continuous variables\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy import ndimage, stats\n",
    "from scipy.stats import f_oneway, ttest_ind, mannwhitneyu\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENHANCED ATLAS VISUALIZATION - ALL DEMOGRAPHICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ====================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ====================================================================\n",
    "\n",
    "def plot_atlas_panel(ax, atlas_slice, value_slice, title, cmap='OrRd', vmin=None, vmax=None):\n",
    "    \"\"\"Create a single atlas visualization panel.\"\"\"\n",
    "    # 1. WHITE background\n",
    "    ax.imshow(np.ones_like(atlas_slice), cmap='gray', vmin=0, vmax=1)\n",
    "    # 2. LIGHT GREY fill inside regions\n",
    "    mask = atlas_slice > 0\n",
    "    bg = np.ones_like(atlas_slice)\n",
    "    bg[mask] = 0.85\n",
    "    ax.imshow(bg, cmap='gray', vmin=0, vmax=1)\n",
    "    # 3. SMOOTH GREY contours\n",
    "    smooth = ndimage.gaussian_filter(atlas_slice.astype(float), sigma=0.8)\n",
    "    regions_arr = np.unique(atlas_slice)[1:]\n",
    "    if regions_arr.size:\n",
    "        ax.contour(\n",
    "            atlas_slice.astype(float), levels=regions_arr + 0.5, colors='#BBBBBB',\n",
    "            linewidths=0.5, alpha=0.6, antialiased=True\n",
    "        )\n",
    "    # 4. Value overlay\n",
    "    vals = np.ma.masked_where(value_slice == 0, value_slice)\n",
    "    im = ax.imshow(vals, cmap=cmap, vmin=vmin, vmax=vmax, alpha=0.9)\n",
    "    ax.set_title(title, fontsize=12, pad=4)\n",
    "    ax.axis('off')\n",
    "    return im\n",
    "\n",
    "\n",
    "def calculate_regional_means_by_group(regional_df, feature, region_col, group_col, group_value):\n",
    "    \"\"\"Calculate mean feature values by region for a specific group.\"\"\"\n",
    "    group_data = regional_df[regional_df[group_col] == group_value]\n",
    "    regional_means = {}\n",
    "    \n",
    "    for region in group_data[region_col].unique():\n",
    "        region_data = group_data[group_data[region_col] == region][feature].dropna()\n",
    "        if len(region_data) > 0:\n",
    "            regional_means[region] = region_data.mean()\n",
    "    \n",
    "    return regional_means\n",
    "\n",
    "def detect_demographic_variables(df, interactive=True):\n",
    "    \"\"\"\n",
    "    Automatically detect all available demographic variables in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        interactive: If True, prompts user to select variables. If False, includes all detected.\n",
    "    \n",
    "    Returns dict with:\n",
    "        - 'categorical': list of categorical demographic variables with their unique values\n",
    "        - 'continuous': list of continuous demographic variables\n",
    "    \"\"\"\n",
    "    demographics = {\n",
    "        'categorical': {},\n",
    "        'continuous': []\n",
    "    }\n",
    "    \n",
    "    # Known demographic columns\n",
    "    demo_candidates = {\n",
    "        'categorical': ['SEX_ID', 'sex', 'ETHNIC_ID', 'ethnicity', 'MARITAL_ID', \n",
    "                       'marital_status', 'OCCUPATION_ID', 'occupation', 'site',\n",
    "                       'QUALIFICATION_ID', 'qualification', 'age_group', 'bmi_category'],\n",
    "        'continuous': ['AGE', 'age', 'HEIGHT', 'height', 'WEIGHT', 'weight', 'BMI', 'bmi']\n",
    "    }\n",
    "    # Detect available variables\n",
    "    available_categorical = {}\n",
    "    available_continuous = []\n",
    "    \n",
    "    print(\"Detecting available demographic variables...\")\n",
    "    print(\"\\nCategorical variables:\")\n",
    "    for col in demo_candidates['categorical']:\n",
    "        if col in df.columns:\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            if 2 <= len(unique_vals) <= 10:\n",
    "                value_counts = df[col].value_counts()\n",
    "                if all(value_counts >= 10):\n",
    "                    available_categorical[col] = sorted(unique_vals)\n",
    "                    print(f\"  ✓ {col} ({len(unique_vals)} groups)\")\n",
    "    \n",
    "    print(\"\\nContinuous variables:\")\n",
    "    for col in demo_candidates['continuous']:\n",
    "        if col in df.columns:\n",
    "            if df[col].dropna().nunique() > 10 and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                available_continuous.append(col)\n",
    "                print(f\"  ✓ {col}\")\n",
    "            else:\n",
    "                print(f\"  ✗ {col} (not enough unique values or not numeric)\")\n",
    "    \n",
    "    # Interactive selection\n",
    "    if interactive and (available_categorical or available_continuous):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SELECT VARIABLES TO INCLUDE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Select categorical variables\n",
    "        if available_categorical:\n",
    "            print(\"\\nCategorical variables (enter numbers separated by commas, or 'all'):\")\n",
    "            cat_list = list(available_categorical.keys())\n",
    "            for i, col in enumerate(cat_list, 1):\n",
    "                print(f\"  {i}. {col}\")\n",
    "            \n",
    "            cat_input = input(\"Select categorical variables: \").strip()\n",
    "            if cat_input.lower() == 'all':\n",
    "                demographics['categorical'] = available_categorical\n",
    "            elif cat_input:\n",
    "                selected_indices = [int(x.strip()) - 1 for x in cat_input.split(',') if x.strip()]\n",
    "                demographics['categorical'] = {\n",
    "                    cat_list[i]: available_categorical[cat_list[i]] \n",
    "                    for i in selected_indices if 0 <= i < len(cat_list)\n",
    "                }\n",
    "        \n",
    "        # Select continuous variables\n",
    "        if available_continuous:\n",
    "            print(\"\\nContinuous variables (enter numbers separated by commas, or 'all'):\")\n",
    "            for i, col in enumerate(available_continuous, 1):\n",
    "                print(f\"  {i}. {col}\")\n",
    "            \n",
    "            cont_input = input(\"Select continuous variables: \").strip()\n",
    "            if cont_input.lower() == 'all':\n",
    "                demographics['continuous'] = available_continuous\n",
    "            elif cont_input:\n",
    "                selected_indices = [int(x.strip()) - 1 for x in cont_input.split(',') if x.strip()]\n",
    "                demographics['continuous'] = [\n",
    "                    available_continuous[i] \n",
    "                    for i in selected_indices if 0 <= i < len(available_continuous)\n",
    "                ]\n",
    "        \n",
    "        print(\"\\n✓ Selection complete!\")\n",
    "    else:\n",
    "        # Non-interactive mode: include all\n",
    "        demographics['categorical'] = available_categorical\n",
    "        demographics['continuous'] = available_continuous\n",
    "    \n",
    "    return demographics\n",
    "\n",
    "\n",
    "def create_group_labels(df, group_col):\n",
    "    \"\"\"Create human-readable labels for group values.\"\"\"\n",
    "    labels = {}\n",
    "    \n",
    "    # Special handling for common variables\n",
    "    if group_col in ['SEX_ID', 'sex']:\n",
    "        labels = {1: 'Male', 2: 'Female'}\n",
    "    elif 'age_group' in group_col.lower():\n",
    "        # Use the actual category labels\n",
    "        for val in df[group_col].dropna().unique():\n",
    "            labels[val] = str(val)\n",
    "    elif 'bmi' in group_col.lower():\n",
    "        # Use the actual category labels\n",
    "        for val in df[group_col].dropna().unique():\n",
    "            labels[val] = str(val)\n",
    "    elif group_col == 'site':\n",
    "        # Site labels as-is\n",
    "        for val in df[group_col].dropna().unique():\n",
    "            labels[val] = str(val)\n",
    "    else:\n",
    "        # Generic labeling\n",
    "        for val in sorted(df[group_col].dropna().unique()):\n",
    "            labels[val] = f\"{group_col}={val}\"\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_group_differences(regional_df, feature, region_col, group_col):\n",
    "    \"\"\"\n",
    "    Calculate statistical differences between groups for each region.\n",
    "    \n",
    "    Returns DataFrame with regional p-values and effect sizes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    groups = regional_df[group_col].dropna().unique()\n",
    "    \n",
    "    for region in regional_df[region_col].unique():\n",
    "        region_data = regional_df[regional_df[region_col] == region]\n",
    "        \n",
    "        # Get data for each group\n",
    "        group_data = []\n",
    "        for grp in groups:\n",
    "            grp_vals = region_data[region_data[group_col] == grp][feature].dropna()\n",
    "            if len(grp_vals) >= 3:  # Minimum sample size\n",
    "                group_data.append(grp_vals.values)\n",
    "        \n",
    "        # Skip if insufficient groups\n",
    "        if len(group_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Statistical test\n",
    "        if len(groups) == 2:\n",
    "            # Two groups: t-test or Mann-Whitney\n",
    "            stat, p_val = ttest_ind(group_data[0], group_data[1], equal_var=False)\n",
    "            test_name = 't-test'\n",
    "            \n",
    "            # Calculate Cohen's d (effect size)\n",
    "            pooled_std = np.sqrt((np.std(group_data[0])**2 + np.std(group_data[1])**2) / 2)\n",
    "            cohens_d = (np.mean(group_data[0]) - np.mean(group_data[1])) / pooled_std if pooled_std > 0 else 0\n",
    "            effect_size = abs(cohens_d)\n",
    "        else:\n",
    "            # Multiple groups: ANOVA\n",
    "            stat, p_val = f_oneway(*group_data)\n",
    "            test_name = 'ANOVA'\n",
    "            \n",
    "            # Calculate eta-squared (effect size)\n",
    "            grand_mean = np.mean(np.concatenate(group_data))\n",
    "            ss_between = sum(len(g) * (np.mean(g) - grand_mean)**2 for g in group_data)\n",
    "            ss_total = sum((x - grand_mean)**2 for g in group_data for x in g)\n",
    "            effect_size = ss_between / ss_total if ss_total > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'region': region,\n",
    "            'p_value': p_val,\n",
    "            'test': test_name,\n",
    "            'effect_size': effect_size,\n",
    "            'significant': p_val < 0.05,\n",
    "            'n_groups': len(group_data),\n",
    "            'total_n': sum(len(g) for g in group_data)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def create_difference_visualization(atlas_path, regional_df, feature, region_col, \n",
    "                                   group_col, output_prefix, main_title):\n",
    "    \"\"\"\n",
    "    Create visualization showing WHERE groups differ (p-value heatmap on atlas).\n",
    "    \"\"\"\n",
    "    # Calculate differences\n",
    "    diff_results = calculate_group_differences(regional_df, feature, region_col, group_col)\n",
    "    \n",
    "    if len(diff_results) == 0:\n",
    "        print(f\"    ⚠️  No statistical results for {feature}\")\n",
    "        return\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    fig.suptitle(f\"{main_title}\\nRegional Group Differences\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. P-value map on atlas (if atlas available)\n",
    "    if atlas_path and Path(atlas_path).exists():\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            atlas_img = nib.load(atlas_path)\n",
    "            atlas_data = atlas_img.get_fdata()\n",
    "            \n",
    "            # Create p-value map\n",
    "            pval_map = np.zeros_like(atlas_data)\n",
    "            for _, row in diff_results.iterrows():\n",
    "                mask = atlas_data == row['region']\n",
    "                if np.any(mask):\n",
    "                    # Use -log10(p) for better visualization\n",
    "                    pval_map[mask] = -np.log10(row['p_value'] + 1e-10)\n",
    "\n",
    "            \n",
    "            # Plot\n",
    "            atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "            pval_cropped = pval_map[50:-50, 40:-40, 50:-50]\n",
    "            x, y, z = atlas_cropped.shape\n",
    "            \n",
    "            ax = axes[0, 0]\n",
    "            atlas_slice = np.rot90(atlas_cropped[:, :, z//2])\n",
    "            pval_slice = np.rot90(pval_cropped[:, :, z//2])\n",
    "            \n",
    "            im = plot_atlas_panel(ax, atlas_slice, pval_slice, \n",
    "                                 \"Significance Map (-log10 p-value)\",\n",
    "                                 cmap='YlOrRd', vmin=0, vmax=3)\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Add significance threshold line\n",
    "            ax.text(0.02, 0.98, \"Red = more significant\\np < 0.05 ≈ 1.3\", \n",
    "                   transform=ax.transAxes, fontsize=9, va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️  Could not create atlas visualization: {e}\")\n",
    "            axes[0, 0].text(0.5, 0.5, \"Atlas not available\", \n",
    "                          ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].axis('off')\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, \"Atlas not available\", \n",
    "                       ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "        axes[0, 0].axis('off')\n",
    "    \n",
    "    # 2. Effect size by region\n",
    "    ax = axes[0, 1]\n",
    "    sorted_diff = diff_results.sort_values('effect_size', ascending=False).head(20)\n",
    "    colors = ['red' if sig else 'gray' for sig in sorted_diff['significant']]\n",
    "    ax.barh(range(len(sorted_diff)), sorted_diff['effect_size'], color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(sorted_diff)))\n",
    "    ax.set_yticklabels([f\"R{int(r)}\" for r in sorted_diff['region']], fontsize=8)\n",
    "    ax.set_xlabel('Effect Size', fontweight='bold')\n",
    "    ax.set_title('Top 20 Regions by Effect Size\\n(Red = significant p<0.05)', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 3. P-value distribution\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(diff_results['p_value'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(0.05, color='red', linestyle='--', linewidth=2, label='p = 0.05')\n",
    "    ax.set_xlabel('P-value', fontweight='bold')\n",
    "    ax.set_ylabel('Number of Regions')\n",
    "    ax.set_title(f'Distribution of P-values\\n{sum(diff_results[\"significant\"])} significant regions', \n",
    "                fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Summary statistics table\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "STATISTICAL SUMMARY\n",
    "\n",
    "Test: {diff_results['test'].iloc[0]}\n",
    "Total Regions Tested: {len(diff_results)}\n",
    "Significant Regions (p<0.05): {sum(diff_results['significant'])} ({100*sum(diff_results['significant'])/len(diff_results):.1f}%)\n",
    "Bonferroni Threshold: {0.05/len(diff_results):.4f}\n",
    "Significant (Bonferroni): {sum(diff_results['p_value'] < 0.05/len(diff_results))}\n",
    "\n",
    "EFFECT SIZES\n",
    "Mean: {diff_results['effect_size'].mean():.3f}\n",
    "Median: {diff_results['effect_size'].median():.3f}\n",
    "Max: {diff_results['effect_size'].max():.3f}\n",
    "\n",
    "TOP 5 MOST DIFFERENT REGIONS:\n",
    "\"\"\"\n",
    "    top_5 = diff_results.nlargest(5, 'effect_size')\n",
    "    for idx, row in top_5.iterrows():\n",
    "        sig_marker = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"\"\n",
    "        summary_text += f\"\\nR{int(row['region'])}: ES={row['effect_size']:.3f}, p={row['p_value']:.4f} {sig_marker}\"\n",
    "    \n",
    "    ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, \n",
    "           fontsize=10, verticalalignment='top', family='monospace',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / f'{output_prefix}_differences.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return diff_results\n",
    "\n",
    "\n",
    "def create_pairwise_comparison_plots(regional_df, feature, region_col, group_col, \n",
    "                                    output_prefix, main_title):\n",
    "    \"\"\"\n",
    "    For categorical variables with 2+ groups, create pairwise comparison plots.\n",
    "    Shows violin/box plots for each region comparing groups.\n",
    "    \"\"\"\n",
    "    groups = sorted(regional_df[group_col].dropna().unique())\n",
    "    n_groups = len(groups)\n",
    "    \n",
    "    if n_groups < 2:\n",
    "        return\n",
    "    \n",
    "    # Get top 12 most variable regions\n",
    "    regional_variance = []\n",
    "    for region in regional_df[region_col].unique():\n",
    "        region_data = regional_df[regional_df[region_col] == region][feature].dropna()\n",
    "        if len(region_data) > 0:\n",
    "            regional_variance.append({\n",
    "                'region': region,\n",
    "                'variance': region_data.var(),\n",
    "                'mean': region_data.mean()\n",
    "            })\n",
    "    \n",
    "    top_regions = sorted(regional_variance, key=lambda x: x['variance'], reverse=True)[:12]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(f\"{main_title}\\nGroup Comparisons by Region\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    group_labels = create_group_labels(regional_df, group_col)\n",
    "    \n",
    "    for idx, reg_info in enumerate(top_regions):\n",
    "        ax = axes[idx]\n",
    "        region = reg_info['region']\n",
    "        \n",
    "        # Prepare data for this region\n",
    "        plot_data = []\n",
    "        for grp in groups:\n",
    "            grp_data = regional_df[\n",
    "                (regional_df[region_col] == region) & \n",
    "                (regional_df[group_col] == grp)\n",
    "            ][feature].dropna()\n",
    "            \n",
    "            for val in grp_data:\n",
    "                plot_data.append({\n",
    "                    'Region': f\"R{int(region)}\",\n",
    "                    'Group': group_labels.get(grp, str(grp)),\n",
    "                    'Value': val\n",
    "                })\n",
    "        \n",
    "        if len(plot_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "        \n",
    "        # Violin + box plot\n",
    "        sns.violinplot(data=plot_df, x='Group', y='Value', ax=ax, \n",
    "                      inner=None, alpha=0.3)\n",
    "        sns.boxplot(data=plot_df, x='Group', y='Value', ax=ax,\n",
    "                   width=0.3, showcaps=True, boxprops=dict(alpha=0.7))\n",
    "        \n",
    "        # Add mean line\n",
    "        group_means = plot_df.groupby('Group')['Value'].mean()\n",
    "        for i, (grp, mean_val) in enumerate(group_means.items()):\n",
    "            ax.plot([i-0.4, i+0.4], [mean_val, mean_val], \n",
    "                   'r-', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f\"Region {int(region)}\", fontweight='bold')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(feature if idx % 4 == 0 else '')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Rotate x labels if needed\n",
    "        if n_groups > 2:\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(len(top_regions), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / f'{output_prefix}_pairwise.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_continuous_correlation_atlas(atlas_path, regional_df, feature, region_col,\n",
    "                                       continuous_var, output_prefix, main_title,\n",
    "                                       method='pearson'):\n",
    "    \"\"\"\n",
    "    For continuous demographic variables, show correlation strength on atlas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    atlas_path : str\n",
    "        Path to the atlas NIfTI file\n",
    "    regional_df : pd.DataFrame\n",
    "        DataFrame with regional data\n",
    "    feature : str\n",
    "        Name of the feature column to correlate\n",
    "    region_col : str\n",
    "        Name of the region identifier column\n",
    "    continuous_var : str\n",
    "        Name of the continuous variable to correlate with\n",
    "    output_prefix : str\n",
    "        Prefix for output filename\n",
    "    main_title : str\n",
    "        Main title for the figure\n",
    "    method : str, optional (default='pearson')\n",
    "        Correlation method to use. Options:\n",
    "        - 'pearson': Pearson correlation (linear relationships, parametric)\n",
    "        - 'spearman': Spearman rank correlation (monotonic relationships, non-parametric)\n",
    "        - 'kendall': Kendall tau correlation (ordinal data, non-parametric, robust to outliers)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corr_df : pd.DataFrame\n",
    "        DataFrame with correlation results by region\n",
    "    \"\"\"\n",
    "    # Validate method\n",
    "    valid_methods = ['pearson', 'spearman', 'kendall']\n",
    "    if method not in valid_methods:\n",
    "        raise ValueError(f\"method must be one of {valid_methods}, got '{method}'\")\n",
    "    \n",
    "    # Map method to scipy function and coefficient name\n",
    "    method_info = {\n",
    "        'pearson': {'func': stats.pearsonr, 'coef_name': 'r', 'full_name': 'Pearson'},\n",
    "        'spearman': {'func': stats.spearmanr, 'coef_name': 'ρ', 'full_name': 'Spearman'},\n",
    "        'kendall': {'func': stats.kendalltau, 'coef_name': 'τ', 'full_name': 'Kendall'}\n",
    "    }\n",
    "    \n",
    "    corr_func = method_info[method]['func']\n",
    "    coef_name = method_info[method]['coef_name']\n",
    "    full_name = method_info[method]['full_name']\n",
    "    \n",
    "    # Calculate correlations by region\n",
    "    corr_results = []\n",
    "    for region in regional_df[region_col].unique():\n",
    "        region_data = regional_df[regional_df[region_col] == region][[feature, continuous_var]].dropna()\n",
    "        \n",
    "        if len(region_data) >= 10:  # Minimum sample size\n",
    "            corr, p_val = corr_func(region_data[continuous_var], region_data[feature])\n",
    "            corr_results.append({\n",
    "                'region': region,\n",
    "                'correlation': corr,\n",
    "                'p_value': p_val,\n",
    "                'significant': p_val < 0.05,\n",
    "                'n': len(region_data)\n",
    "            })\n",
    "    \n",
    "    if len(corr_results) == 0:\n",
    "        print(f\"    ⚠️  No correlation results for {feature} vs {continuous_var}\")\n",
    "        return\n",
    "    \n",
    "    corr_df = pd.DataFrame(corr_results)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    fig.suptitle(f\"{main_title}\\n{full_name} Correlation with {continuous_var}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Correlation map on atlas\n",
    "    if atlas_path and Path(atlas_path).exists():\n",
    "        try:\n",
    "            import nibabel as nib\n",
    "            atlas_img = nib.load(atlas_path)\n",
    "            atlas_data = atlas_img.get_fdata()\n",
    "            \n",
    "            # Create correlation map\n",
    "            corr_map = np.zeros_like(atlas_data)\n",
    "            for _, row in corr_df.iterrows():\n",
    "                mask = atlas_data == row['region']\n",
    "                if np.any(mask):\n",
    "                    corr_map[mask] = row['correlation']\n",
    "            \n",
    "            # Plot\n",
    "            atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "            corr_cropped = corr_map[50:-50, 40:-40, 50:-50]\n",
    "            x, y, z = atlas_cropped.shape\n",
    "            \n",
    "            ax = axes[0, 0]\n",
    "            atlas_slice = np.rot90(atlas_cropped[:, :, z//2])\n",
    "            corr_slice = np.rot90(corr_cropped[:, :, z//2])\n",
    "            \n",
    "            vmax = max(abs(corr_df['correlation'].min()), abs(corr_df['correlation'].max()))\n",
    "            im = plot_atlas_panel(ax, atlas_slice, corr_slice,\n",
    "                                \"Correlation Map\",\n",
    "                                cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label(f'{full_name} {coef_name}', fontsize=10)\n",
    "            \n",
    "            ax.text(0.02, 0.98, \"Blue = negative\\nRed = positive\", \n",
    "                   transform=ax.transAxes, fontsize=9, va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️  Could not create atlas visualization: {e}\")\n",
    "            axes[0, 0].text(0.5, 0.5, \"Atlas not available\", \n",
    "                          ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].axis('off')\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, \"Atlas not available\", \n",
    "                       ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "        axes[0, 0].axis('off')\n",
    "    \n",
    "    # 2. Correlation strength by region\n",
    "    ax = axes[0, 1]\n",
    "    sorted_corr = corr_df.reindex(corr_df['correlation'].abs().sort_values(ascending=False).index).head(20)\n",
    "    colors = ['red' if sig else 'gray' for sig in sorted_corr['significant']]\n",
    "    ax.barh(range(len(sorted_corr)), sorted_corr['correlation'], color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(sorted_corr)))\n",
    "    ax.set_yticklabels([f\"R{int(r)}\" for r in sorted_corr['region']], fontsize=8)\n",
    "    ax.set_xlabel(f'Correlation ({coef_name})', fontweight='bold')\n",
    "    ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_title(f'Top 20 Regions by |Correlation| ({full_name})\\n(Red = significant p<0.05)', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # 3. Scatter plots for top correlated regions\n",
    "    ax = axes[1, 0]\n",
    "    top_positive = corr_df.nlargest(1, 'correlation').iloc[0]\n",
    "    top_negative = corr_df.nsmallest(1, 'correlation').iloc[0]\n",
    "    \n",
    "    # Plot both on same axis with different colors\n",
    "    for region_info, color, label in [(top_positive, 'red', f\"R{int(top_positive['region'])} ({coef_name}={top_positive['correlation']:.3f})\"),\n",
    "                                       (top_negative, 'blue', f\"R{int(top_negative['region'])} ({coef_name}={top_negative['correlation']:.3f})\")]:\n",
    "        region_data = regional_df[regional_df[region_col] == region_info['region']][[continuous_var, feature]].dropna()\n",
    "        ax.scatter(region_data[continuous_var], region_data[feature], \n",
    "                  alpha=0.5, s=30, color=color, label=label)\n",
    "        \n",
    "        # Add regression line (only for visualization, even for non-parametric tests)\n",
    "        z = np.polyfit(region_data[continuous_var], region_data[feature], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(region_data[continuous_var].min(), region_data[continuous_var].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), color=color, linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(continuous_var, fontweight='bold')\n",
    "    ax.set_ylabel(feature, fontweight='bold')\n",
    "    ax.set_title(f'Most Positive & Negative Correlations\\n(Line shown for reference)', fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Summary statistics\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "CORRELATION SUMMARY ({full_name.upper()})\n",
    "\n",
    "Method: {full_name} ({coef_name})\n",
    "Variable: {continuous_var}\n",
    "Total Regions Tested: {len(corr_df)}\n",
    "Significant Correlations (p<0.05): {sum(corr_df['significant'])} ({100*sum(corr_df['significant'])/len(corr_df):.1f}%)\n",
    "\n",
    "CORRELATION STATISTICS\n",
    "Mean |{coef_name}|: {corr_df['correlation'].abs().mean():.3f}\n",
    "Median |{coef_name}|: {corr_df['correlation'].abs().median():.3f}\n",
    "Max |{coef_name}|: {corr_df['correlation'].abs().max():.3f}\n",
    "\n",
    "POSITIVE CORRELATIONS\n",
    "Count: {sum(corr_df['correlation'] > 0)}\n",
    "Mean {coef_name}: {corr_df[corr_df['correlation'] > 0]['correlation'].mean():.3f}\n",
    "Significant: {sum((corr_df['correlation'] > 0) & corr_df['significant'])}\n",
    "\n",
    "NEGATIVE CORRELATIONS\n",
    "Count: {sum(corr_df['correlation'] < 0)}\n",
    "Mean {coef_name}: {corr_df[corr_df['correlation'] < 0]['correlation'].mean():.3f}\n",
    "Significant: {sum((corr_df['correlation'] < 0) & corr_df['significant'])}\n",
    "\n",
    "STRONGEST CORRELATIONS:\n",
    "\"\"\"\n",
    "    top_5_abs = corr_df.iloc[corr_df['correlation'].abs().argsort()[-5:][::-1]]\n",
    "    for _, row in top_5_abs.iterrows():\n",
    "        sig_marker = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"\"\n",
    "        summary_text += f\"\\nR{int(row['region'])}: {coef_name}={row['correlation']:.3f}, p={row['p_value']:.4f} {sig_marker}\"\n",
    "    \n",
    "    ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, \n",
    "           fontsize=10, verticalalignment='top', family='monospace',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / f'{output_prefix}_correlation_{method}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "# ====================================================================\n",
    "# MAIN ANALYSIS WORKFLOW\n",
    "# ====================================================================\n",
    "\n",
    "def run_comprehensive_demographic_analysis(regional_with_demo, atlas_path, region_col, \n",
    "                                          top_features, FIGURES_DIR, TABLES_DIR,correlation_method):\n",
    "    \"\"\"\n",
    "    Main function to run analysis for ALL available demographics automatically.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    regional_with_demo : pd.DataFrame\n",
    "        Regional data merged with demographics\n",
    "    atlas_path : str or Path\n",
    "        Path to atlas NIfTI file\n",
    "    region_col : str\n",
    "        Column name for brain regions\n",
    "    top_features : list\n",
    "        List of features to analyze (e.g., top 10 by variance)\n",
    "    FIGURES_DIR : Path\n",
    "        Directory to save figures\n",
    "    TABLES_DIR : Path\n",
    "        Directory to save tables\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: AUTO-DETECTING DEMOGRAPHIC VARIABLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    demographics = detect_demographic_variables(regional_with_demo)\n",
    "    print(f\"\\nFound {len(demographics['categorical'])} categorical and \"\n",
    "          f\"{len(demographics['continuous'])} continuous demographic variables\")\n",
    "    \n",
    "    # Store all results\n",
    "    all_statistical_results = {}\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PART 1: CATEGORICAL DEMOGRAPHIC ANALYSES\n",
    "    # ====================================================================\n",
    "    \n",
    "    if len(demographics['categorical']) > 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: CATEGORICAL DEMOGRAPHIC COMPARISONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for demo_var, unique_vals in demographics['categorical'].items():\n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            \n",
    "            print(f\"Analyzing: {demo_var.upper()}\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            \n",
    "            # Get group labels\n",
    "            group_labels = create_group_labels(regional_with_demo, demo_var)\n",
    "            print(f\"Groups: {', '.join([f'{k}={v}' for k,v in group_labels.items()])}\")\n",
    "            \n",
    "            # Count subjects per group\n",
    "            group_counts = regional_with_demo[demo_var].value_counts()\n",
    "            print(f\"Sample sizes: {dict(group_counts)}\")\n",
    "            \n",
    "            for feature_idx, feature in enumerate(top_features, 1):\n",
    "                if feature not in regional_with_demo.columns:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n  [{feature_idx}/{len(top_features)}] Processing: {feature}\")\n",
    "                \n",
    "                # 1. Create group comparison atlas visualization\n",
    "                print(f\"    → Creating group comparison atlas...\")\n",
    "                create_group_comparison_atlas(\n",
    "                    atlas_path=atlas_path,\n",
    "                    regional_df=regional_with_demo,\n",
    "                    feature=feature,\n",
    "                    region_col=region_col,\n",
    "                    group_col=demo_var,\n",
    "                    group_labels=group_labels,\n",
    "                    output_prefix=f'{demo_var}_{feature}',\n",
    "                    main_title=f'{feature} - {demo_var.replace(\"_\", \" \").title()} Comparison',\n",
    "                    colorbar_label=f'{feature}'\n",
    "                )\n",
    "                \n",
    "                # 2. Create difference visualization (statistical testing)\n",
    "                print(f\"    → Creating statistical difference maps...\")\n",
    "                diff_results = create_difference_visualization(\n",
    "                    atlas_path=atlas_path,\n",
    "                    regional_df=regional_with_demo,\n",
    "                    feature=feature,\n",
    "                    region_col=region_col,\n",
    "                    group_col=demo_var,\n",
    "                    output_prefix=f'{demo_var}_{feature}',\n",
    "                    main_title=f'{feature} - {demo_var.replace(\"_\", \" \").title()}'\n",
    "                )\n",
    "                \n",
    "                # 3. Create pairwise comparison plots\n",
    "                print(f\"    → Creating pairwise comparison plots...\")\n",
    "                create_pairwise_comparison_plots(\n",
    "                    regional_df=regional_with_demo,\n",
    "                    feature=feature,\n",
    "                    region_col=region_col,\n",
    "                    group_col=demo_var,\n",
    "                    output_prefix=f'{demo_var}_{feature}',\n",
    "                    main_title=f'{feature} - {demo_var.replace(\"_\", \" \").title()}'\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                if diff_results is not None:\n",
    "                    key = f\"{demo_var}_{feature}\"\n",
    "                    all_statistical_results[key] = diff_results\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    diff_results.to_csv(\n",
    "                        TABLES_DIR / f'stats_{demo_var}_{feature}.csv', \n",
    "                        index=False\n",
    "                    )\n",
    "                    print(f\"    ✓ Saved statistical results\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PART 2: CONTINUOUS DEMOGRAPHIC ANALYSES\n",
    "    # ====================================================================\n",
    "    \n",
    "    if len(demographics['continuous']) > 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: CONTINUOUS DEMOGRAPHIC CORRELATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for demo_var in demographics['continuous']:\n",
    "            print(f\"\\n{'-'*80}\")\n",
    "            print(f\"Analyzing: {demo_var.upper()}\")\n",
    "            print(f\"{'-'*80}\")\n",
    "            \n",
    "            # Get variable statistics\n",
    "            var_data = regional_with_demo[demo_var].dropna()\n",
    "            print(f\"Range: {var_data.min():.1f} - {var_data.max():.1f}\")\n",
    "            print(f\"Mean ± SD: {var_data.mean():.1f} ± {var_data.std():.1f}\")\n",
    "            print(f\"N = {len(var_data)}\")\n",
    "            \n",
    "            for feature_idx, feature in enumerate(top_features, 1):\n",
    "                if feature not in regional_with_demo.columns:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\n  [{feature_idx}/{len(top_features)}] Processing: {feature}\")\n",
    "                \n",
    "                # Create correlation analysis\n",
    "                print(f\"    → Creating correlation atlas...\")\n",
    "                corr_results = create_continuous_correlation_atlas(\n",
    "                    atlas_path=atlas_path,\n",
    "                    regional_df=regional_with_demo,\n",
    "                    feature=feature,\n",
    "                    region_col=region_col,\n",
    "                    continuous_var=demo_var,\n",
    "                    output_prefix=f'{demo_var}_{feature}',\n",
    "                    main_title=f'{feature}',\n",
    "                    method=correlation_method\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                if corr_results is not None:\n",
    "                    key = f\"{demo_var}_{feature}_corr\"\n",
    "                    all_statistical_results[key] = corr_results\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    corr_results.to_csv(\n",
    "                        TABLES_DIR / f'corr_{demo_var}_{feature}.csv',\n",
    "                        index=False\n",
    "                    )\n",
    "                    print(f\"    ✓ Saved correlation results\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PART 3: SUMMARY REPORT\n",
    "    # ====================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE - GENERATING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_data = {\n",
    "        'demographics_analyzed': {\n",
    "            'categorical': list(demographics['categorical'].keys()),\n",
    "            'continuous': demographics['continuous']\n",
    "        },\n",
    "        'features_analyzed': top_features,\n",
    "        'total_comparisons': len(all_statistical_results),\n",
    "        'figures_generated': len(list(FIGURES_DIR.glob('*.png'))),\n",
    "        'tables_generated': len(list(TABLES_DIR.glob('stats_*.csv'))) + \n",
    "                           len(list(TABLES_DIR.glob('corr_*.csv')))\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    import json\n",
    "    with open(TABLES_DIR / 'demographic_analysis_summary.json', 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ ANALYSIS SUMMARY:\")\n",
    "    print(f\"   • Categorical variables: {len(demographics['categorical'])}\")\n",
    "    print(f\"   • Continuous variables: {len(demographics['continuous'])}\")\n",
    "    print(f\"   • Features analyzed: {len(top_features)}\")\n",
    "    print(f\"   • Total statistical tests: {len(all_statistical_results)}\")\n",
    "    print(f\"   • Figures generated: {summary_data['figures_generated']}\")\n",
    "    print(f\"   • Tables generated: {summary_data['tables_generated']}\")\n",
    "    \n",
    "    return all_statistical_results, demographics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an AGE_BIN based on quartile \n",
    "regional_with_demo['age_group'] = pd.qcut(\n",
    "    regional_with_demo['AGE'], \n",
    "    q=4, \n",
    "    labels=[25,40,55,75]\n",
    ")\n",
    "# remove any NA age_group\n",
    "regional_with_demo = regional_with_demo.dropna(subset=['age_group'])\n",
    "# save is float\n",
    "regional_with_demo['age_group'] = regional_with_demo['age_group'].astype(float)\n",
    "\n",
    "print(f\"Number of patient per AGE BIN QUARTILE:\")\n",
    "print(f\"  {regional_with_demo['age_group'].value_counts().to_dict()}\")\n",
    "\n",
    "\n",
    "# Add an 'bmi_category' based on 10,18,5,25,25+\n",
    "regional_with_demo['bmi_category'] = pd.cut(\n",
    "    regional_with_demo['BMI'],\n",
    "    bins=[18.5, 25, 30,50],\n",
    "    labels=[22, 27, 35]\n",
    ")\n",
    "\n",
    "regional_with_demo = regional_with_demo.dropna(subset=['bmi_category'])\n",
    "regional_with_demo['bmi_category'] = regional_with_demo['bmi_category'].astype(float)\n",
    "\n",
    "regional_with_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features (or use all features)\n",
    "top_features = regional_var_df.head(10)['feature'].tolist()\n",
    "\n",
    "# SELECT OCCUPATION (4,6,7) + AGE+BMI (1,2)\n",
    "\n",
    "# BMI ==> Solo len\n",
    "# OCCUPATION, AGE ==> ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive analysis\n",
    "results, detected_demographics = run_comprehensive_demographic_analysis(\n",
    "    regional_with_demo=regional_with_demo,\n",
    "    atlas_path=ATLAS_PATH,\n",
    "    region_col=region_col,\n",
    "    top_features=top_features,\n",
    "    FIGURES_DIR=FIGURES_DIR,\n",
    "    TABLES_DIR=TABLES_DIR,\n",
    "    correlation_method='pearson' # 'pearson', 'spearman', or 'kendall'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 10.9 Statistical Testing - Regional Features vs Demographics\n",
    "# ====================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10.9 STATISTICAL TESTING - REGIONAL FEATURES VS DEMOGRAPHICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'regional_with_demo' not in locals() or len(regional_with_demo) == 0:\n",
    "    print(\"\\n⚠️  Regional data with demographics not available\")\n",
    "else:\n",
    "    \n",
    "    # ================================================================\n",
    "    # Helper Functions for Statistical Testing\n",
    "    # ================================================================\n",
    "    \n",
    "    def test_continuous_demographic(regional_df, feature, region_col, demo_var):\n",
    "        \"\"\"Test correlation between feature and continuous demographic variable by region.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for region in regional_df[region_col].unique():\n",
    "            region_data = regional_df[regional_df[region_col] == region][[demo_var, feature]].dropna()\n",
    "            \n",
    "            if len(region_data) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Pearson correlation\n",
    "            r, p = pearsonr(region_data[demo_var], region_data[feature])\n",
    "            \n",
    "            # Spearman correlation (non-parametric)\n",
    "            rho, p_spearman = spearmanr(region_data[demo_var], region_data[feature])\n",
    "            \n",
    "            results.append({\n",
    "                'region': region,\n",
    "                'feature': feature,\n",
    "                'demographic': demo_var,\n",
    "                'n': len(region_data),\n",
    "                'pearson_r': r,\n",
    "                'pearson_p': p,\n",
    "                'spearman_rho': rho,\n",
    "                'spearman_p': p_spearman\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_categorical_demographic(regional_df, feature, region_col, demo_var):\n",
    "        \"\"\"Test differences between demographic groups by region.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Get unique categories\n",
    "        categories = regional_df[demo_var].dropna().unique()\n",
    "        \n",
    "        if len(categories) < 2:\n",
    "            return results\n",
    "        \n",
    "        for region in regional_df[region_col].unique():\n",
    "            region_data = regional_df[regional_df[region_col] == region]\n",
    "            \n",
    "            # Get data for each category\n",
    "            group_data = []\n",
    "            group_labels = []\n",
    "            group_sizes = []\n",
    "            group_means = []\n",
    "            \n",
    "            for cat in categories:\n",
    "                cat_data = region_data[region_data[demo_var] == cat][feature].dropna()\n",
    "                if len(cat_data) >= 5:  # Minimum group size\n",
    "                    group_data.append(cat_data)\n",
    "                    group_labels.append(cat)\n",
    "                    group_sizes.append(len(cat_data))\n",
    "                    group_means.append(cat_data.mean())\n",
    "            \n",
    "            if len(group_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Perform statistical tests\n",
    "            try:\n",
    "                # ANOVA (parametric)\n",
    "                f_stat, f_pval = f_oneway(*group_data)\n",
    "                \n",
    "                # Kruskal-Wallis (non-parametric)\n",
    "                h_stat, h_pval = kruskal(*group_data)\n",
    "                \n",
    "                # Effect size (eta-squared for ANOVA)\n",
    "                grand_mean = np.mean(np.concatenate(group_data))\n",
    "                ss_between = sum(len(d) * (np.mean(d) - grand_mean)**2 for d in group_data)\n",
    "                ss_total = sum(np.sum((d - grand_mean)**2) for d in group_data)\n",
    "                eta_squared = ss_between / ss_total if ss_total > 0 else 0\n",
    "                \n",
    "                # For binary comparisons, also calculate Cohen's d\n",
    "                cohens_d = np.nan\n",
    "                if len(group_data) == 2:\n",
    "                    pooled_std = np.sqrt(\n",
    "                        ((len(group_data[0])-1)*group_data[0].std()**2 + \n",
    "                         (len(group_data[1])-1)*group_data[1].std()**2) / \n",
    "                        (len(group_data[0])+len(group_data[1])-2)\n",
    "                    )\n",
    "                    cohens_d = (group_data[0].mean() - group_data[1].mean()) / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                result = {\n",
    "                    'region': region,\n",
    "                    'feature': feature,\n",
    "                    'demographic': demo_var,\n",
    "                    'n_groups': len(group_data),\n",
    "                    'total_n': sum(group_sizes),\n",
    "                    'f_statistic': f_stat,\n",
    "                    'anova_p': f_pval,\n",
    "                    'kruskal_h': h_stat,\n",
    "                    'kruskal_p': h_pval,\n",
    "                    'eta_squared': eta_squared,\n",
    "                    'cohens_d': cohens_d\n",
    "                }\n",
    "                \n",
    "                # Add group-specific info\n",
    "                for idx, label in enumerate(group_labels):\n",
    "                    result[f'group_{idx+1}_label'] = str(label)\n",
    "                    result[f'group_{idx+1}_n'] = group_sizes[idx]\n",
    "                    result[f'group_{idx+1}_mean'] = group_means[idx]\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not test region {region} for {demo_var}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1. IDENTIFY DEMOGRAPHIC VARIABLES\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"1. IDENTIFYING DEMOGRAPHIC VARIABLES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Continuous variables (for correlation testing)\n",
    "continuous_vars = []\n",
    "for var in DEMOGRAPHIC_VARS:\n",
    "    if var in regional_with_demo.columns:\n",
    "        if pd.api.types.is_numeric_dtype(regional_with_demo[var]):\n",
    "            var_std = regional_with_demo[var].std()\n",
    "            var_nunique = regional_with_demo[var].nunique()\n",
    "            if var_std > 0 and var_nunique > 10:\n",
    "                continuous_vars.append(var)\n",
    "            else:\n",
    "                print(f\"Excluding {var}: std={var_std}, unique={var_nunique} (not continuous)\")\n",
    "        else:\n",
    "            print(f\"Excluding {var}: not numeric dtype\")\n",
    "    else:\n",
    "        print(f\"Excluding {var}: not in data columns\")\n",
    "            \n",
    "# Categorical variables (for group comparison testing)\n",
    "categorical_vars = []\n",
    "for var in DEMOGRAPHIC_VARS:\n",
    "    if var in regional_with_demo.columns:\n",
    "        var_nunique = regional_with_demo[var].nunique()\n",
    "        if 2 <= var_nunique <= 10:  # Between 2 and 10 categories\n",
    "            categorical_vars.append(var)\n",
    "        else:\n",
    "            print(f\"Excluding {var}: {var_nunique} unique values (not categorical)\")\n",
    "    else:\n",
    "        print(f\"Excluding {var}: not in data columns\")\n",
    "        \n",
    "print(f\"\\nContinuous variables (correlation tests): {continuous_vars}\")\n",
    "print(f\"Categorical variables (group tests): {categorical_vars}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 2. TEST CONTINUOUS DEMOGRAPHICS (CORRELATIONS)\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"2. TESTING CONTINUOUS DEMOGRAPHIC CORRELATIONS BY REGION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_continuous_results = []\n",
    "\n",
    "for demo_var in continuous_vars:\n",
    "    print(f\"\\n  Testing {demo_var}...\")\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        if feature not in regional_with_demo.columns or 'region' in feature:\n",
    "            continue\n",
    "        \n",
    "        results = test_continuous_demographic(\n",
    "            regional_with_demo, feature, region_col, demo_var\n",
    "        )\n",
    "        all_continuous_results.extend(results)\n",
    "    \n",
    "    print(f\"    Completed {len([r for r in all_continuous_results if r['demographic'] == demo_var])} region-feature tests\")\n",
    "\n",
    "if len(all_continuous_results) > 0:\n",
    "    continuous_results_df = pd.DataFrame(all_continuous_results)\n",
    "    \n",
    "    # FDR correction for each demographic variable\n",
    "    for demo_var in continuous_vars:\n",
    "        mask = continuous_results_df['demographic'] == demo_var\n",
    "        if mask.sum() > 0:\n",
    "            continuous_results_df.loc[mask, 'pearson_p_fdr'] = multipletests(\n",
    "                continuous_results_df.loc[mask, 'pearson_p'], method='fdr_bh')[1]\n",
    "            continuous_results_df.loc[mask, 'spearman_p_fdr'] = multipletests(\n",
    "                continuous_results_df.loc[mask, 'spearman_p'], method='fdr_bh')[1]\n",
    "    \n",
    "    continuous_results_df['pearson_significant'] = continuous_results_df['pearson_p_fdr'] < 0.05\n",
    "    continuous_results_df['spearman_significant'] = continuous_results_df['spearman_p_fdr'] < 0.05\n",
    "    \n",
    "    # Save results\n",
    "    continuous_results_df.to_csv(\n",
    "        TABLES_DIR / 'regional_continuous_demographics_tests.csv', index=False\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n  CONTINUOUS DEMOGRAPHICS SUMMARY:\")\n",
    "    print(f\"  Total tests performed: {len(continuous_results_df)}\")\n",
    "    for demo_var in continuous_vars:\n",
    "        demo_results = continuous_results_df[continuous_results_df['demographic'] == demo_var]\n",
    "        n_sig_pearson = demo_results['pearson_significant'].sum()\n",
    "        n_sig_spearman = demo_results['spearman_significant'].sum()\n",
    "        print(f\"\\n  {demo_var}:\")\n",
    "        print(f\"    Significant Pearson correlations: {n_sig_pearson} / {len(demo_results)} ({100*n_sig_pearson/len(demo_results):.1f}%)\")\n",
    "        print(f\"    Significant Spearman correlations: {n_sig_spearman} / {len(demo_results)} ({100*n_sig_spearman/len(demo_results):.1f}%)\")\n",
    "        \n",
    "        # Top correlations\n",
    "        if n_sig_pearson > 0:\n",
    "            top_corr = demo_results[demo_results['pearson_significant']].nlargest(5, 'pearson_r', keep='all')\n",
    "            print(f\"    Top 5 positive correlations:\")\n",
    "            for idx, row in top_corr.head(5).iterrows():\n",
    "                print(f\"      Region {row['region']}, {row['feature']}: r={row['pearson_r']:.3f}, p={row['pearson_p_fdr']:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 3. TEST CATEGORICAL DEMOGRAPHICS (GROUP DIFFERENCES)\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"3. TESTING CATEGORICAL DEMOGRAPHIC GROUP DIFFERENCES BY REGION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_categorical_results = []\n",
    "\n",
    "for demo_var in categorical_vars:\n",
    "    print(f\"\\n  Testing {demo_var}...\")\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        if feature not in regional_with_demo.columns or 'region' in feature:\n",
    "            continue\n",
    "        \n",
    "        results = test_categorical_demographic(\n",
    "            regional_with_demo, feature, region_col, demo_var\n",
    "        )\n",
    "        all_categorical_results.extend(results)\n",
    "    \n",
    "    print(f\"    Completed {len([r for r in all_categorical_results if r['demographic'] == demo_var])} region-feature tests\")\n",
    "\n",
    "if len(all_categorical_results) > 0:\n",
    "    categorical_results_df = pd.DataFrame(all_categorical_results)\n",
    "    \n",
    "    # FDR correction for each demographic variable\n",
    "    for demo_var in categorical_vars:\n",
    "        mask = categorical_results_df['demographic'] == demo_var\n",
    "        if mask.sum() > 0:\n",
    "            categorical_results_df.loc[mask, 'anova_p_fdr'] = multipletests(\n",
    "                categorical_results_df.loc[mask, 'anova_p'], method='fdr_bh')[1]\n",
    "            categorical_results_df.loc[mask, 'kruskal_p_fdr'] = multipletests(\n",
    "                categorical_results_df.loc[mask, 'kruskal_p'], method='fdr_bh')[1]\n",
    "    \n",
    "    categorical_results_df['anova_significant'] = categorical_results_df['anova_p_fdr'] < 0.05\n",
    "    categorical_results_df['kruskal_significant'] = categorical_results_df['kruskal_p_fdr'] < 0.05\n",
    "    \n",
    "    # Save results\n",
    "    categorical_results_df.to_csv(\n",
    "        TABLES_DIR / 'regional_categorical_demographics_tests.csv', index=False\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n  CATEGORICAL DEMOGRAPHICS SUMMARY:\")\n",
    "    print(f\"  Total tests performed: {len(categorical_results_df)}\")\n",
    "    for demo_var in categorical_vars:\n",
    "        demo_results = categorical_results_df[categorical_results_df['demographic'] == demo_var]\n",
    "        n_sig_anova = demo_results['anova_significant'].sum()\n",
    "        n_sig_kruskal = demo_results['kruskal_significant'].sum()\n",
    "        print(f\"\\n  {demo_var}:\")\n",
    "        print(f\"    Significant ANOVA differences: {n_sig_anova} / {len(demo_results)} ({100*n_sig_anova/len(demo_results):.1f}%)\")\n",
    "        print(f\"    Significant Kruskal-Wallis differences: {n_sig_kruskal} / {len(demo_results)} ({100*n_sig_kruskal/len(demo_results):.1f}%)\")\n",
    "        \n",
    "        # Top effect sizes\n",
    "        if n_sig_anova > 0:\n",
    "            top_effects = demo_results[demo_results['anova_significant']].nlargest(5, 'eta_squared', keep='all')\n",
    "            print(f\"    Top 5 largest effect sizes:\")\n",
    "            for idx, row in top_effects.head(5).iterrows():\n",
    "                print(f\"      Region {row['region']}, {row['feature']}: η²={row['eta_squared']:.3f}, p={row['anova_p_fdr']:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 4. CREATE SUMMARY HEATMAPS\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4. CREATING SUMMARY HEATMAPS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Heatmap 1: Significant correlations (continuous variables)\n",
    "if len(all_continuous_results) > 0 and len(continuous_vars) > 0:\n",
    "    print(\"\\n  Creating continuous demographics heatmap...\")\n",
    "    \n",
    "    # Create significance matrix\n",
    "    sig_matrix = []\n",
    "    features_in_order = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        if 'region' in feature:\n",
    "            continue\n",
    "        feature_results = continuous_results_df[continuous_results_df['feature'] == feature]\n",
    "        if len(feature_results) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = []\n",
    "        for demo_var in continuous_vars:\n",
    "            demo_feature = feature_results[feature_results['demographic'] == demo_var]\n",
    "            n_sig = demo_feature['pearson_significant'].sum()\n",
    "            pct_sig = 100 * n_sig / len(demo_feature) if len(demo_feature) > 0 else 0\n",
    "            row.append(pct_sig)\n",
    "        \n",
    "        if sum(row) > 0:  # Only include features with some significance\n",
    "            sig_matrix.append(row)\n",
    "            features_in_order.append(feature)\n",
    "    \n",
    "    if len(sig_matrix) > 0:\n",
    "        sig_df = pd.DataFrame(sig_matrix, index=features_in_order, columns=continuous_vars)\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(max(8, len(continuous_vars)*1.5), \n",
    "                                        max(10, len(features_in_order)*0.3)))\n",
    "        sns.heatmap(sig_df, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': '% Regions Significant'}, ax=ax,\n",
    "                    linewidths=0.5, linecolor='gray')\n",
    "        ax.set_title('Percentage of Regions with Significant Correlations\\n(Continuous Demographics)', \n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.set_xlabel('Demographic Variable', fontweight='bold')\n",
    "        ax.set_ylabel('Vessel Feature', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'regional_continuous_demographics_heatmap.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"    ✓ Continuous demographics heatmap saved\")\n",
    "\n",
    "# Heatmap 2: Significant group differences (categorical variables)\n",
    "if len(all_categorical_results) > 0 and len(categorical_vars) > 0:\n",
    "    print(\"\\n  Creating categorical demographics heatmap...\")\n",
    "    \n",
    "    # Create significance matrix\n",
    "    sig_matrix = []\n",
    "    features_in_order = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        if 'region' in feature:\n",
    "            continue\n",
    "        feature_results = categorical_results_df[categorical_results_df['feature'] == feature]\n",
    "        if len(feature_results) == 0:\n",
    "            continue\n",
    "        \n",
    "        row = []\n",
    "        for demo_var in categorical_vars:\n",
    "            demo_feature = feature_results[feature_results['demographic'] == demo_var]\n",
    "            n_sig = demo_feature['anova_significant'].sum()\n",
    "            pct_sig = 100 * n_sig / len(demo_feature) if len(demo_feature) > 0 else 0\n",
    "            row.append(pct_sig)\n",
    "        \n",
    "        if sum(row) > 0:\n",
    "            sig_matrix.append(row)\n",
    "            features_in_order.append(feature)\n",
    "    \n",
    "    if len(sig_matrix) > 0:\n",
    "        sig_df = pd.DataFrame(sig_matrix, index=features_in_order, columns=categorical_vars)\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(max(8, len(categorical_vars)*1.5), \n",
    "                                        max(10, len(features_in_order)*0.3)))\n",
    "        sns.heatmap(sig_df, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': '% Regions Significant'}, ax=ax,\n",
    "                    linewidths=0.5, linecolor='gray')\n",
    "        ax.set_title('Percentage of Regions with Significant Group Differences\\n(Categorical Demographics)', \n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.set_xlabel('Demographic Variable', fontweight='bold')\n",
    "        ax.set_ylabel('Vessel Feature', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'regional_categorical_demographics_heatmap.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"    ✓ Categorical demographics heatmap saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ATLAS-BASED HEATMAP VISUALIZATION\n",
    "# Replace the heatmap creation code from your document with this version\n",
    "# ================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import ndimage\n",
    "\n",
    "# ================================================================\n",
    "# HELPER FUNCTIONS FOR ATLAS VISUALIZATION\n",
    "# ================================================================\n",
    "\n",
    "def plot_atlas_panel(ax, atlas_slice, value_slice, title, cmap='YlOrRd', vmin=None, vmax=None):\n",
    "    \"\"\"Create a single atlas visualization panel with smooth rendering.\"\"\"\n",
    "    # 1. WHITE background\n",
    "    ax.imshow(np.ones_like(atlas_slice), cmap='gray', vmin=0, vmax=1)\n",
    "    # 2. LIGHT GREY fill inside regions\n",
    "    mask = atlas_slice > 0\n",
    "    bg = np.ones_like(atlas_slice)\n",
    "    bg[mask] = 0.85\n",
    "    ax.imshow(bg, cmap='gray', vmin=0, vmax=1)\n",
    "    # 3. SMOOTH GREY contours\n",
    "    regions_arr = np.unique(atlas_slice)[1:]\n",
    "    if regions_arr.size:\n",
    "        ax.contour(\n",
    "            atlas_slice.astype(float), levels=regions_arr + 0.5, colors='#BBBBBB',\n",
    "            linewidths=0.5, alpha=0.6, antialiased=True\n",
    "        )\n",
    "    # 4. Value overlay\n",
    "    vals = np.ma.masked_where(value_slice == 0, value_slice)\n",
    "    im = ax.imshow(vals, cmap=cmap, vmin=vmin, vmax=vmax, alpha=0.9)\n",
    "    ax.set_title(title, fontsize=12, pad=4)\n",
    "    ax.axis('off')\n",
    "    return im\n",
    "\n",
    "\n",
    "def create_atlas_heatmap_from_dataframe(atlas_path, results_df, demo_var, \n",
    "                                        is_continuous=True, output_prefix='',\n",
    "                                        figures_dir=Path('./figures')):\n",
    "    \"\"\"\n",
    "    Create atlas-based heatmap directly from your results dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    atlas_path : str\n",
    "        Path to atlas NIfTI file\n",
    "    results_df : pd.DataFrame\n",
    "        Your continuous_results_df or categorical_results_df\n",
    "    demo_var : str\n",
    "        Demographic variable name (e.g., 'AGE', 'SEX_ID')\n",
    "    is_continuous : bool\n",
    "        True for continuous (correlation), False for categorical (ANOVA)\n",
    "    output_prefix : str\n",
    "        Prefix for output filename\n",
    "    figures_dir : Path\n",
    "        Directory to save figures\n",
    "    \"\"\"\n",
    "    \n",
    "    import nibabel as nib\n",
    "    \n",
    "    # Filter for this demographic variable\n",
    "    demo_results = results_df[results_df['demographic'] == demo_var]\n",
    "    \n",
    "    # Calculate percentage significant per region\n",
    "    sig_col = 'pearson_significant' if is_continuous else 'anova_significant'\n",
    "    regional_sig = {}\n",
    "    \n",
    "    for region in demo_results['region'].unique():\n",
    "        region_data = demo_results[demo_results['region'] == region]\n",
    "        n_sig = region_data[sig_col].sum()\n",
    "        total = len(region_data)\n",
    "        pct_sig = 100 * n_sig / total if total > 0 else 0\n",
    "        regional_sig[region] = pct_sig\n",
    "    \n",
    "    # Load atlas\n",
    "    try:\n",
    "        print(f\"  Loading atlas from {atlas_path}...\")\n",
    "        atlas_img = nib.load(atlas_path)\n",
    "        atlas_data = atlas_img.get_fdata()\n",
    "        \n",
    "        # Create value map\n",
    "        value_map = np.zeros_like(atlas_data)\n",
    "        for region_id, pct_sig in regional_sig.items():\n",
    "            mask = atlas_data == region_id\n",
    "            if np.any(mask):\n",
    "                value_map[mask] = pct_sig\n",
    "        \n",
    "        # Crop to brain region\n",
    "        atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "        value_cropped = value_map[50:-50, 40:-40, 50:-50]\n",
    "        \n",
    "        # Get slice positions\n",
    "        x, y, z = atlas_cropped.shape\n",
    "        x_slice, y_slice, z_slice = x//2, y//2, z//2\n",
    "        \n",
    "        # Create figure with three views\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        slices = {\n",
    "            'Sagittal': (np.rot90(atlas_cropped[x_slice, :, :]), \n",
    "                        np.rot90(value_cropped[x_slice, :, :])),\n",
    "            'Coronal': (np.rot90(atlas_cropped[:, y_slice, :]), \n",
    "                       np.rot90(value_cropped[:, y_slice, :])),\n",
    "            'Axial': (np.rot90(atlas_cropped[:, :, z_slice]), \n",
    "                     np.rot90(value_cropped[:, :, z_slice]))\n",
    "        }\n",
    "        \n",
    "        # Plot each view\n",
    "        vmin, vmax = 0, 100\n",
    "        for ax, (view_name, (atlas_sl, val_sl)) in zip(axes, slices.items()):\n",
    "            im = plot_atlas_panel(ax, atlas_sl, val_sl, view_name, \n",
    "                                 cmap='YlOrRd', vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        # Title and colorbar\n",
    "        analysis_type = \"Correlations\" if is_continuous else \"Group Differences\"\n",
    "        fig.suptitle(f'{demo_var} - Regional {analysis_type}', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        cbar = fig.colorbar(im, ax=axes, orientation='horizontal', \n",
    "                           fraction=0.046, pad=0.04, aspect=40)\n",
    "        cbar.set_label('% Features with Significant Association', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        output_file = figures_dir / f'{output_prefix}atlas_{demo_var}.png'\n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"    ✓ Atlas heatmap saved: {output_file.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️  Atlas visualization failed: {e}\")\n",
    "        print(f\"    Creating fallback bar plot...\")\n",
    "        create_fallback_barplot(regional_sig, demo_var, output_prefix, \n",
    "                               is_continuous, figures_dir)\n",
    "\n",
    "\n",
    "def create_fallback_barplot(regional_sig, demo_var, output_prefix, \n",
    "                            is_continuous, figures_dir):\n",
    "    \"\"\"Fallback bar plot if atlas fails.\"\"\"\n",
    "    regions = sorted(regional_sig.keys())\n",
    "    values = [regional_sig[r] for r in regions]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    \n",
    "    cmap = plt.cm.get_cmap('YlOrRd')\n",
    "    norm = plt.Normalize(vmin=0, vmax=100)\n",
    "    colors = [cmap(norm(v)) for v in values]\n",
    "    \n",
    "    bars = ax.bar(range(len(regions)), values, color=colors, \n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    analysis_type = \"Correlations\" if is_continuous else \"Group Differences\"\n",
    "    ax.set_xlabel('Region ID', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('% Features Significant', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{demo_var} - Regional {analysis_type}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(range(len(regions)))\n",
    "    ax.set_xticklabels(regions, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_file = figures_dir / f'{output_prefix}atlas_{demo_var}.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"    ✓ Bar plot saved: {output_file.name}\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4. CREATE ATLAS-BASED HEATMAPS\n",
    "# Replace your existing heatmap code with this\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4. CREATING ATLAS-BASED HEATMAPS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# SET YOUR ATLAS PATH HERE\n",
    "ATLAS_PATH = '/home/falcetta/ISBI2025/LIANE/ArterialAtlas.nii.gz'  # UPDATE THIS\n",
    "\n",
    "# Check if atlas exists\n",
    "atlas_available = Path(ATLAS_PATH).exists() if ATLAS_PATH else False\n",
    "if atlas_available:\n",
    "    print(f\"✓ Atlas found at: {ATLAS_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠️  Atlas not found. Will create bar plots instead.\")\n",
    "    print(f\"   Set ATLAS_PATH to enable atlas visualization.\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Heatmap 1: Continuous demographics (one atlas per variable)\n",
    "# ----------------------------------------------------------------\n",
    "if len(all_continuous_results) > 0 and len(continuous_vars) > 0:\n",
    "    print(\"\\n  Creating continuous demographics atlas heatmaps...\")\n",
    "    \n",
    "    for demo_var in continuous_vars:\n",
    "        print(f\"\\n  Processing {demo_var}...\")\n",
    "        create_atlas_heatmap_from_dataframe(\n",
    "            atlas_path=ATLAS_PATH,\n",
    "            results_df=continuous_results_df,\n",
    "            demo_var=demo_var,\n",
    "            is_continuous=True,\n",
    "            output_prefix='regional_continuous_',\n",
    "            figures_dir=FIGURES_DIR\n",
    "        )\n",
    "    \n",
    "    print(\"\\n  ✓ All continuous demographics atlas heatmaps complete\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Heatmap 2: Categorical demographics (one atlas per variable)\n",
    "# ----------------------------------------------------------------\n",
    "if len(all_categorical_results) > 0 and len(categorical_vars) > 0:\n",
    "    print(\"\\n  Creating categorical demographics atlas heatmaps...\")\n",
    "    \n",
    "    for demo_var in categorical_vars:\n",
    "        print(f\"\\n  Processing {demo_var}...\")\n",
    "        create_atlas_heatmap_from_dataframe(\n",
    "            atlas_path=ATLAS_PATH,\n",
    "            results_df=categorical_results_df,\n",
    "            demo_var=demo_var,\n",
    "            is_continuous=False,\n",
    "            output_prefix='regional_categorical_',\n",
    "            figures_dir=FIGURES_DIR\n",
    "        )\n",
    "    \n",
    "    print(\"\\n  ✓ All categorical demographics atlas heatmaps complete\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Optional: Combined heatmap averaging all demographics\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n  Creating combined atlas heatmap...\")\n",
    "\n",
    "# Combine all regional significance values\n",
    "all_regional_sig = {}\n",
    "\n",
    "# Process continuous variables\n",
    "if len(all_continuous_results) > 0:\n",
    "    for demo_var in continuous_vars:\n",
    "        demo_results = continuous_results_df[continuous_results_df['demographic'] == demo_var]\n",
    "        for region in demo_results['region'].unique():\n",
    "            region_data = demo_results[demo_results['region'] == region]\n",
    "            n_sig = region_data['pearson_significant'].sum()\n",
    "            total = len(region_data)\n",
    "            pct_sig = 100 * n_sig / total if total > 0 else 0\n",
    "            \n",
    "            if region not in all_regional_sig:\n",
    "                all_regional_sig[region] = []\n",
    "            all_regional_sig[region].append(pct_sig)\n",
    "\n",
    "# Process categorical variables\n",
    "if len(all_categorical_results) > 0:\n",
    "    for demo_var in categorical_vars:\n",
    "        demo_results = categorical_results_df[categorical_results_df['demographic'] == demo_var]\n",
    "        for region in demo_results['region'].unique():\n",
    "            region_data = demo_results[demo_results['region'] == region]\n",
    "            n_sig = region_data['anova_significant'].sum()\n",
    "            total = len(region_data)\n",
    "            pct_sig = 100 * n_sig / total if total > 0 else 0\n",
    "            \n",
    "            if region not in all_regional_sig:\n",
    "                all_regional_sig[region] = []\n",
    "            all_regional_sig[region].append(pct_sig)\n",
    "\n",
    "# Average across all demographics\n",
    "avg_regional_sig = {region: np.mean(values) for region, values in all_regional_sig.items()}\n",
    "\n",
    "# Create combined atlas\n",
    "if atlas_available:\n",
    "    try:\n",
    "        import nibabel as nib\n",
    "        \n",
    "        print(f\"  Loading atlas from {ATLAS_PATH}...\")\n",
    "        atlas_img = nib.load(ATLAS_PATH)\n",
    "        atlas_data = atlas_img.get_fdata()\n",
    "        \n",
    "        value_map = np.zeros_like(atlas_data)\n",
    "        for region_id, pct_sig in avg_regional_sig.items():\n",
    "            mask = atlas_data == region_id\n",
    "            if np.any(mask):\n",
    "                value_map[mask] = pct_sig\n",
    "        \n",
    "        # Crop and prepare slices\n",
    "        atlas_cropped = atlas_data[50:-50, 40:-40, 50:-50]\n",
    "        value_cropped = value_map[50:-50, 40:-40, 50:-50]\n",
    "        \n",
    "        x, y, z = atlas_cropped.shape\n",
    "        x_slice, y_slice, z_slice = x//2, y//2, z//2\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        slices = {\n",
    "            'Sagittal': (np.rot90(atlas_cropped[x_slice, :, :]), \n",
    "                        np.rot90(value_cropped[x_slice, :, :])),\n",
    "            'Coronal': (np.rot90(atlas_cropped[:, y_slice, :]), \n",
    "                       np.rot90(value_cropped[:, y_slice, :])),\n",
    "            'Axial': (np.rot90(atlas_cropped[:, :, z_slice]), \n",
    "                     np.rot90(value_cropped[:, :, z_slice]))\n",
    "        }\n",
    "        \n",
    "        vmin, vmax = 0, 100\n",
    "        for ax, (view_name, (atlas_sl, val_sl)) in zip(axes, slices.items()):\n",
    "            im = plot_atlas_panel(ax, atlas_sl, val_sl, view_name, \n",
    "                                 cmap='YlOrRd', vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        n_demos = len(continuous_vars) + len(categorical_vars)\n",
    "        fig.suptitle(f'Combined Atlas - Average Across {n_demos} Demographics', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        cbar = fig.colorbar(im, ax=axes, orientation='horizontal', \n",
    "                           fraction=0.046, pad=0.04, aspect=40)\n",
    "        cbar.set_label('Average % Features with Significant Association', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'regional_combined_atlas_heatmap.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"    ✓ Combined atlas heatmap saved\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠️  Combined atlas failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ATLAS-BASED HEATMAP GENERATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGenerated outputs:\")\n",
    "if len(continuous_vars) > 0:\n",
    "    print(f\"  • {len(continuous_vars)} continuous demographics atlas heatmaps\")\n",
    "if len(categorical_vars) > 0:\n",
    "    print(f\"  • {len(categorical_vars)} categorical demographics atlas heatmaps\")\n",
    "print(f\"  • 1 combined atlas heatmap (average across all demographics)\")\n",
    "print(f\"\\nAll files saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ENHANCED ATLAS-BASED HEATMAP WITH FEATURE IMPORTANCE ANALYSIS\n",
    "# Add this after your atlas heatmap creation\n",
    "# ================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS FUNCTIONS\n",
    "# ================================================================\n",
    "\n",
    "def analyze_regional_feature_importance_continuous(results_df, demo_var, top_n=3):\n",
    "    \"\"\"\n",
    "    Identify which features drive significance in each region for continuous demographics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        continuous_results_df\n",
    "    demo_var : str\n",
    "        Demographic variable (e.g., 'AGE')\n",
    "    top_n : int\n",
    "        Number of top features to report per region\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with columns: region, feature, r_value, p_value, rank\n",
    "    \"\"\"\n",
    "    # Filter for this demographic\n",
    "    demo_results = results_df[results_df['demographic'] == demo_var].copy()\n",
    "    \n",
    "    # Only keep significant results\n",
    "    demo_results = demo_results[demo_results['pearson_significant'] == True]\n",
    "    \n",
    "    if len(demo_results) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Rank features within each region by effect size (absolute correlation)\n",
    "    demo_results['abs_r'] = demo_results['pearson_r'].abs()\n",
    "    \n",
    "    regional_importance = []\n",
    "    \n",
    "    for region in sorted(demo_results['region'].unique()):\n",
    "        region_data = demo_results[demo_results['region'] == region].copy()\n",
    "        \n",
    "        # Sort by effect size (absolute correlation)\n",
    "        region_data = region_data.sort_values('abs_r', ascending=False)\n",
    "        \n",
    "        # Get top N features\n",
    "        for idx, row in enumerate(region_data.head(top_n).itertuples(), 1):\n",
    "            regional_importance.append({\n",
    "                'region': region,\n",
    "                'rank': idx,\n",
    "                'feature': row.feature,\n",
    "                'r_value': row.pearson_r,\n",
    "                'abs_r': row.abs_r,\n",
    "                'p_value': row.pearson_p,  # ← FIXED: was pearson_pvalue\n",
    "                'n_sig_in_region': len(region_data)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(regional_importance)\n",
    "\n",
    "\n",
    "def analyze_regional_feature_importance_categorical(results_df, demo_var, top_n=3):\n",
    "    \"\"\"\n",
    "    Identify which features drive significance in each region for categorical demographics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        categorical_results_df\n",
    "    demo_var : str\n",
    "        Demographic variable (e.g., 'SEX_ID')\n",
    "    top_n : int\n",
    "        Number of top features to report per region\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with columns: region, feature, eta_squared, f_statistic, p_value, rank\n",
    "    \"\"\"\n",
    "    # Filter for this demographic\n",
    "    demo_results = results_df[results_df['demographic'] == demo_var].copy()\n",
    "    \n",
    "    # Only keep significant results\n",
    "    demo_results = demo_results[demo_results['anova_significant'] == True]\n",
    "    \n",
    "    if len(demo_results) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    regional_importance = []\n",
    "    \n",
    "    for region in sorted(demo_results['region'].unique()):\n",
    "        region_data = demo_results[demo_results['region'] == region].copy()\n",
    "        \n",
    "        # Sort by effect size (eta squared)\n",
    "        region_data = region_data.sort_values('eta_squared', ascending=False)\n",
    "        \n",
    "        # Get top N features\n",
    "        for idx, row in enumerate(region_data.head(top_n).itertuples(), 1):\n",
    "            regional_importance.append({\n",
    "                'region': region,\n",
    "                'rank': idx,\n",
    "                'feature': row.feature,\n",
    "                'eta_squared': row.eta_squared,\n",
    "                'f_statistic': row.f_statistic,\n",
    "                'p_value': row.anova_p,\n",
    "                'n_sig_in_region': len(region_data)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(regional_importance)\n",
    "\n",
    "def create_feature_importance_summary_continuous(results_df, demo_var, figures_dir):\n",
    "    \"\"\"Create visual summary of top features per region for continuous demographics.\"\"\"\n",
    "    \n",
    "    importance_df = analyze_regional_feature_importance_continuous(results_df, demo_var, top_n=3)\n",
    "    \n",
    "    if len(importance_df) == 0:\n",
    "        print(f\"    No significant results for {demo_var}\")\n",
    "        return\n",
    "    \n",
    "    # Create text summary\n",
    "    summary_lines = [\n",
    "        f\"\\n{'='*80}\",\n",
    "        f\"FEATURE IMPORTANCE ANALYSIS: {demo_var}\",\n",
    "        f\"{'='*80}\\n\",\n",
    "        f\"Top features driving regional significance patterns:\\n\"\n",
    "    ]\n",
    "    \n",
    "    # Group by region and create formatted summary\n",
    "    regions_with_sig = importance_df['region'].unique()\n",
    "    \n",
    "    for region in sorted(regions_with_sig):\n",
    "        region_data = importance_df[importance_df['region'] == region]\n",
    "        n_sig = region_data.iloc[0]['n_sig_in_region']\n",
    "        \n",
    "        summary_lines.append(f\"\\n📍 REGION {region} ({n_sig} significant features total):\")\n",
    "        summary_lines.append(\"-\" * 60)\n",
    "        \n",
    "        for _, row in region_data.iterrows():\n",
    "            direction = \"↑ Positive\" if row['r_value'] > 0 else \"↓ Negative\"\n",
    "            summary_lines.append(\n",
    "                f\"  {row['rank']}. {row['feature']:<35} \"\n",
    "                f\"r={row['r_value']:>6.3f} {direction:>12} \"\n",
    "                f\"(p={row['p_value']:.2e})\"\n",
    "            )\n",
    "    \n",
    "    # Print summary\n",
    "    summary_text = '\\n'.join(summary_lines)\n",
    "    print(summary_text)\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = figures_dir / f'feature_importance_{demo_var}_detailed.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(f\"\\n✓ Detailed feature importance saved to: {output_file.name}\")\n",
    "    \n",
    "    # Create visualization: Heatmap of top features by region\n",
    "    create_feature_importance_heatmap_continuous(importance_df, demo_var, figures_dir)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def create_feature_importance_summary_categorical(results_df, demo_var, figures_dir):\n",
    "    \"\"\"Create visual summary of top features per region for categorical demographics.\"\"\"\n",
    "    \n",
    "    importance_df = analyze_regional_feature_importance_categorical(results_df, demo_var, top_n=3)\n",
    "    \n",
    "    if len(importance_df) == 0:\n",
    "        print(f\"    No significant results for {demo_var}\")\n",
    "        return\n",
    "    \n",
    "    # Create text summary\n",
    "    summary_lines = [\n",
    "        f\"\\n{'='*80}\",\n",
    "        f\"FEATURE IMPORTANCE ANALYSIS: {demo_var}\",\n",
    "        f\"{'='*80}\\n\",\n",
    "        f\"Top features driving regional group differences:\\n\"\n",
    "    ]\n",
    "    \n",
    "    # Group by region\n",
    "    regions_with_sig = importance_df['region'].unique()\n",
    "    \n",
    "    for region in sorted(regions_with_sig):\n",
    "        region_data = importance_df[importance_df['region'] == region]\n",
    "        n_sig = region_data.iloc[0]['n_sig_in_region']\n",
    "        \n",
    "        summary_lines.append(f\"\\n📍 REGION {region} ({n_sig} significant features total):\")\n",
    "        summary_lines.append(\"-\" * 60)\n",
    "        \n",
    "        for _, row in region_data.iterrows():\n",
    "            effect_size = \"Large\" if row['eta_squared'] > 0.14 else \"Medium\" if row['eta_squared'] > 0.06 else \"Small\"\n",
    "            summary_lines.append(\n",
    "                f\"  {row['rank']}. {row['feature']:<35} \"\n",
    "                f\"η²={row['eta_squared']:>6.3f} ({effect_size:>6}) \"\n",
    "                f\"F={row['f_statistic']:>6.2f} (p={row['p_value']:.2e})\"\n",
    "            )\n",
    "    \n",
    "    # Print summary\n",
    "    summary_text = '\\n'.join(summary_lines)\n",
    "    print(summary_text)\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = figures_dir / f'feature_importance_{demo_var}_detailed.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(f\"\\n✓ Detailed feature importance saved to: {output_file.name}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    create_feature_importance_heatmap_categorical(importance_df, demo_var, figures_dir)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def create_feature_importance_heatmap_continuous(importance_df, demo_var, figures_dir):\n",
    "    \"\"\"Create heatmap showing which features are most important in each region.\"\"\"\n",
    "    \n",
    "    # Pivot to get feature × region matrix with effect sizes\n",
    "    pivot_data = importance_df.pivot_table(\n",
    "        index='feature',\n",
    "        columns='region',\n",
    "        values='abs_r',\n",
    "        aggfunc='first'  # Take the first (highest ranked) value if duplicates\n",
    "    )\n",
    "    \n",
    "    # Fill NaN with 0 (feature not in top 3 for that region)\n",
    "    pivot_data = pivot_data.fillna(0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(max(12, len(pivot_data.columns)*0.5), \n",
    "                                     max(8, len(pivot_data)*0.4)))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                cbar_kws={'label': '|Correlation Coefficient|'},\n",
    "                linewidths=0.5, linecolor='gray', ax=ax,\n",
    "                vmin=0, vmax=pivot_data.max().max())\n",
    "    \n",
    "    ax.set_title(f'{demo_var}: Top Feature Importance by Region\\n(Absolute Correlation Coefficients)', \n",
    "                fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Region ID', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Feature', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = figures_dir / f'feature_importance_heatmap_{demo_var}.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Feature importance heatmap saved: {output_file.name}\")\n",
    "\n",
    "\n",
    "def create_feature_importance_heatmap_categorical(importance_df, demo_var, figures_dir):\n",
    "    \"\"\"Create heatmap showing which features are most important in each region.\"\"\"\n",
    "    \n",
    "    # Pivot to get feature × region matrix with effect sizes\n",
    "    pivot_data = importance_df.pivot_table(\n",
    "        index='feature',\n",
    "        columns='region',\n",
    "        values='eta_squared',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    pivot_data = pivot_data.fillna(0)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(max(12, len(pivot_data.columns)*0.5), \n",
    "                                     max(8, len(pivot_data)*0.4)))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Eta Squared (η²)'},\n",
    "                linewidths=0.5, linecolor='gray', ax=ax,\n",
    "                vmin=0, vmax=pivot_data.max().max())\n",
    "    \n",
    "    ax.set_title(f'{demo_var}: Top Feature Importance by Region\\n(Effect Sizes - η²)', \n",
    "                fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Region ID', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Feature', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = figures_dir / f'feature_importance_heatmap_{demo_var}.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Feature importance heatmap saved: {output_file.name}\")\n",
    "\n",
    "\n",
    "def create_regional_feature_profile_plot(importance_df, demo_var, figures_dir, is_continuous=True):\n",
    "    \"\"\"\n",
    "    Create bar plot showing distribution of significant features across regions.\n",
    "    Identifies which regions have most/least feature diversity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count significant features per region\n",
    "    region_counts = importance_df.groupby('region')['n_sig_in_region'].first().sort_values(ascending=False)\n",
    "    \n",
    "    if len(region_counts) == 0:\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Create colormap based on counts\n",
    "    cmap = plt.cm.get_cmap('YlOrRd')\n",
    "    norm = plt.Normalize(vmin=region_counts.min(), vmax=region_counts.max())\n",
    "    colors = [cmap(norm(v)) for v in region_counts.values]\n",
    "    \n",
    "    bars = ax.bar(range(len(region_counts)), region_counts.values, color=colors,\n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Region ID', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Significant Features', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    analysis_type = \"Correlations\" if is_continuous else \"Group Differences\"\n",
    "    ax.set_title(f'{demo_var}: Regional Feature Diversity\\nNumber of Significant {analysis_type} per Region',\n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(len(region_counts)))\n",
    "    ax.set_xticklabels(region_counts.index, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, region_counts.values)):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "               f'{int(count)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = figures_dir / f'regional_feature_diversity_{demo_var}.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Regional feature diversity plot saved: {output_file.name}\")\n",
    "\n",
    "\n",
    "def create_overall_feature_ranking(all_importance_dfs, demo_var, figures_dir, is_continuous=True):\n",
    "    \"\"\"\n",
    "    Create overall ranking of features across all regions.\n",
    "    Shows which features are consistently important.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(all_importance_dfs) == 0:\n",
    "        return\n",
    "    \n",
    "    # Count how many regions each feature appears in (top 3)\n",
    "    feature_frequency = all_importance_dfs['feature'].value_counts()\n",
    "    \n",
    "    # Calculate average effect size for each feature\n",
    "    if is_continuous:\n",
    "        feature_avg_effect = all_importance_dfs.groupby('feature')['abs_r'].mean()\n",
    "    else:\n",
    "        feature_avg_effect = all_importance_dfs.groupby('feature')['eta_squared'].mean()\n",
    "    \n",
    "    # Combine into summary\n",
    "    feature_summary = pd.DataFrame({\n",
    "        'frequency': feature_frequency,\n",
    "        'avg_effect': feature_avg_effect\n",
    "    }).sort_values('frequency', ascending=False)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Frequency (in how many regions is this feature important?)\n",
    "    top_features = feature_summary.head(10)\n",
    "    \n",
    "    ax1.barh(range(len(top_features)), top_features['frequency'].values, \n",
    "            color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "    ax1.set_yticks(range(len(top_features)))\n",
    "    ax1.set_yticklabels(top_features.index)\n",
    "    ax1.set_xlabel('Number of Regions (Top 3)', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Most Frequently Important Features', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(top_features['frequency'].values):\n",
    "        ax1.text(v, i, f' {int(v)}', va='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Average effect size\n",
    "    effect_label = 'Average |r|' if is_continuous else 'Average η²'\n",
    "    \n",
    "    ax2.barh(range(len(top_features)), top_features['avg_effect'].values,\n",
    "            color='coral', edgecolor='black', linewidth=0.5)\n",
    "    ax2.set_yticks(range(len(top_features)))\n",
    "    ax2.set_yticklabels(top_features.index)\n",
    "    ax2.set_xlabel(effect_label, fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Average Effect Size', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(top_features['avg_effect'].values):\n",
    "        ax2.text(v, i, f' {v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    fig.suptitle(f'{demo_var}: Overall Feature Importance Ranking', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = figures_dir / f'overall_feature_ranking_{demo_var}.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Overall feature ranking saved: {output_file.name}\")\n",
    "    \n",
    "    # Save summary table\n",
    "    summary_file = figures_dir / f'overall_feature_ranking_{demo_var}.csv'\n",
    "    feature_summary.to_csv(summary_file)\n",
    "    print(f\"✓ Feature ranking table saved: {summary_file.name}\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5. FEATURE IMPORTANCE ANALYSIS FOR ALL DEMOGRAPHICS\n",
    "# Add this as a new section after your atlas heatmaps\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAnalyzing which features drive regional significance patterns...\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Continuous Demographics\n",
    "# ----------------------------------------------------------------\n",
    "if len(all_continuous_results) > 0 and len(continuous_vars) > 0:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CONTINUOUS DEMOGRAPHICS - FEATURE IMPORTANCE\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    all_continuous_importance = {}\n",
    "    \n",
    "    for demo_var in continuous_vars:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Analyzing: {demo_var}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Get detailed feature importance\n",
    "        importance_df = create_feature_importance_summary_continuous(\n",
    "            continuous_results_df, demo_var, FIGURES_DIR\n",
    "        )\n",
    "        \n",
    "        if importance_df is not None and len(importance_df) > 0:\n",
    "            all_continuous_importance[demo_var] = importance_df\n",
    "            \n",
    "            # Create regional feature profile\n",
    "            create_regional_feature_profile_plot(\n",
    "                importance_df, demo_var, FIGURES_DIR, is_continuous=True\n",
    "            )\n",
    "            \n",
    "            # Create overall feature ranking\n",
    "            create_overall_feature_ranking(\n",
    "                importance_df, demo_var, FIGURES_DIR, is_continuous=True\n",
    "            )\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Categorical Demographics\n",
    "# ----------------------------------------------------------------\n",
    "if len(all_categorical_results) > 0 and len(categorical_vars) > 0:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CATEGORICAL DEMOGRAPHICS - FEATURE IMPORTANCE\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    all_categorical_importance = {}\n",
    "    \n",
    "    for demo_var in categorical_vars:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Analyzing: {demo_var}\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Get detailed feature importance\n",
    "        importance_df = create_feature_importance_summary_categorical(\n",
    "            categorical_results_df, demo_var, FIGURES_DIR\n",
    "        )\n",
    "        \n",
    "        if importance_df is not None and len(importance_df) > 0:\n",
    "            all_categorical_importance[demo_var] = importance_df\n",
    "            \n",
    "            # Create regional feature profile\n",
    "            create_regional_feature_profile_plot(\n",
    "                importance_df, demo_var, FIGURES_DIR, is_continuous=False\n",
    "            )\n",
    "            \n",
    "            # Create overall feature ranking\n",
    "            create_overall_feature_ranking(\n",
    "                importance_df, demo_var, FIGURES_DIR, is_continuous=False\n",
    "            )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ FEATURE IMPORTANCE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated outputs for each demographic variable:\")\n",
    "print(\"  • Detailed text summary (feature_importance_[VAR]_detailed.txt)\")\n",
    "print(\"  • Feature importance heatmap (feature_importance_heatmap_[VAR].png)\")\n",
    "print(\"  • Regional feature diversity plot (regional_feature_diversity_[VAR].png)\")\n",
    "print(\"  • Overall feature ranking plot (overall_feature_ranking_[VAR].png)\")\n",
    "print(\"  • Feature ranking table (overall_feature_ranking_[VAR].csv)\")\n",
    "print(f\"\\nAll files saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 5. CREATE FEATURE-SPECIFIC REGIONAL MAPS\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"5. CREATING FEATURE-SPECIFIC REGIONAL SIGNIFICANCE MAPS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# For top 5 features, show which regions have significant demographic effects\n",
    "if len(all_continuous_results) > 0 or len(all_categorical_results) > 0:\n",
    "    \n",
    "    # Get top features by total number of significant findings\n",
    "    feature_sig_counts = {}\n",
    "    \n",
    "    if len(all_continuous_results) > 0:\n",
    "        for feature in ALL_FEATURES:\n",
    "            if 'region' in feature:\n",
    "                continue\n",
    "            cont_count = continuous_results_df[\n",
    "                (continuous_results_df['feature'] == feature) & \n",
    "                (continuous_results_df['pearson_significant'])\n",
    "            ].shape[0]\n",
    "            feature_sig_counts[feature] = feature_sig_counts.get(feature, 0) + cont_count\n",
    "    \n",
    "    if len(all_categorical_results) > 0:\n",
    "        for feature in ALL_FEATURES:\n",
    "            if 'region' in feature:\n",
    "                continue\n",
    "            cat_count = categorical_results_df[\n",
    "                (categorical_results_df['feature'] == feature) & \n",
    "                (categorical_results_df['anova_significant'])\n",
    "            ].shape[0]\n",
    "            feature_sig_counts[feature] = feature_sig_counts.get(feature, 0) + cat_count\n",
    "    \n",
    "    top_features = sorted(feature_sig_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    print(f\"\\n  Top 5 features by demographic significance:\")\n",
    "    for feat, count in top_features:\n",
    "        print(f\"    {feat}: {count} significant region-demographic combinations\")\n",
    "    \n",
    "    for feature, _ in top_features:\n",
    "        if 'region' in feature:\n",
    "            continue\n",
    "        print(f\"\\n  Creating regional significance map for {feature}...\")\n",
    "        \n",
    "        # Collect significance data by region\n",
    "        region_sig_data = {}\n",
    "        \n",
    "        for region in regions:\n",
    "            sig_demos = []\n",
    "            \n",
    "            # Check continuous variables\n",
    "            if len(all_continuous_results) > 0:\n",
    "                cont_region = continuous_results_df[\n",
    "                    (continuous_results_df['feature'] == feature) & \n",
    "                    (continuous_results_df['region'] == region) & \n",
    "                    (continuous_results_df['pearson_significant'])\n",
    "                ]\n",
    "                sig_demos.extend(cont_region['demographic'].tolist())\n",
    "            \n",
    "            # Check categorical variables\n",
    "            if len(all_categorical_results) > 0:\n",
    "                cat_region = categorical_results_df[\n",
    "                    (categorical_results_df['feature'] == feature) & \n",
    "                    (categorical_results_df['region'] == region) & \n",
    "                    (categorical_results_df['anova_significant'])\n",
    "                ]\n",
    "                sig_demos.extend(cat_region['demographic'].tolist())\n",
    "            \n",
    "            region_sig_data[region] = len(set(sig_demos))  # Count unique demographics\n",
    "        \n",
    "        # Create bar plot\n",
    "        fig, ax = plt.subplots(figsize=(14, max(6, len(regions)*0.25)))\n",
    "        \n",
    "        sorted_regions = sorted(region_sig_data.keys())\n",
    "        counts = [region_sig_data[r] for r in sorted_regions]\n",
    "        \n",
    "        bars = ax.barh(range(len(sorted_regions)), counts, \n",
    "                        color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        ax.set_yticks(range(len(sorted_regions)))\n",
    "        ax.set_yticklabels([f'Region {r}' for r in sorted_regions], fontsize=8)\n",
    "        ax.set_xlabel('Number of Demographics with Significant Effect', fontweight='bold')\n",
    "        ax.set_title(f'{feature} - Regional Demographic Sensitivity', \n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Color bars by count\n",
    "        max_count = max(counts) if counts else 1\n",
    "        for bar, count in zip(bars, counts):\n",
    "            bar.set_color(plt.cm.YlOrRd(count / max_count))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / f'regional_demographic_sensitivity_{feature}.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"    ✓ Saved regional sensitivity map for {feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# 6. COMPREHENSIVE SUMMARY TABLE\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"6. CREATING COMPREHENSIVE SUMMARY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# Summarize each feature\n",
    "for feature in ALL_FEATURES:\n",
    "    if 'region' in feature:\n",
    "        continue\n",
    "    row = {\n",
    "        'feature': feature,\n",
    "        'category': feature_categories[feature]\n",
    "    }\n",
    "    \n",
    "    # Continuous demographics\n",
    "    if len(all_continuous_results) > 0:\n",
    "        feat_cont = continuous_results_df[continuous_results_df['feature'] == feature]\n",
    "        for demo_var in continuous_vars:\n",
    "            demo_feat = feat_cont[feat_cont['demographic'] == demo_var]\n",
    "            if len(demo_feat) > 0:\n",
    "                n_sig = demo_feat['pearson_significant'].sum()\n",
    "                pct_sig = 100 * n_sig / len(demo_feat)\n",
    "                max_r = demo_feat['pearson_r'].abs().max()\n",
    "                row[f'{demo_var}_n_regions_sig'] = n_sig\n",
    "                row[f'{demo_var}_pct_regions_sig'] = pct_sig\n",
    "                row[f'{demo_var}_max_abs_r'] = max_r\n",
    "    \n",
    "    # Categorical demographics\n",
    "    if len(all_categorical_results) > 0:\n",
    "        feat_cat = categorical_results_df[categorical_results_df['feature'] == feature]\n",
    "        for demo_var in categorical_vars:\n",
    "            demo_feat = feat_cat[feat_cat['demographic'] == demo_var]\n",
    "            if len(demo_feat) > 0:\n",
    "                n_sig = demo_feat['anova_significant'].sum()\n",
    "                pct_sig = 100 * n_sig / len(demo_feat)\n",
    "                max_eta = demo_feat['eta_squared'].max()\n",
    "                row[f'{demo_var}_n_regions_sig'] = n_sig\n",
    "                row[f'{demo_var}_pct_regions_sig'] = pct_sig\n",
    "                row[f'{demo_var}_max_eta_squared'] = max_eta\n",
    "    \n",
    "    summary_rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv(TABLES_DIR / 'regional_demographics_summary.csv', index=False)\n",
    "print(f\"✓ Comprehensive summary saved to {TABLES_DIR / 'regional_demographics_summary.csv'}\")\n",
    "\n",
    "# Display top features\n",
    "print(f\"\\nTop features by average demographic sensitivity:\")\n",
    "sig_cols = [c for c in summary_df.columns if '_pct_regions_sig' in c]\n",
    "if len(sig_cols) > 0:\n",
    "    summary_df['avg_pct_sig'] = summary_df[sig_cols].mean(axis=1)\n",
    "    top_summary = summary_df.nlargest(10, 'avg_pct_sig')[['feature', 'category', 'avg_pct_sig'] + sig_cols]\n",
    "    display(top_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FINAL SUMMARY\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ STATISTICAL TESTING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated outputs:\")\n",
    "print(f\"  • Continuous demographics tests: {TABLES_DIR / 'regional_continuous_demographics_tests.csv'}\")\n",
    "print(f\"  • Categorical demographics tests: {TABLES_DIR / 'regional_categorical_demographics_tests.csv'}\")\n",
    "print(f\"  • Comprehensive summary: {TABLES_DIR / 'regional_demographics_summary.csv'}\")\n",
    "print(f\"  • Significance heatmaps (by feature and demographic)\")\n",
    "print(f\"  • Regional sensitivity maps (top 5 features)\")\n",
    "\n",
    "if len(all_continuous_results) > 0:\n",
    "    total_sig_cont = continuous_results_df['pearson_significant'].sum()\n",
    "    total_tests_cont = len(continuous_results_df)\n",
    "    print(f\"\\n📊 CONTINUOUS DEMOGRAPHICS:\")\n",
    "    print(f\"  Total significant correlations: {total_sig_cont} / {total_tests_cont} ({100*total_sig_cont/total_tests_cont:.1f}%)\")\n",
    "\n",
    "if len(all_categorical_results) > 0:\n",
    "    total_sig_cat = categorical_results_df['anova_significant'].sum()\n",
    "    total_tests_cat = len(categorical_results_df)\n",
    "    print(f\"\\n📊 CATEGORICAL DEMOGRAPHICS:\")\n",
    "    print(f\"  Total significant group differences: {total_sig_cat} / {total_tests_cat} ({100*total_sig_cat/total_tests_cat:.1f}%)\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"  • Each test controls for FDR at 5% within demographic variable\")\n",
    "print(\"  • Both parametric (Pearson/ANOVA) and non-parametric (Spearman/Kruskal-Wallis) tests\")\n",
    "print(\"  • Effect sizes included (Pearson r, η², Cohen's d)\")\n",
    "print(\"  • Results show feature-specific regional sensitivity to demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Hemispheric Asymmetry Analysis (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HAS_HEMISPHERE:\n",
    "    print(\"\\n⚠️  No hemisphere information available. Skipping hemispheric asymmetry analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HEMISPHERIC ASYMMETRY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n⚠️  Hemisphere analysis requires manual implementation based on your specific data structure.\")\n",
    "    print(\"     Please refer to the IXI_Vessel_Advanced_Analysis.ipynb for detailed hemisphere analysis code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Advanced Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Machine Learning: Age Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AGE' not in df.columns:\n",
    "    print(\"\\n⚠️  AGE variable not available. Skipping ML age prediction.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MACHINE LEARNING: AGE PREDICTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Prepare data - remove rows with missing age\n",
    "    valid_data = df[['AGE'] + ALL_FEATURES].dropna(subset=['AGE'])\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(valid_data) < 20:\n",
    "        print(f\"\\n⚠️  Insufficient data for ML ({len(valid_data)} subjects with valid age). Skipping.\")\n",
    "    else:\n",
    "        print(f\"\\nUsing {len(valid_data)} subjects with complete data\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = valid_data[ALL_FEATURES].fillna(0)  # Impute missing feature values with 0\n",
    "        y = valid_data['AGE']\n",
    "        \n",
    "        # Check for features with no variance\n",
    "        feature_variance = X.var()\n",
    "        zero_variance_features = feature_variance[feature_variance == 0].index.tolist()\n",
    "        if len(zero_variance_features) > 0:\n",
    "            print(f\"\\n⚠️  Removing {len(zero_variance_features)} zero-variance features\")\n",
    "            X = X.drop(columns=zero_variance_features)\n",
    "            features_for_ml = [f for f in ALL_FEATURES if f not in zero_variance_features]\n",
    "        else:\n",
    "            features_for_ml = ALL_FEATURES\n",
    "        \n",
    "        print(f\"Using {len(features_for_ml)} features for prediction\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Random Forest age prediction with cross-validation\n",
    "        print(\"\\nTraining Random Forest regressor...\")\n",
    "        rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # 5-fold cross-validation\n",
    "        cv_scores = cross_val_score(rf, X_scaled, y, cv=5, scoring='r2')\n",
    "        cv_mae = -cross_val_score(rf, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "        \n",
    "        print(f\"\\nCross-validation results (5-fold):\")\n",
    "        print(f\"  R² score: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "        print(f\"  MAE: {cv_mae.mean():.2f} ± {cv_mae.std():.2f} years\")\n",
    "        \n",
    "        # Train final model and get predictions\n",
    "        rf.fit(X_scaled, y)\n",
    "        y_pred = cross_val_predict(rf, X_scaled, y, cv=5)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': features_for_ml,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 most important features for age prediction:\")\n",
    "        display(feature_importance.head(10))\n",
    "        \n",
    "        feature_importance.to_csv(TABLES_DIR / 'age_prediction_feature_importance.csv', index=False)\n",
    "        \n",
    "        # Plot predictions vs actual\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.scatter(y, y_pred, alpha=0.5, s=30)\n",
    "        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "        ax.set_xlabel('Actual Age (years)', fontweight='bold')\n",
    "        ax.set_ylabel('Predicted Age (years)', fontweight='bold')\n",
    "        ax.set_title(f'Age Prediction from Vessel Features\\nR²={cv_scores.mean():.3f}, MAE={cv_mae.mean():.1f} years', \n",
    "                     fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        correlation = np.corrcoef(y, y_pred)[0, 1]\n",
    "        ax.text(0.05, 0.95, f'Pearson r = {correlation:.3f}', \n",
    "                transform=ax.transAxes, fontsize=10, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'age_prediction.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n✓ Age prediction analysis complete\")\n",
    "        print(f\"  Subjects used: {len(valid_data)}\")\n",
    "        print(f\"  Features used: {len(features_for_ml)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Age × Sex Interaction Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'AGE' not in df.columns or 'SEX_ID' not in df.columns:\n",
    "    print(\"\\n⚠️  AGE or SEX_ID not available. Skipping interaction analysis.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGE × SEX INTERACTION EFFECTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    interaction_results = []\n",
    "    \n",
    "    for feature in ALL_FEATURES:\n",
    "        # Prepare data\n",
    "        data_for_model = df[['AGE', 'SEX_ID', feature]].dropna()\n",
    "        \n",
    "        if len(data_for_model) < 20:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Fit model with interaction\n",
    "            model = smf.ols(f'{feature} ~ AGE + C(SEX_ID) + AGE:C(SEX_ID)', data=data_for_model).fit()\n",
    "            \n",
    "            # Get interaction term p-value\n",
    "            interaction_pval = model.pvalues['AGE:C(SEX_ID)[T.2]'] if 'AGE:C(SEX_ID)[T.2]' in model.pvalues else np.nan\n",
    "            \n",
    "            interaction_results.append({\n",
    "                'feature': feature,\n",
    "                'category': feature_categories[feature],\n",
    "                'n': len(data_for_model),\n",
    "                'interaction_pvalue': interaction_pval,\n",
    "                'model_r_squared': model.rsquared\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(interaction_results) > 0:\n",
    "        interaction_df = pd.DataFrame(interaction_results)\n",
    "        interaction_df = interaction_df.dropna(subset=['interaction_pvalue'])\n",
    "        \n",
    "        if len(interaction_df) > 0:\n",
    "            interaction_df['interaction_pvalue_fdr'] = multipletests(interaction_df['interaction_pvalue'], method='fdr_bh')[1]\n",
    "            interaction_df['significant'] = interaction_df['interaction_pvalue_fdr'] < 0.05\n",
    "            interaction_df = interaction_df.sort_values('interaction_pvalue')\n",
    "            \n",
    "            n_sig = interaction_df['significant'].sum()\n",
    "            print(f\"\\nInteraction analysis summary:\")\n",
    "            print(f\"  Features tested: {len(interaction_df)}\")\n",
    "            print(f\"  Significant Age×Sex interactions (FDR<0.05): {n_sig}\")\n",
    "            \n",
    "            if n_sig > 0:\n",
    "                print(f\"\\nFeatures with significant Age×Sex interaction:\")\n",
    "                display(interaction_df[interaction_df['significant']][['feature', 'category', 'interaction_pvalue_fdr', 'model_r_squared']])\n",
    "            \n",
    "            interaction_df.to_csv(TABLES_DIR / 'age_sex_interactions.csv', index=False)\n",
    "            print(f\"\\n✓ Interaction analysis saved to {TABLES_DIR / 'age_sex_interactions.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Summary and Export for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS SUMMARY FOR PAPER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Helper function to convert numpy/pandas types to Python native types\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Convert numpy/pandas types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    return obj\n",
    "\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'total_subjects': int(len(df)),\n",
    "        'n_regions': int(regional_df[region_col].nunique()) if 'regional_df' in locals() and 'region_col' in locals() else 'N/A',\n",
    "        'age_range': f\"{df['AGE'].min():.1f}-{df['AGE'].max():.1f}\" if 'AGE' in df.columns and df['AGE'].notna().any() else 'N/A',\n",
    "        'age_mean': float(df['AGE'].mean()) if 'AGE' in df.columns and df['AGE'].notna().any() else 'N/A',\n",
    "        'age_std': float(df['AGE'].std()) if 'AGE' in df.columns and df['AGE'].notna().any() else 'N/A',\n",
    "        'male_subjects': int(len(df[df['SEX_ID']==1])) if 'SEX_ID' in df.columns else 'N/A',\n",
    "        'female_subjects': int(len(df[df['SEX_ID']==2])) if 'SEX_ID' in df.columns else 'N/A',\n",
    "        'n_sites': int(df['site'].nunique()) if 'site' in df.columns else 'N/A'\n",
    "    },\n",
    "    'features': {\n",
    "        'total_features': int(len(ALL_FEATURES)),\n",
    "        'morphometric': int(len(MORPHOMETRIC_FEATURES)),\n",
    "        'topological': int(len(TOPOLOGICAL_FEATURES)),\n",
    "        'curvature': int(len(CURVATURE_FEATURES)),\n",
    "        'other': int(len(OTHER_FEATURES))\n",
    "    },\n",
    "    'regional_analysis': {},\n",
    "    'key_findings': {}\n",
    "}\n",
    "\n",
    "# Regional demographic associations\n",
    "if 'continuous_results_df' in locals() and len(continuous_results_df) > 0:\n",
    "    summary['regional_analysis']['continuous_demographics'] = {}\n",
    "    for demo_var in continuous_vars:\n",
    "        demo_data = continuous_results_df[continuous_results_df['demographic'] == demo_var]\n",
    "        n_sig = demo_data['pearson_significant'].sum()\n",
    "        total_tests = len(demo_data)\n",
    "        \n",
    "        summary['regional_analysis']['continuous_demographics'][demo_var] = {\n",
    "            'total_tests': int(total_tests),\n",
    "            'significant_associations': int(n_sig),\n",
    "            'percent_significant': float(100 * n_sig / total_tests) if total_tests > 0 else 0.0,\n",
    "            'n_regions_with_effects': int(demo_data[demo_data['pearson_significant']]['region'].nunique()),\n",
    "            'strongest_correlation': {\n",
    "                'feature': str(demo_data.loc[demo_data['pearson_r'].abs().idxmax(), 'feature']) if n_sig > 0 else 'N/A',\n",
    "                'region': int(demo_data.loc[demo_data['pearson_r'].abs().idxmax(), 'region']) if n_sig > 0 else 'N/A',\n",
    "                'r': float(demo_data.loc[demo_data['pearson_r'].abs().idxmax(), 'pearson_r']) if n_sig > 0 else 'N/A',\n",
    "                'p_fdr': float(demo_data.loc[demo_data['pearson_r'].abs().idxmax(), 'pearson_p_fdr']) if n_sig > 0 else 'N/A'\n",
    "            }\n",
    "        }\n",
    "\n",
    "if 'categorical_results_df' in locals() and len(categorical_results_df) > 0:\n",
    "    summary['regional_analysis']['categorical_demographics'] = {}\n",
    "    for demo_var in categorical_vars:\n",
    "        demo_data = categorical_results_df[categorical_results_df['demographic'] == demo_var]\n",
    "        n_sig = demo_data['anova_significant'].sum()\n",
    "        total_tests = len(demo_data)\n",
    "        \n",
    "        summary['regional_analysis']['categorical_demographics'][demo_var] = {\n",
    "            'total_tests': int(total_tests),\n",
    "            'significant_differences': int(n_sig),\n",
    "            'percent_significant': float(100 * n_sig / total_tests) if total_tests > 0 else 0.0,\n",
    "            'n_regions_with_effects': int(demo_data[demo_data['anova_significant']]['region'].nunique()),\n",
    "            'largest_effect': {\n",
    "                'feature': str(demo_data.loc[demo_data['eta_squared'].idxmax(), 'feature']) if n_sig > 0 else 'N/A',\n",
    "                'region': int(demo_data.loc[demo_data['eta_squared'].idxmax(), 'region']) if n_sig > 0 else 'N/A',\n",
    "                'eta_squared': float(demo_data.loc[demo_data['eta_squared'].idxmax(), 'eta_squared']) if n_sig > 0 else 'N/A',\n",
    "                'p_fdr': float(demo_data.loc[demo_data['eta_squared'].idxmax(), 'anova_p_fdr']) if n_sig > 0 else 'N/A'\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Feature importance summary\n",
    "if 'all_continuous_importance' in locals() and len(all_continuous_importance) > 0:\n",
    "    summary['feature_importance'] = {}\n",
    "    \n",
    "    for demo_var, importance_df in all_continuous_importance.items():\n",
    "        if len(importance_df) > 0:\n",
    "            # Get most frequently important features\n",
    "            feature_freq = importance_df['feature'].value_counts()\n",
    "            top_feature = feature_freq.index[0] if len(feature_freq) > 0 else 'N/A'\n",
    "            \n",
    "            # Get average effect size for top feature\n",
    "            if top_feature != 'N/A':\n",
    "                top_feature_data = importance_df[importance_df['feature'] == top_feature]\n",
    "                avg_effect = top_feature_data['abs_r'].mean()\n",
    "            else:\n",
    "                avg_effect = 'N/A'\n",
    "            \n",
    "            summary['feature_importance'][demo_var] = {\n",
    "                'n_regions_analyzed': int(importance_df['region'].nunique()),\n",
    "                'most_important_feature': str(top_feature),\n",
    "                'n_regions_where_top': int(feature_freq.iloc[0]) if len(feature_freq) > 0 else 0,\n",
    "                'avg_effect_size': float(avg_effect) if avg_effect != 'N/A' else 'N/A'\n",
    "            }\n",
    "\n",
    "# Traditional whole-brain findings (if available)\n",
    "if 'AGE' in df.columns and 'age_corr_df' in locals() and len(age_corr_df) > 0:\n",
    "    summary['key_findings']['whole_brain_age'] = {\n",
    "        'n_correlated_features': int(age_corr_df['significant_pearson'].sum()),\n",
    "        'strongest_correlation': {\n",
    "            'feature': str(age_corr_df.iloc[0]['feature']),\n",
    "            'r': float(age_corr_df.iloc[0]['pearson_r']),\n",
    "            'p_fdr': float(age_corr_df.iloc[0]['pearson_p_fdr'])\n",
    "        }\n",
    "    }\n",
    "\n",
    "if 'SEX_ID' in df.columns and 'sex_comp_df' in locals() and len(sex_comp_df) > 0:\n",
    "    summary['key_findings']['whole_brain_sex'] = {\n",
    "        'n_differences': int(sex_comp_df['significant_ttest'].sum()),\n",
    "        'largest_difference': {\n",
    "            'feature': str(sex_comp_df.iloc[0]['feature']),\n",
    "            'cohens_d': float(sex_comp_df.iloc[0]['cohens_d']),\n",
    "            'p_fdr': float(sex_comp_df.iloc[0]['t_pvalue_fdr'])\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ML results\n",
    "if 'cv_scores' in locals():\n",
    "    summary['key_findings']['age_prediction'] = {\n",
    "        'r2_mean': float(cv_scores.mean()),\n",
    "        'r2_std': float(cv_scores.std()),\n",
    "        'mae_mean': float(cv_mae.mean()),\n",
    "        'mae_std': float(cv_mae.std()),\n",
    "        'n_subjects': int(len(valid_data)) if 'valid_data' in locals() else 'N/A',\n",
    "        'n_features': int(len(features_for_ml)) if 'features_for_ml' in locals() else int(len(ALL_FEATURES))\n",
    "    }\n",
    "\n",
    "# Save summary\n",
    "with open(TABLES_DIR / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# PRINT FORMATTED SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"   Total subjects: {summary['dataset']['total_subjects']}\")\n",
    "print(f\"   Brain regions analyzed: {summary['dataset']['n_regions']}\")\n",
    "print(f\"   Age range: {summary['dataset']['age_range']} years\")\n",
    "if summary['dataset']['age_mean'] != 'N/A':\n",
    "    print(f\"   Age: {summary['dataset']['age_mean']:.1f} ± {summary['dataset']['age_std']:.1f} years\")\n",
    "if summary['dataset']['male_subjects'] != 'N/A':\n",
    "    male_pct = 100 * summary['dataset']['male_subjects'] / summary['dataset']['total_subjects']\n",
    "    female_pct = 100 * summary['dataset']['female_subjects'] / summary['dataset']['total_subjects']\n",
    "    print(f\"   Sex: {summary['dataset']['male_subjects']} male ({male_pct:.1f}%), {summary['dataset']['female_subjects']} female ({female_pct:.1f}%)\")\n",
    "if summary['dataset']['n_sites'] != 'N/A':\n",
    "    print(f\"   Multi-center sites: {summary['dataset']['n_sites']}\")\n",
    "\n",
    "print(\"\\n🔬 VESSEL FEATURES ANALYZED:\")\n",
    "print(f\"   Total features: {summary['features']['total_features']}\")\n",
    "print(f\"   • Morphometric: {summary['features']['morphometric']}\")\n",
    "print(f\"   • Topological: {summary['features']['topological']}\")\n",
    "print(f\"   • Curvature: {summary['features']['curvature']}\")\n",
    "if summary['features']['other'] > 0:\n",
    "    print(f\"   • Other: {summary['features']['other']}\")\n",
    "\n",
    "# Regional analysis summary\n",
    "if 'regional_analysis' in summary and summary['regional_analysis']:\n",
    "    print(\"\\n🗺️  REGIONAL DEMOGRAPHIC ASSOCIATIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'continuous_demographics' in summary['regional_analysis']:\n",
    "        print(\"\\nContinuous Demographics (Correlations):\")\n",
    "        for demo_var, stats in summary['regional_analysis']['continuous_demographics'].items():\n",
    "            print(f\"\\n  {demo_var}:\")\n",
    "            print(f\"    • Total region-feature tests: {stats['total_tests']}\")\n",
    "            print(f\"    • Significant associations: {stats['significant_associations']} ({stats['percent_significant']:.1f}%)\")\n",
    "            print(f\"    • Regions with effects: {stats['n_regions_with_effects']}\")\n",
    "            if stats['strongest_correlation']['feature'] != 'N/A':\n",
    "                print(f\"    • Strongest correlation:\")\n",
    "                print(f\"      - Feature: {stats['strongest_correlation']['feature']}\")\n",
    "                print(f\"      - Region: {stats['strongest_correlation']['region']}\")\n",
    "                print(f\"      - r = {stats['strongest_correlation']['r']:.3f}, p(FDR) = {stats['strongest_correlation']['p_fdr']:.2e}\")\n",
    "    \n",
    "    if 'categorical_demographics' in summary['regional_analysis']:\n",
    "        print(\"\\nCategorical Demographics (Group Differences):\")\n",
    "        for demo_var, stats in summary['regional_analysis']['categorical_demographics'].items():\n",
    "            print(f\"\\n  {demo_var}:\")\n",
    "            print(f\"    • Total region-feature tests: {stats['total_tests']}\")\n",
    "            print(f\"    • Significant differences: {stats['significant_differences']} ({stats['percent_significant']:.1f}%)\")\n",
    "            print(f\"    • Regions with effects: {stats['n_regions_with_effects']}\")\n",
    "            if stats['largest_effect']['feature'] != 'N/A':\n",
    "                print(f\"    • Largest effect size:\")\n",
    "                print(f\"      - Feature: {stats['largest_effect']['feature']}\")\n",
    "                print(f\"      - Region: {stats['largest_effect']['region']}\")\n",
    "                print(f\"      - η² = {stats['largest_effect']['eta_squared']:.3f}, p(FDR) = {stats['largest_effect']['p_fdr']:.2e}\")\n",
    "\n",
    "# Feature importance summary\n",
    "if 'feature_importance' in summary and summary['feature_importance']:\n",
    "    print(\"\\n⭐ FEATURE IMPORTANCE (Top Drivers by Region):\")\n",
    "    print(\"=\"*80)\n",
    "    for demo_var, stats in summary['feature_importance'].items():\n",
    "        if stats['most_important_feature'] != 'N/A':\n",
    "            print(f\"\\n  {demo_var}:\")\n",
    "            print(f\"    • Most consistently important: {stats['most_important_feature']}\")\n",
    "            print(f\"    • Appears in top 3 for: {stats['n_regions_where_top']} / {stats['n_regions_analyzed']} regions\")\n",
    "            if stats['avg_effect_size'] != 'N/A':\n",
    "                print(f\"    • Average effect size: {stats['avg_effect_size']:.3f}\")\n",
    "\n",
    "# Traditional findings\n",
    "if summary['key_findings']:\n",
    "    print(\"\\n🎯 ADDITIONAL KEY FINDINGS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'whole_brain_age' in summary['key_findings']:\n",
    "        age_stats = summary['key_findings']['whole_brain_age']\n",
    "        print(f\"\\nWhole-Brain Age Associations:\")\n",
    "        print(f\"  • Significant features: {age_stats['n_correlated_features']}\")\n",
    "        print(f\"  • Strongest: {age_stats['strongest_correlation']['feature']} (r={age_stats['strongest_correlation']['r']:.3f})\")\n",
    "    \n",
    "    if 'whole_brain_sex' in summary['key_findings']:\n",
    "        sex_stats = summary['key_findings']['whole_brain_sex']\n",
    "        print(f\"\\nWhole-Brain Sex Differences:\")\n",
    "        print(f\"  • Significant features: {sex_stats['n_differences']}\")\n",
    "        print(f\"  • Largest: {sex_stats['largest_difference']['feature']} (d={sex_stats['largest_difference']['cohens_d']:.3f})\")\n",
    "    \n",
    "    if 'age_prediction' in summary['key_findings']:\n",
    "        ml_stats = summary['key_findings']['age_prediction']\n",
    "        print(f\"\\nAge Prediction (Machine Learning):\")\n",
    "        print(f\"  • Cross-validated R²: {ml_stats['r2_mean']:.3f} ± {ml_stats['r2_std']:.3f}\")\n",
    "        print(f\"  • Mean Absolute Error: {ml_stats['mae_mean']:.2f} ± {ml_stats['mae_std']:.2f} years\")\n",
    "        print(f\"  • Subjects: {ml_stats['n_subjects']}, Features: {ml_stats['n_features']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📁 OUTPUT FILES GENERATED:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count files by category\n",
    "csv_files = sorted(TABLES_DIR.glob('*.csv'))\n",
    "txt_files = sorted(FIGURES_DIR.glob('*.txt'))\n",
    "png_files = sorted(FIGURES_DIR.glob('*.png'))\n",
    "\n",
    "print(f\"\\n📊 Tables & Data ({len(csv_files)} CSV files in {TABLES_DIR}):\")\n",
    "for file in csv_files:\n",
    "    print(f\"  ✓ {file.name}\")\n",
    "\n",
    "print(f\"\\n📝 Feature Importance Reports ({len(txt_files)} text files in {FIGURES_DIR}):\")\n",
    "for file in txt_files:\n",
    "    print(f\"  ✓ {file.name}\")\n",
    "\n",
    "print(f\"\\n📈 Visualizations ({len(png_files)} PNG files in {FIGURES_DIR}):\")\n",
    "# Group by type\n",
    "atlas_files = [f for f in png_files if 'atlas' in f.name]\n",
    "importance_files = [f for f in png_files if 'importance' in f.name or 'diversity' in f.name or 'ranking' in f.name]\n",
    "other_files = [f for f in png_files if f not in atlas_files and f not in importance_files]\n",
    "\n",
    "if atlas_files:\n",
    "    print(f\"\\n  Atlas Visualizations ({len(atlas_files)}):\")\n",
    "    for file in atlas_files:\n",
    "        print(f\"    ✓ {file.name}\")\n",
    "\n",
    "if importance_files:\n",
    "    print(f\"\\n  Feature Importance Analyses ({len(importance_files)}):\")\n",
    "    for file in importance_files:\n",
    "        print(f\"    ✓ {file.name}\")\n",
    "\n",
    "if other_files:\n",
    "    print(f\"\\n  Other Visualizations ({len(other_files)}):\")\n",
    "    for file in other_files:\n",
    "        print(f\"    ✓ {file.name}\")\n",
    "\n",
    "print(f\"\\n📄 Summary: {TABLES_DIR / 'analysis_summary.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ COMPREHENSIVE POPULATION-LEVEL ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📝 PAPER WRITING GUIDE:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor your ISBI conference paper, use these outputs:\\n\")\n",
    "\n",
    "print(\"MAIN FIGURES (Choose 3-4):\")\n",
    "print(\"  1. Combined atlas heatmap (regional_combined_atlas_heatmap.png)\")\n",
    "print(\"     → Shows overall spatial pattern of demographic associations\")\n",
    "print(\"  2. AGE atlas + feature importance (create multi-panel figure)\")\n",
    "print(\"     → Shows WHERE age effects occur and WHICH features drive them\")\n",
    "print(\"  3. SEX atlas (if significant effects found)\")\n",
    "print(\"     → Shows sex-specific regional patterns\")\n",
    "print(\"  4. Age prediction results (if ML analysis performed)\")\n",
    "print(\"     → Shows predictive power of vascular features\")\n",
    "\n",
    "print(\"\\nSUPPLEMENTARY MATERIALS:\")\n",
    "print(\"  • All individual demographic atlases\")\n",
    "print(\"  • All feature importance heatmaps\")\n",
    "print(\"  • Complete statistical tables (CSV files)\")\n",
    "print(\"  • Feature ranking tables for all demographics\")\n",
    "\n",
    "print(\"\\nMETHODS SECTION:\")\n",
    "print(\"  Use: analysis_summary.json for dataset statistics\")\n",
    "print(\"  Mention: 30 brain regions, 15 morphometric features\")\n",
    "print(\"  Describe: Regional correlation analysis + feature importance ranking\")\n",
    "print(\"  State: FDR correction (q<0.05) for multiple comparisons\")\n",
    "\n",
    "print(\"\\nRESULTS SECTION - Key Points to Highlight:\")\n",
    "if 'regional_analysis' in summary and 'continuous_demographics' in summary['regional_analysis']:\n",
    "    if 'AGE' in summary['regional_analysis']['continuous_demographics']:\n",
    "        age_stats = summary['regional_analysis']['continuous_demographics']['AGE']\n",
    "        print(f\"  • Age effects: {age_stats['percent_significant']:.1f}% of region-feature tests significant\")\n",
    "        print(f\"    - {age_stats['n_regions_with_effects']} regions show age-related changes\")\n",
    "        if 'feature_importance' in summary and 'AGE' in summary['feature_importance']:\n",
    "            age_imp = summary['feature_importance']['AGE']\n",
    "            print(f\"    - {age_imp['most_important_feature']} most consistently affected\")\n",
    "            print(f\"      (top 3 in {age_imp['n_regions_where_top']} regions)\")\n",
    "\n",
    "if 'regional_analysis' in summary and 'categorical_demographics' in summary['regional_analysis']:\n",
    "    if 'SEX_ID' in summary['regional_analysis']['categorical_demographics']:\n",
    "        sex_stats = summary['regional_analysis']['categorical_demographics']['SEX_ID']\n",
    "        print(f\"  • Sex differences: {sex_stats['percent_significant']:.1f}% of region-feature tests significant\")\n",
    "        print(f\"    - {sex_stats['n_regions_with_effects']} regions show sex-specific patterns\")\n",
    "\n",
    "print(\"\\nDISCUSSION POINTS:\")\n",
    "print(\"  • Spatial heterogeneity of demographic effects across brain regions\")\n",
    "print(\"  • Feature-specific mechanisms (e.g., vessel rarefaction vs. tortuosity)\")\n",
    "print(\"  • Anterior-posterior gradients (if observed)\")\n",
    "print(\"  • Clinical implications for age-related vascular changes\")\n",
    "print(\"  • Comparison with existing literature on cerebrovascular aging\")\n",
    "\n",
    "print(\"\\n💡 TIPS:\")\n",
    "print(\"  1. Atlas visualizations are publication-ready at 300 DPI\")\n",
    "print(\"  2. Feature importance heatmaps complement atlas maps perfectly\")\n",
    "print(\"  3. Use CSV files to create custom summary tables in manuscript\")\n",
    "print(\"  4. Text files contain top 3 features per region for easy reference\")\n",
    "print(\"  5. Consider creating multi-panel figures combining atlas + importance\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"  1. Review all atlas visualizations for biological plausibility\")\n",
    "print(\"  2. Examine feature importance patterns for mechanistic insights\")\n",
    "print(\"  3. Cross-reference strongest effects with known vascular anatomy\")\n",
    "print(\"  4. Prepare figure legends explaining color scales and interpretations\")\n",
    "print(\"  5. Draft results paragraph for each key demographic variable\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VFEATENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
